For analysis purposes, you need to send all the logs from all of your Compute Engine instances to a BigQuery dataset called platform-logs. You have already installed the Stackdriver Logging agent on all the instances. You want to minimize cost. What should you do?
	
	A. 1. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. 2. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs.
	
	B. 1. In Stackdriver Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. 2. Create a Cloud Function that is triggered by messages in the logs topic. 3. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset.
	
	C. 1. In Stackdriver Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination. [✔️]
	
	D. 1. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. 2. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) 3. Use Cloud Scheduler to trigger this Cloud Function once a day.

-------

C. 1. In Stackdriver Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.

Explanation:
	
	This option utilizes Stackdriver Logging and BigQuery integration to export Compute Engine logs to a BigQuery dataset.
	
	By creating a filter to view only Compute Engine logs in Stackdriver Logging, you can specify the logs you want to export to BigQuery.
	
	The export is set up using Stackdriver Logging, and the destination is chosen as BigQuery with the specified dataset (platform-logs).
	
	This approach is cost-effective and directly exports relevant logs to BigQuery without the need for additional services or Cloud Functions.

Other Options:

	A. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs:

		This involves granting broader permissions (Data Editor role) to service accounts, and the metadata update 	might not be necessary for achieving the log export.
	
	B. In Stackdriver Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. Create a Cloud Function that is triggered by messages in the logs topic. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset:

		This involves additional complexity with Cloud Pub/Sub and Cloud Function, which may not be necessary for the given scenario.
	
	D. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY). Use Cloud Scheduler to trigger this Cloud Function once a day:

		This involves additional Cloud Function logic and scheduling to perform periodic inserts, which might not be the most straightforward solution for continuous log export. The continuous export option provided by Stackdriver Logging is often more efficient for this use case.