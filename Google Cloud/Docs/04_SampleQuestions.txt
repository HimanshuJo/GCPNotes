** You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do? [Question 180]

	A. Enable the Cloud Spanner API. [✔️]
	
	B. Configure your Cloud Spanner instance to be multi-regional. [Suggested]
	
	C. Create a new VPC network with subnetworks in all desired regions.
	
	D. Grant yourself the IAM role of Cloud Spanner Admin. [Bard Suggested]

	-------

	The appropriate answer is A. Enable the Cloud Spanner API.

	Explanation:

		Before creating a Cloud Spanner instance, you need to enable the Cloud Spanner API. Enabling the API allows you to interact with and use Cloud Spanner services in your project. Once the API is enabled, you can proceed with creating and configuring your Cloud Spanner instance.

	Option B (Configure your Cloud Spanner instance to be multi-regional) is a configuration step that comes after creating the Cloud Spanner instance. Multi-region configuration allows you to distribute the instance across multiple regions for high availability.

	Option C (Create a new VPC network with subnetworks in all desired regions) is related to networking and might be necessary for your application, but it is not the first step in preparing to create a Cloud Spanner instance. Enabling the API comes before setting up the networking.

	Option D (Grant yourself the IAM role of Cloud Spanner Admin) is not the first step in preparing to create a Cloud Spanner instance. While IAM roles are important for access control, enabling the API is the initial action you should take.

	Therefore, the first step in preparation for creating a Cloud Spanner instance is to enable the Cloud Spanner API (Option A).

	-------

	Bard:

		The most appropriate step to prepare for creating a Cloud Spanner instance in a newly created project is 

			D. Grant yourself the IAM role of Cloud Spanner Admin.

		This option ensures that you have the necessary administrative privileges to create and manage Cloud Spanner resources within your project. Without the Cloud Spanner Admin role, you won't be able to perform actions such as creating instances, managing databases, or configuring settings.

		The other options are not the first step in preparing for creating a Cloud Spanner instance:

		Option A: Enable the Cloud Spanner API. While it is important to enable the Cloud Spanner API before creating an instance, this is not the first step. Granting yourself the Cloud Spanner Admin role is the initial step to enable access to Cloud Spanner resources.

		Option B: Configure your Cloud Spanner instance to be multi-regional. This decision is made during the instance creation process, not in preparation. You can choose to create a multi-regional instance later based on your specific needs.

		Option C: Create a new VPC network with subnetworks in all desired regions. While this is a crucial step for deploying a globally distributed application, it is not related to preparing for creating a Cloud Spanner instance. The Cloud Spanner instance can be created independently of the VPC network.

		Therefore, granting yourself the IAM role of Cloud Spanner Admin is the first step you should take to prepare for creating a Cloud Spanner instance in your newly created project. This will ensure that you have the necessary permissions to manage Cloud Spanner resources and proceed with the instance creation process.

-------

** Your managed instance group raised an alert stating that new instance creation has failed to create new instances. You need to maintain the number of running instances specified by the template to be able to process expected application traffic. What should you do? [Question 121]
	
	A. Create an instance template that contains valid syntax which will be used by the instance group. Delete any persistent disks with the same name as instance names. [✔️]
	
	B. Create an instance template that contains valid syntax that will be used by the instance group. Verify that the instance name and persistent disk name values are not the same in the template. [ChatGPT Suggested]
	
	C. Verify that the instance template being used by the instance group contains valid syntax. Delete any persistent disks with the same name as instance names. Set the disks.autoDelete property to true in the instance template. [Suggested]
	
	D. Delete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template. [Bard Suggested]

	-------

	Bard:

		The most suitable solution to address the failed instance creation in the managed instance group and ensure sufficient instances for application traffic is 

			D. Delete the current instance template and replace it with a new instance template. Verify that the instance name and persistent disk name values are not the same in the template. Set the disks.autoDelete property to true in the instance template.

		The failed instance creation indicates an issue with the instance template being used by the instance group. Replacing the instance template with a valid one is essential to resolve this issue.

		The option to delete persistent disks with the same name as instance names is unnecessary and not directly related to the failed instance creation.

		Setting the disks.autoDelete property to true in the instance template ensures that persistent disks are automatically deleted when the instances are terminated, freeing up disk resources.

		Therefore, deleting the current instance template, verifying that instance and persistent disk names are unique, and setting the disks.autoDelete property to true in the new template are the necessary steps to address the failed instance creation issue and maintain the required number of running instances for application traffic.

-------

You are running multiple VPC-native Google Kubernetes Engine clusters in the same subnet. The IPs available for the nodes are exhausted, and you want to ensure that the clusters can grow in nodes when needed. What should you do?
[Question 149]
	
	A. Create a new subnet in the same region as the subnet being used.
	
	B. Add an alias IP range to the subnet used by the GKE clusters.
	
	C. Create a new VPC, and set up VPC peering with the existing VPC.
	
	D. Expand the CIDR range of the relevant subnet for the cluster. [✔️]

	-------

	Bard:

		The most suitable solution to address the IP exhaustion issue and ensure the ability to expand the clusters with more nodes is D. Expand the CIDR range of the relevant subnet for the cluster.

		Expanding the CIDR range of the subnet provides additional IP addresses for the GKE clusters, enabling them to grow in capacity as needed. This approach is relatively straightforward and allows for seamless integration with the existing network configuration.

		Options A and B, creating a new subnet or adding an alias IP range, might provide temporary solutions but do not offer a scalable and sustainable approach for larger-scale GKE deployments.

		Option C, creating a new VPC and setting up VPC peering, adds complexity and introduces additional routing considerations to the network architecture. It's not the most efficient or straightforward method for addressing the IP exhaustion issue.

		Therefore, expanding the CIDR range of the relevant subnet for the cluster is the recommended approach to provide additional IP addresses for the GKE clusters and accommodate future growth requirements.

-------

* You are the organization and billing administrator for your company. The engineering team has the Project Creator role on the organization. You do not want the engineering team to be able to link projects to the billing account. Only the finance team should be able to link a project to a billing account, but they should not be able to make any other changes to projects. What should you do? [Question 59]
	
	A. Assign the finance team only the Billing Account User role on the billing account. [Another Most Voted]
	
	B. Assign the engineering team only the Billing Account User role on the billing account.
	
	C. Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization. [✔️]
	
	D. Assign the engineering team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.

	-------

	Bard:

		The most suitable approach to restrict the engineering team from linking projects to the billing account while enabling the finance team to manage billing accounts is C. Assign the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization.

		This arrangement provides the finance team with the necessary permissions to link projects to the billing account without granting them excessive control over projects. The Billing Account User role allows them to view billing information and manage billing accounts, while the Project Billing Manager role enables them to link projects to billing accounts.

		Option A, assigning the finance team the Billing Account User role only, would grant them limited access to billing information but not the ability to link projects. This would not address the requirement of having a dedicated team for linking projects to billing accounts.

		Option B, assigning the engineering team the Billing Account User role, would provide them with unnecessary access to billing information, potentially leading to unauthorized billing changes.

		Option D, assigning the engineering team the Billing Account User role and the Project Billing Manager role, would grant them excessive control over both projects and billing accounts, contradicting the requirement of limiting their access to project linking capabilities.

		Therefore, assigning the finance team the Billing Account User role on the billing account and the Project Billing Manager role on the organization is the most appropriate solution to ensure that only the designated team can link projects to billing accounts while maintaining appropriate access control.

-------

Your company uses Cloud Storage to store application backup files for disaster recovery purposes. You want to follow Google's recommended practices. Which storage option should you use? [Question 6]
	
	A. Multi-Regional Storage
	
	B. Regional Storage
	
	C. Nearline Storage
	
	D. Coldline Storage [✔️]

	-------

	Bard:

		The most suitable storage option for storing application backup files for disaster recovery purposes, following Google's recommended practices, is D. Coldline Storage.

			Coldline Storage is designed for infrequently accessed data that can tolerate longer retrieval times (latency) and higher retrieval costs. It offers the lowest storage cost and is ideal for long-term data archival and disaster recovery scenarios.v

		Options A, B, and C are not recommended for disaster recovery purposes due to their retrieval latency and potential data loss concerns:

			Multi-Regional Storage: While it offers geographically dispersed storage for redundancy, it can have high retrieval latency and data transfer costs.

			Regional Storage: It offers lower latency than Multi-Regional Storage but still falls short of the requirements for rapid retrieval in disaster recovery.

			Nearline Storage: It offers improved retrieval latency compared to Coldline Storage but still incurs higher costs than Coldline Storage for infrequent access.

		Therefore, Coldline Storage is the most suitable option for storing application backup files for disaster recovery purposes, aligning with Google's recommended practices for cost-effective and reliable data archival.

-------

** You are hosting an application from Compute Engine virtual machines (VMs) in us""central1""a. You want to adjust your design to support the failure of a single Compute Engine zone, eliminate downtime, and minimize cost. What should you do?
[Question 36]
	
	A. "" Create Compute Engine resources in us""central1""b. "" Balance the load across both us""central1""a and us""central1""b. [✔️]
	
	B. "" Create a Managed Instance Group and specify us""central1""a as the zone. "" Configure the Health Check with a short Health Interval. [Suggested and chatGPT recommended]
	
	C. "" Create an HTTP(S) Load Balancer. "" Create one or more global forwarding rules to direct traffic to your VMs.
	
	D. "" Perform regular backups of your application. "" Create a Cloud Monitoring Alert and be notified if your application becomes unavailable. "" Restore from backups when notified.

	-------

	Bard:

		The most suitable approach to adjust the design to support the failure of a single Compute Engine zone, eliminate downtime, and minimize cost is A. Create Compute Engine resources in us""central1""b. Balance the load across both us""central1""a and us""central1""b.

			This solution ensures that the application can continue to operate even if a single Compute Engine zone becomes unavailable, minimizing downtime. By creating Compute Engine resources in another zone, the load can be balanced across both zones, ensuring that traffic is not disrupted in the event of a zone failure.

		Options B and C are not sufficient to address the requirement for high availability. A Managed Instance Group with a short Health Interval only ensures that unhealthy VMs are quickly replaced, but it does not protect against zone failures. A global forwarding rule can direct traffic to VMs in multiple zones, but it does not provide load balancing.

		Option D focuses on recovery after a failure rather than preventing it. Regular backups can help to restore the application from a previous state, but they do not address the issue of downtime. Cloud Monitoring Alerts can be used to notify you of application outages, but they do not automatically restore the application.

		Therefore, creating Compute Engine resources in us""central1""b and balancing the load across both zones is the most suitable solution to achieve high availability, eliminate downtime, and minimize cost.

-------

You need to immediately change the storage class of an existing Google Cloud bucket. You need to reduce service cost for infrequently accessed files stored in that bucket and for all files that will be added to that bucket in the future. What should you do? [Question 53]
	
	A. Use the gsutil to rewrite the storage class for the bucket. Change the default storage class for the bucket
	
	B. Use the gsutil to rewrite the storage class for the bucket. Set up Object Lifecycle management on the bucket [✔️]
	
	C. Create a new bucket and change the default storage class for the bucket. Set up Object Lifecycle management on lite bucket
	
	D. Create a new bucket and change the default storage class for the bucket import the files from the previous bucket into the new bucket

	-------

	Option B is the most appropriate choice because it combines two actions to achieve the desired result:

		Use the gsutil to rewrite the storage class for the bucket: You can use the gsutil tool to perform a rewrite of the storage class for the existing files in the bucket. This changes the storage class for the existing files.

		Set up Object Lifecycle management on the bucket: By setting up Object Lifecycle management on the bucket, you can define rules to automatically transition objects to a different storage class, including moving them to a more cost-effective storage class like Nearline or Coldline after a certain period of time.

	Option A (Use the gsutil to rewrite the storage class for the bucket. Change the default storage class for the bucket) is not entirely correct because changing the default storage class does not automatically change the storage class of existing objects. You would still need to use gsutil or another method to rewrite the storage class for existing files.

	Option C (Create a new bucket and change the default storage class for the bucket. Set up Object Lifecycle management on the new bucket) involves creating a new bucket, which might not be necessary. You can achieve the desired result by modifying the existing bucket.

	Option D (Create a new bucket and change the default storage class for the bucket. Import the files from the previous bucket into the new bucket) is a more complex solution and involves unnecessary steps. Modifying the existing bucket is a more straightforward approach.

	Therefore, option B is the recommended approach to immediately change the storage class of an existing Google Cloud bucket, reduce service costs for infrequently accessed files, and manage future objects' storage class through Object Lifecycle management.

	-------

	Bard:

		The most suitable approach to immediately change the storage class of an existing Google Cloud bucket while minimizing service costs for infrequently accessed files is B. Use the gsutil to rewrite the storage class for the bucket. Set up Object Lifecycle management on the bucket.

		This solution enables immediate cost savings for existing infrequently accessed files by rewriting the storage class for those files using gsutil. It also ensures that all new files uploaded to the bucket will be stored in the most cost-efficient storage class by setting up Object Lifecycle Management (OLM) for the bucket.

		Option A, rewriting the storage class for the bucket using gsutil, only addresses existing files and does not apply to new files uploaded in the future.

		Option C, creating a new bucket with a more cost-efficient storage class, requires copying files from the existing bucket to the new bucket, which can be time-consuming and inefficient.

		Option D, creating a new bucket and then importing files from the previous bucket, also involves unnecessary copying and is not the most efficient solution.

		Therefore, using gsutil to rewrite the storage class for existing files and setting up OLM for the bucket is the most effective approach to immediately reduce service costs and manage storage lifecycles effectively.

-------

You have a single binary application that you want to run on Google Cloud Platform. You decided to automatically scale the application based on underlying infrastructure CPU usage. Your organizational policies require you to use virtual machines directly. You need to ensure that the application scaling is operationally efficient and completed as quickly as possible. What should you do? [Question 16]
	
	A. Create a Google Kubernetes Engine cluster, and use horizontal pod autoscaling to scale the application.
	
	B. Create an instance template, and use the template in a managed instance group with autoscaling configured. [✔️]
	
	C. Create an instance template, and use the template in a managed instance group that scales up and down based on the time of day.
	
	D. Use a set of third-party tools to build automation around scaling the application up and down, based on Stackdriver CPU usage monitoring

	-------

	Option B is the most suitable choice based on the requirements and constraints specified:

		Create an instance template: An instance template defines the machine type, boot disk image, labels, and other instance properties. It provides a consistent configuration for creating new instances.

		Use the template in a managed instance group with autoscaling configured: A managed instance group allows you to create a group of identical instances that can automatically scale based on load or other metrics. By configuring autoscaling, you ensure that the number of instances can dynamically adjust based on CPU usage, meeting the requirement for automatic scaling based on infrastructure CPU usage.

	Options A and C involve Google Kubernetes Engine (GKE) and managed instance groups with specific scaling behaviors. While they are valid options for certain scenarios, they introduce additional complexity and may not align with the specified organizational policy to use virtual machines directly.

	Option D suggests using third-party tools to build automation around scaling based on Stackdriver CPU usage monitoring. While this is possible, using native Google Cloud Platform tools like managed instance groups with autoscaling (Option B) is generally more straightforward and operationally efficient.

	Therefore, option B is the recommended approach for automatically scaling the application based on underlying infrastructure CPU usage using virtual machines directly.

	-------

	Bard:

		The most suitable approach to automatically scale the single binary application based on underlying infrastructure CPU usage using virtual machines directly and ensuring operational efficiency is B. Create an instance template, and use the template in a managed instance group with autoscaling configured.

			This solution leverages the Managed Instance Groups (MIG) feature of Google Cloud Platform (GCP) to dynamically scale the number of VMs based on the CPU usage of the application running on those VMs. This approach offers several advantages:

			Operational Efficiency: 

				MIG provides built-in autoscaling capabilities, eliminating the need to manually manage the scaling process.

			Scalability: 

				MIG can quickly scale up or down the number of VMs based on CPU usage, ensuring that the application resources are always optimized for demand.

			Cost-Effectiveness: 

				MIG only provisions VMs when needed, reducing unnecessary costs.

		Option A, using Google Kubernetes Engine (GKE) for horizontal pod autoscaling, is not the most suitable choice because it introduces additional complexity and requires a Kubernetes environment.

		Option C, using an instance template in a MIG that scales based on the time of day, is not the most appropriate choice because it does not directly address dynamic scaling based on CPU usage.

		Option D, using third-party tools for automation, introduces additional complexity and may require ongoing maintenance.

		Therefore, leveraging the Managed Instance Groups feature with built-in autoscaling is the most efficient and effective approach to automatically scale the application based on CPU usage while adhering to the organizational policy of using virtual machines directly.

-------

You have a development project with appropriate IAM roles defined. You are creating a production project and want to have the same IAM roles on the new project, using the fewest possible steps. What should you do? [Question 10]
	
	A. Use gcloud iam roles copy and specify the production project as the destination project. [✔️]
	
	B. Use gcloud iam roles copy and specify your organization as the destination organization.
	
	C. In the Google Cloud Platform Console, use the 'create role from role' functionality.
	
	D. In the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions.

	-------

	Option A is the appropriate choice because the gcloud iam roles copy command allows you to copy IAM roles from one project to another, and you can specify the destination project directly.

	Option B is not suitable because there is no gcloud iam roles copy command that copies roles to an entire organization. IAM roles are typically copied at the project level.

	Option C (In the Google Cloud Platform Console, use the 'create role from role' functionality) allows you to create a new role based on an existing role, but it involves using the console and might require more steps than using the gcloud command.

	Option D (In the Google Cloud Platform Console, use the 'create role' functionality and select all applicable permissions) is a manual process that involves selecting permissions individually, and it might not be the most efficient way to replicate roles from the development project to the production project.

	Therefore, option A is the recommended choice as it allows you to copy IAM roles from the development project to the production project using the gcloud command-line tool with the least effort.

	-------

	Bard:

		The most efficient and least steps approach to replicate the IAM roles from the development project to the production project is A. Use gcloud iam roles copy and specify the production project as the destination project.

		This command allows you to copy IAM roles from one project to another in a single step, saving time and effort. It is the most direct and straightforward method to achieve the desired outcome.

		Option B, using gcloud iam roles copy and specifying the organization as the destination organization, is not the correct approach. This command is intended for copying IAM roles within an organization, not between projects.

		Options C and D involve creating new roles from scratch, which require manually selecting all applicable permissions. This approach is more time-consuming and error-prone compared to copying existing roles.

		Therefore, using the gcloud iam roles copy command to replicate the IAM roles from the development project to the production project is the most efficient and streamlined method.

-------

Your projects incurred more costs than you expected last month. Your research reveals that a development GKE container emitted a huge number of logs, which resulted in higher costs. You want to disable the logs quickly using the minimum number of steps. What should you do? [Question 55]
	
	A. 1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource. 
	[✔️]
	
	B. 1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE Cluster Operations resource.
	
	C. 1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Logging.
	
	D. 1. Go to the GKE console, and delete existing clusters. 2. Recreate a new cluster. 3. Clear the option to enable legacy Stackdriver Monitoring.

	-------

	Option A is the most appropriate and direct solution for quickly disabling logs from a specific GKE container resource. In Stackdriver Logging, you can go to the Logs ingestion window and disable the log source for the GKE container resource, preventing it from emitting logs.

	Option B suggests disabling the log source for the GKE Cluster Operations resource, which is not specific to the container emitting a large number of logs. It may impact other logs related to GKE Cluster Operations but may not address the specific issue.

	Options C and D involve deleting and recreating GKE clusters, which is a drastic measure and might result in downtime and potential data loss. Additionally, clearing options related to legacy Stackdriver Logging or Monitoring may not directly address the issue at hand.

	Therefore, option A is the recommended choice for quickly and specifically disabling logs from the problematic GKE container resource.

	-------

	Bard:

		The most efficient and quickest approach to disable the logs from the development GKE container and reduce costs is A. 1. Go to the Logs ingestion window in Stackdriver Logging, and disable the log source for the GKE container resource.

			This method directly targets the source of the excessive logging, enabling you to quickly resolve the issue without affecting other aspects of the application or the cluster.

		Options B and C involve deleting existing clusters and recreating new ones, which is a more drastic and time-consuming solution. This approach should only be considered as a last resort if disabling log sources proves ineffective.

		Therefore, disabling the log source for the specific GKE container using the Logs ingestion window in Stackdriver Logging is the most efficient and direct method to address the excessive logging and reduce costs without disrupting other components.

-------

You have an instance group that you want to load balance. You want the load balancer to terminate the client SSL session. The instance group is used to serve a public web application over HTTPS. You want to follow Google-recommended practices. What should you do? [Question 40]
	
	A. Configure an HTTP(S) load balancer. [✔️]
	
	B. Configure an internal TCP load balancer.
	
	C. Configure an external SSL proxy load balancer.
	
	D. Configure an external TCP proxy load balancer.

	-------

	Bard:

		The most appropriate approach to load balance the instance group with client-side SSL termination and follow Google-recommended practices is A. Configure an HTTP(S) load balancer.

			An HTTP(S) load balancer is specifically designed to handle HTTPS traffic and terminate SSL connections. It offloads the SSL processing from the instance group, improving performance and security.

		Option B, configuring an internal TCP load balancer, is not suitable for terminating SSL connections. TCP load balancers handle plain TCP traffic and do not support SSL termination.

		Option C, configuring an external SSL proxy load balancer, involves placing an additional layer of infrastructure, which can increase complexity and cost. It's recommended to use an HTTP(S) load balancer directly for simpler and more efficient SSL termination.

		Option D, configuring an external TCP proxy load balancer, is similar to option C and also not recommended for client-side SSL termination. It adds an additional layer of complexity and cost without providing any additional benefits for SSL handling.

		Therefore, configuring an HTTP(S) load balancer to terminate the client SSL session directly is the most appropriate and efficient approach, aligning with Google's recommendations for securing HTTPS traffic.

-------

You need to set up permissions for a set of Compute Engine instances to enable them to write data into a particular Cloud Storage bucket. You want to follow Google-recommended practices. What should you do?
[Question 26]
	
	A. Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/devstorage.write_only'.
	
	B. Create a service account with an access scope. Use the access scope 'https://www.googleapis.com/auth/cloud-platform'.
	
	C. Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket. [✔️]
	
	D. Create a service account and add it to the IAM role 'storage.objectAdmin' for that bucket.

	-------

	Option C is the recommended practice to grant permissions for Compute Engine instances to write data into a specific Cloud Storage bucket. Here's why:

		Create a service account: A service account is a special type of Google account that represents a non-human user, allowing you to authenticate and authorize API requests.

		Add it to the IAM role 'storage.objectCreator': The 'storage.objectCreator' role provides the minimum necessary permissions for creating objects (writing data) in a Cloud Storage bucket. It grants write access to the specific bucket without providing unnecessary broad permissions.

	Option A is not the best choice because specifying access scopes is more relevant for controlling what an instance can do beyond just Cloud Storage, and 'devstorage.write_only' is not a standard access scope.

	Option B suggests using the access scope 'https://www.googleapis.com/auth/cloud-platform,' which is a broader scope than needed. It grants the instance permissions to more Google Cloud services than necessary for writing data to a Cloud Storage bucket.

	Option D suggests using the IAM role 'storage.objectAdmin,' which provides more permissions than needed for just writing data. It includes read, write, and delete permissions, which may be excessive if you only want the instances to write data.

	Therefore, option C is the recommended approach for setting up permissions to enable Compute Engine instances to write data into a specific Cloud Storage bucket while following Google-recommended practices.

	-------

	Bard:

		The most appropriate approach to grant write access to a Cloud Storage bucket for a set of Compute Engine instances and follow Google-recommended practices is C. Create a service account and add it to the IAM role 'storage.objectCreator' for that bucket.

			This method grants the service account the specific permission to create objects in the bucket, allowing the Compute Engine instances to write data into the bucket. It adheres to the principle of least privilege, granting only the necessary access.

		Option A, using the access scope 'https://www.googleapis.com/auth/devstorage.write_only', provides more restrictive access than needed. While it allows writing to the bucket, it does not grant the ability to create objects, which is required in this scenario.

		Option B, using the access scope 'https://www.googleapis.com/auth/cloud-platform', grants excessively broad access to the service account. This can increase the risk of unauthorized access and should be avoided.

		Option D, using the IAM role 'storage.objectAdmin', grants excessive access to the service account. It allows not only creating objects but also managing them, which is not necessary in this case.

		Therefore, creating a service account with the specific permission to create objects in the bucket and adding it to the IAM role 'storage.objectCreator' for that bucket is the most secure and appropriate approach to grant write access to the Compute Engine instances.

-------

Your organization uses G Suite for communication and collaboration. All users in your organization have a G Suite account. You want to grant some G Suite users access to your Cloud Platform project. What should you do? [Question 83]

	A. Enable Cloud Identity in the GCP Console for your domain.
	
	B. Grant them the required IAM roles using their G Suite email address. [✔️]
	
	C. Create a CSV sheet with all users' email addresses. Use the gcloud command line tool to convert them into Google Cloud Platform accounts.
	
	D. In the G Suite console, add the users to a special group called cloud-console-users@yourdomain.com. Rely on the default behavior of the Cloud Platform to grant users access if they are members of this group.

	-------

	The appropriate answer is B. Grant them the required IAM roles using their G Suite email address.

	Explanation:

		Google Cloud Identity and Access Management (IAM) allows you to control access to resources within Google Cloud. When you want to grant G Suite users access to your Cloud Platform project, you should use their G Suite email addresses to assign the necessary IAM roles directly.

		Option A (Enable Cloud Identity in the GCP Console for your domain) is incorrect because Cloud Identity is a separate service that allows you to manage users and devices, but it may not grant access to your Cloud Platform project by itself.

		Option C (Create a CSV sheet with all users' email addresses and use the gcloud command line tool) is not a standard practice for granting access to G Suite users in the context of IAM roles in Google Cloud.

		Option D (In the G Suite console, add users to a special group called cloud-console-users@yourdomain.com) is not the recommended way to manage access in Google Cloud. While you can use groups for IAM roles, it's more common to directly assign roles to individual users using their email addresses.

		Therefore, option B is the most appropriate and standard way to grant G Suite users access to your Google Cloud Platform project by assigning the necessary IAM roles using their G Suite email addresses.

-------

* You need to manage a third-party application that will run on a Compute Engine instance. Other Compute Engine instances are already running with default configuration. Application installation files are hosted on Cloud Storage. You need to access these files from the new instance without allowing other virtual machines (VMs) to access these files. What should you do? [Question 183]
	
	A. Create the instance with the default Compute Engine service account. Grant the service account permissions on Cloud Storage. [Suggested]
	
	B. Create the instance with the default Compute Engine service account. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance.
	
	C. Create a new service account and assign this service account to the new instance. Grant the service account permissions on Cloud Storage. [✔️]
	
	D. Create a new service account and assign this service account to the new instance. Add metadata to the objects on Cloud Storage that matches the metadata on the new instance.

	-------

	The appropriate answer is C. Create a new service account and assign this service account to the new instance. Grant the service account permissions on Cloud Storage.

	Explanation:

		Option C is the recommended approach for secure and controlled access to Cloud Storage for the third-party application running on the Compute Engine instance. Here's why:

			Create a new service account: Creating a new service account allows you to control the permissions and access of the specific instance without affecting other instances.

			Assign this service account to the new instance: By assigning the new service account to the specific instance, you ensure that the permissions associated with the service account are only applicable to that instance.

			Grant the service account permissions on Cloud Storage: To allow the instance to access the files in Cloud Storage, you need to grant the service account the necessary permissions on the specific Cloud Storage bucket or objects. This way, only the instances associated with this service account will have access.

	Options A and B both involve using the default Compute Engine service account, which may lead to broader permissions and potential security risks. It is generally recommended to create a dedicated service account for specific instances to follow the principle of least privilege.

	Option D suggests adding metadata to the objects on Cloud Storage that matches the metadata on the new instance, but this is not a standard practice for controlling access permissions. Using service accounts and IAM roles is the preferred method for managing access control.

	Therefore, option C is the most appropriate choice for accessing Cloud Storage files from the new instance without allowing other virtual machines to access these files while following Google-recommended practices.

	-------

	Bard:

		The most appropriate approach to manage the third-party application on a Compute Engine instance without allowing other VMs to access the application installation files is C. Create a new service account and assign this service account to the new instance. Grant the service account permissions on Cloud Storage.

			Creating a new service account and granting it the necessary permissions on Cloud Storage provides the most secure and controlled access to the application installation files. This approach adheres to the principle of least privilege by granting only the specific permissions required for the application to function.

		Option A, creating the instance with the default Compute Engine service account, grants excessive access to the application installation files. This could potentially allow other VMs to access and potentially modify the files, compromising the application's integrity.

		Option B, adding metadata to the objects on Cloud Storage, does not provide a secure mechanism for restricting access to the application files. It's a less reliable and granular approach than using a service account with specific permissions.

		Option D, creating a new service account, assigning it to the new instance, and adding metadata to the objects on Cloud Storage, is an unnecessary combination of methods. Granting the service account the necessary permissions on Cloud Storage alone is sufficient to restrict access.

		Therefore, creating a new service account, granting it the necessary permissions on Cloud Storage, and assigning it to the new instance is the most secure and efficient approach to manage the third-party application and protect the application installation files from unauthorized access.

-------

You have a website hosted on App Engine standard environment. You want 1% of your users to see a new test version of the website. You want to minimize complexity. What should you do? [Question 56]
	
	A. Deploy the new version in the same application and use the --migrate option.
	
	B. Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version. [✔️]
	
	C. Create a new App Engine application in the same project. Deploy the new version in that application. Use the App Engine library to proxy 1% of the requests to the new version.
	
	D. Create a new App Engine application in the same project. Deploy the new version in that application. Configure your network load balancer to send 1% of the traffic to that new application.

	-------

	Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version.

	Explanation:

		Option B is the most straightforward and suitable solution for achieving the goal of exposing 1% of users to a new test version of the website. In App Engine, you can use traffic splitting to direct a specified percentage of traffic to a new version.

		Deploy the new version in the same application: This ensures that both versions are part of the same App Engine application.

		Use the --splits option: With the gcloud app deploy command, you can use the --splits option to control the traffic distribution between versions. In this case, you would set a weight of 99 for the current version and a weight of 1 for the new version, effectively directing 1% of the traffic to the new version.

		Option A (Deploy the new version in the same application and use the --migrate option) is not a valid option for splitting traffic based on a percentage. The --migrate option is used for a different purpose, primarily for handling database migrations during the deployment process.

		Options C and D involve creating a new App Engine application, which is unnecessary complexity for achieving a simple traffic-splitting scenario. Using traffic splitting within the same application is a more direct and easier approach.

		Therefore, option B is the recommended choice for minimizing complexity and achieving the desired 1% traffic split for the new test version.

	-------

	The most appropriate approach to deploy the new test version of the website and expose it to 1% of users while minimizing complexity is B. Deploy the new version in the same application and use the --splits option to give a weight of 99 to the current version and a weight of 1 to the new version.

		This method utilizes App Engine's traffic splitting feature to divert a small portion of traffic (1%) to the new test version while maintaining the majority of traffic (99%) on the current production version. This approach is simpler and more efficient compared to creating a separate App Engine application or using App Engine library to proxy requests.

	Option A, using the --migrate option, forces a complete switch to the new test version, which is not the intended approach for a gradual rollout.

	Option C, creating a new App Engine application for the new version and using the App Engine library to proxy requests, introduces unnecessary complexity and requires manual configuration.

	Option D, creating a new App Engine application for the new version and configuring a network load balancer to send 1% of traffic to that application, is also more complex and requires additional infrastructure management.

	Therefore, deploying the new version in the same application using the --splits option to assign a 1% weight to the new version is the most straightforward and efficient approach to gradually expose the test version to a small portion of users while maintaining the stability of the production version.

-------

Your company is moving from an on-premises environment to Google Cloud. You have multiple development teams that use Cassandra environments as backend databases. They all need a development environment that is isolated from other Cassandra instances. You want to move to Google Cloud quickly and with minimal support effort. What should you do?
[Question 122]
	
	A. 1. Build an instruction guide to install Cassandra on Google Cloud. 2. Make the instruction guide accessible to your developers.
	
	B. 1. Advise your developers to go to Cloud Marketplace. 2. Ask the developers to launch a Cassandra image for their development work. [✔️]
	
	C. 1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Use the snapshot to create instances for your developers.
	
	D. 1. Build a Cassandra Compute Engine instance and take a snapshot of it. 2. Upload the snapshot to Cloud Storage and make it accessible to your developers. 3. Build instructions to create a Compute Engine instance from the snapshot so that developers can do it themselves.

	-------

	The appropriate answer is B. Advise your developers to go to Cloud Marketplace. Ask the developers to launch a Cassandra image for their development work.

	Explanation:

		Option B is the most efficient and straightforward approach to provide isolated development environments for multiple teams using Cassandra on Google Cloud. Here's why:

			Cloud Marketplace: Google Cloud Marketplace provides a wide range of pre-configured solutions, including Cassandra images. It allows developers to quickly deploy and configure software without manual installation steps.

			Cassandra image from Cloud Marketplace: By directing developers to Cloud Marketplace, they can easily launch a Cassandra image tailored for their development work. This eliminates the need for manual installations and ensures consistency across different development environments.

		Options A, C, and D involve manual steps or more complex processes that may not be as quick, efficient, or scalable for providing isolated development environments:

		Option A: Building an instruction guide for manual installation requires more effort from developers and increases the chances of inconsistencies between environments.

		Option C: Taking a snapshot of a single Compute Engine instance and creating instances from that snapshot may introduce limitations and is not as scalable for multiple development teams.

		Option D: Taking a snapshot and uploading it to Cloud Storage adds unnecessary complexity. Additionally, instructing developers to create Compute Engine instances from the snapshot might be more involved than using Cloud Marketplace.

		Therefore, option B is the recommended choice for a quick and minimal-effort migration to Google Cloud, providing isolated Cassandra development environments for multiple teams.

-------

** You have successfully created a development environment in a project for an application. This application uses Compute Engine and Cloud SQL. Now you need to create a production environment for this application. The security team has forbidden the existence of network routes between these 2 environments and has asked you to follow Google-recommended practices. What should you do? [Question 110]
	
	A. Create a new project, enable the Compute Engine and Cloud SQL APIs in that project, and replicate the setup you have created in the development environment. [✔️]
	
	B. Create a new production subnet in the existing VPC and a new production Cloud SQL instance in your existing project, and deploy your application using those resources.
	
	C. Create a new project, modify your existing VPC to be a Shared VPC, share that VPC with your new project, and replicate the setup you have in the development environment in that new project in the Shared VPC. 
	[✔️] [Bard and chatGPT recommended]
	
	D. Ask the security team to grant you the Project Editor role in an existing production project used by another division of your company. Once they grant you that role, replicate the setup you have in the development environment in that project.

	-------

	Bard:

		The most appropriate approach to create a production environment for the application while adhering to the security team's requirements is C. Create a new project, modify your existing VPC to be a Shared VPC, share that VPC with your new project, and replicate the setup you have in the development environment in that new project in the Shared VPC.

			This method establishes a separate production environment in a new project while maintaining isolation between the development and production environments. By using a Shared VPC, the production environment can securely share the underlying network infrastructure (VPC) with the development environment without direct network routes between them. This aligns with the security team's directive and follows Google's recommended practices for isolating production environments.

		Option A, creating a new project and replicating the setup in it, would create a separate production environment but would not address the network isolation requirement.

		Option B, creating a new production subnet and Cloud SQL instance in the existing VPC, would not provide the isolation needed, as network routes between the development and production environments would still exist.

		Option D, requesting access to another division's production project, would violate the security team's policy of not allowing network routes between development and production environments.

		Therefore, creating a new project, enabling the Shared VPC feature, sharing the VPC with the new project, and replicating the setup in the Shared VPC is the most secure and compliant approach to establish a separate production environment for the application.

-------

* You have a batch workload that runs every night and uses a large number of virtual machines (VMs). It is fault-tolerant and can tolerate some of the VMs being terminated. The current cost of VMs is too high. What should you do?
[Question 150]

	A. Run a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs. [✔️]
	
	B. Run a test using simulated maintenance events. If the test is successful, use N1 Standard VMs when running future jobs. [Suggested]
	
	C. Run a test using a managed instance group. If the test is successful, use N1 Standard VMs in the managed instance group when running future jobs.
	
	D. Run a test using N1 standard VMs instead of N2. If the test is successful, use N1 Standard VMs when running future jobs.

	-------

	The appropriate answer is A. Run a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs.

	Explanation:

		Option A, using preemptible VMs, is a suitable and cost-effective solution for a batch workload that is fault-tolerant and can tolerate some VMs being terminated. Here's why:

			Preemptible VMs: Preemptible VMs are short-lived, cost-effective instances that are suitable for fault-tolerant workloads. They are significantly cheaper than regular (non-preemptible) instances, making them a cost-effective choice for batch workloads.

			Simulated maintenance events: Running a test using simulated maintenance events helps assess how well the batch workload handles VM terminations. Preemptible VMs can be preempted by Google Cloud with little notice, and simulating these events helps validate the workload's fault tolerance.

		Option B (Run a test using simulated maintenance events. If the test is successful, use N1 Standard VMs when running future jobs) suggests using regular (non-preemptible) N1 Standard VMs. While this may be a valid option, preemptible VMs are designed explicitly for cost savings and can offer a more economical solution for fault-tolerant workloads.

		Option C (Run a test using a managed instance group. If the test is successful, use N1 Standard VMs in the managed instance group when running future jobs) introduces the concept of a managed instance group but doesn't explicitly address the cost concerns. Preemptible VMs can be used within a managed instance group to enhance scalability and cost savings.

		Option D (Run a test using N1 standard VMs instead of N2. If the test is successful, use N1 Standard VMs when running future jobs) focuses on changing the machine type but doesn't specifically leverage the cost advantages of preemptible VMs for fault-tolerant batch workloads.

		Therefore, option A is the recommended choice for reducing costs while maintaining fault tolerance for the nightly batch workload.

	-------

	Bard:

		The most suitable approach to address the high cost of VMs for the fault-tolerant batch workload by utilizing preemptible VMs is A. Run a test using simulated maintenance events. If the test is successful, use preemptible N1 Standard VMs when running future jobs.

			Preemptible VMs are a cost-effective option for batch workloads that can tolerate disruptions, as they can be terminated by Google Cloud without prior notice. By running a test using simulated maintenance events, you can assess whether the batch workload can handle the potential for VM terminations without impacting its functionality. If the test proves successful, it's recommended to switch to preemptible N1 Standard VMs for the batch workload, as they offer significant cost savings compared to regular VMs.

		Option B, using N1 Standard VMs without testing for potential disruption, could lead to unexpected downtime if the batch workload is not resilient to VM terminations.

		Option C, using a managed instance group, provides management and orchestration capabilities for VMs, but it doesn't address the cost concern directly.

		Option D, switching to N1 Standard VMs from N2 without testing, could introduce performance or compatibility issues if the batch workload has specific requirements that N2 VMs fulfill better.

		Therefore, conducting a test using simulated maintenance events to verify the batch workload's resilience to VM terminations and then transitioning to preemptible N1 Standard VMs is the most efficient and cost-effective approach.

-------

* You have deployed multiple Linux instances on Compute Engine. You plan on adding more instances in the coming weeks. You want to be able to access all of these instances through your SSH client over the internet without having to configure specific access on the existing and new instances. You do not want the
Compute Engine instances to have a public IP. What should you do? [Question 191]
	
	A. Configure Cloud Identity-Aware Proxy for HTTPS resources.
	
	B. Configure Cloud Identity-Aware Proxy for SSH and TCP resources. [✔️]
	
	C. Create an SSH keypair and store the public key as a project-wide SSH Key. [Bard Suggested]
	
	D. Create an SSH keypair and store the private key as a project-wide SSH Key.

	-------

	The appropriate answer is B. Configure Cloud Identity-Aware Proxy for SSH and TCP resources.

	Explanation:

		Option B is the most suitable choice for allowing secure SSH access to Compute Engine instances without public IPs using Cloud Identity-Aware Proxy (IAP). Here's why:

			Cloud Identity-Aware Proxy (IAP): IAP allows you to establish secure, identity-based access to your Compute Engine instances. With IAP for TCP resources, you can enable SSH access to instances without exposing them directly to the internet. IAP provides authentication and authorization, enhancing security.

			SSH and TCP resources: Configuring IAP for SSH and TCP resources means that you can use IAP to secure both SSH (for shell access) and other TCP-based protocols.

		Options A, C, and D are not suitable or do not address the requirement in the context of providing secure SSH access without public IPs:

		Option A (Configure Cloud Identity-Aware Proxy for HTTPS resources): This option is focused on HTTPS resources and is not directly related to SSH access to Compute Engine instances.

		Option C (Create an SSH keypair and store the public key as a project-wide SSH Key): While creating an SSH keypair is necessary for authentication, storing the public key as a project-wide SSH key doesn't address the requirement of secure access without public IPs.

		Option D (Create an SSH keypair and store the private key as a project-wide SSH Key): Storing private keys as project-wide SSH keys is not a recommended practice for securing SSH access. Private keys should be kept secure and only accessible by authorized individuals.

		Therefore, option B is the recommended choice for providing secure SSH access to Compute Engine instances over the internet without the need for public IPs.

	-------

	Bard:

		The most appropriate approach to address the requirement of accessing multiple Linux instances through SSH without configuring access on each instance individually is C. Create an SSH keypair and store the public key as a project-wide SSH Key.

			By creating an SSH keypair and storing the public key as a project-wide SSH key, you can connect to the instances using the SSH client without having to configure SSH access on each instance. This method allows for centralized management of SSH access, ensuring consistent access rules across all instances.

		Option A, configuring Cloud Identity-Aware Proxy (IAP) for HTTPS resources, focuses on securing HTTPS traffic and doesn't address the SSH access requirement.

		Option B, configuring Cloud IAP for SSH and TCP resources, although applicable for SSH access, requires provisioning a new IAP tunnel for each instance, which is not efficient for multiple instances.

		Option D, storing the private key as a project-wide SSH key, poses a security risk as the private key grants full access to the instances.

		Therefore, creating an SSH keypair and storing the public key as a project-wide SSH key is the most secure and efficient approach to centrally manage SSH access to multiple Linux instances on Compute Engine without requiring explicit configuration on each instance.

-------

** You are using Container Registry to centrally store your company's container images in a separate project. In another project, you want to create a Google Kubernetes Engine (GKE) cluster. You want to ensure that Kubernetes can download images from Container Registry. What should you do?

	A. In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes. [✔️]
	
	B. When you create the GKE cluster, choose the Allow full access to all Cloud APIs option under 'Access scopes'.
	[Bard Suggested]
	
	C. Create a service account, and give it access to Cloud Storage. Create a P12 key for this service account and 
	use it as an imagePullSecrets in Kubernetes. [Suggested]
	
	D. Configure the ACLs on each image in Cloud Storage to give read-only access to the default Compute Engine service account.

	-------

	The correct answer is A. In the project where the images are stored, grant the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes.

	Explanation:

		Option A is the recommended approach for ensuring that Kubernetes nodes in a GKE cluster can download container images from Container Registry:

			Storage Object Viewer IAM role: Granting the Storage Object Viewer role to the service account used by the Kubernetes nodes in the project where the images are stored allows the nodes to read the container images from Cloud Storage.
		
		Options B, C, and D are not the best choices for achieving this goal, and here's why:

			Option B (Allow full access to all Cloud APIs): Enabling full access to all Cloud APIs is overly permissive and not the best practice for security. It grants more permissions than needed and does not specifically address access to Container Registry images.

			Option C (Create a service account and use it as imagePullSecrets): While creating a service account and using it as an imagePullSecret in Kubernetes is a valid approach for accessing private container images, it is not necessary when dealing with Container Registry. GKE can automatically use the appropriate service account credentials to pull images from Container Registry.

		Option D (Configure ACLs on each image in Cloud Storage): Manually configuring ACLs on each image in Cloud Storage is not a scalable or practical solution. Granting the Storage Object Viewer role to the service account used by the Kubernetes nodes provides a more centralized and manageable approach.

		Therefore, option A is the recommended choice for granting the necessary permissions to the Kubernetes nodes in the GKE cluster to pull container images from Container Registry.

	-------

	Bard:

		The most appropriate approach to allow Kubernetes to download images from Container Registry in a separate project is 	

			B. When you create the GKE cluster, choose the Allow full access to all Cloud APIs option under 'Access scopes'.

				By granting "Allow full access to all Cloud APIs" when creating the GKE cluster, you provide the cluster's service account with the necessary permissions to access Container Registry and download images from it. This option simplifies the setup and ensures that the cluster has the required access without the need to manage IAM roles or service accounts explicitly.

		Option A, granting the Storage Object Viewer IAM role to the service account used by the Kubernetes nodes, provides read-only access to Cloud Storage, but it doesn't encompass the specific permissions needed to download images from Container Registry.

		Option C, creating a service account and granting it access to Cloud Storage, involves manual configuration and managing IAM roles. It's more complex compared to the simpler option of granting broad access when creating the cluster.

		Option D, configuring ACLs on each image in Cloud Storage to give read-only access to the default Compute Engine service account, requires managing ACLs for each image, which can be tedious and error-prone.

		Therefore, granting "Allow full access to all Cloud APIs" when creating the GKE cluster is the most straightforward and efficient approach to ensure that Kubernetes can download images from Container Registry without additional configuration and management.

-------

* You need a dynamic way of provisioning VMs on Compute Engine. The exact specifications will be in a dedicated configuration file. You want to follow Google's recommended practices. Which method should you use? [Question 11]
	
	A. Deployment Manager [✔️]
	
	B. Cloud Composer
	
	C. Managed Instance Group [Suggested]
	
	D. Unmanaged Instance Group

	-------

	The appropriate answer is A. Deployment Manager.

	Explanation:

		Google's recommended practice for dynamically provisioning resources, especially when the specifications are defined in a dedicated configuration file, is to use Deployment Manager. Here's why:

			Deployment Manager: Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. It allows you to describe resources and properties in a configuration file (written in YAML or Python), and then use that configuration to create or update resources in a predictable and repeatable way.
		
		Options B, C, and D are not as suitable for dynamically provisioning VMs based on a dedicated configuration file:

			Option B (Cloud Composer): Cloud Composer is a fully managed workflow orchestration service that is built on Apache Airflow. While it allows you to define and execute workflows, it is more focused on data workflows and orchestration rather than infrastructure provisioning.

		Option C (Managed Instance Group): Managed Instance Group is a good choice for creating groups of identical VM instances, but it may not be the most suitable for dynamic provisioning based on a configuration file. It is more designed for scaling and managing groups of identical instances.

		Option D (Unmanaged Instance Group): Unmanaged Instance Group is not typically used for dynamic provisioning based on a configuration file. It provides basic autohealing features for groups of instances but lacks the level of automation and configuration management found in tools like Deployment Manager.

		Therefore, for dynamic provisioning of VMs with specifications defined in a dedicated configuration file, Google's recommended practice is to use Deployment Manager (option A).

	-------

	Bard:

		The most appropriate approach to dynamically provision VMs on Compute Engine based on a dedicated configuration file while adhering to Google's recommended practices is A. Deployment Manager.

			Deployment Manager provides a declarative way to provision and manage infrastructure resources, including Compute Engine instances. It utilizes templates that define the desired infrastructure state, and it automatically manages the creation and updates of those resources based on changes to the templates.

		Option B, Cloud Composer, is a workflow orchestration platform that utilizes Apache Airflow to manage and schedule workflows. It's not specifically designed for dynamic provisioning of VMs.

		Option C, Managed Instance Group (MIG), is a managed service for provisioning and managing multiple Compute Engine instances. While it provides some level of automation, it's not as flexible or dynamic as Deployment Manager for provisioning based on configuration files.

		Option D, Unmanaged Instance Group, requires manual configuration and management of instance provisioning and scaling. It's not a recommended approach for dynamic provisioning.

		Therefore, utilizing Deployment Manager with its declarative templates and ability to manage infrastructure changes based on configuration files is the most suitable method for dynamic provisioning of VMs on Compute Engine in a manner aligned with Google's recommended practices.

-------

** You are setting up a Windows VM on Compute Engine and want to make sure you can log in to the VM via RDP. What should you do? [Question 73]
	
	A. After the VM has been created, use your Google Account credentials to log in into the VM.
	
	B. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM. [✔️]
	
	C. When creating the VM, add metadata to the instance using 'windows-password' as the key and a password as the value.
	[Bard Suggested]
	
	D. After the VM has been created, download the JSON private key for the default Compute Engine service account. Use the credentials in the JSON file to log in to the VM. [Suggested]

	-------

	The appropriate answer is B. After the VM has been created, use gcloud compute reset-windows-password to retrieve the login credentials for the VM.

	Explanation:

		Option B is the correct choice for retrieving the login credentials for a Windows VM on Compute Engine:

			gcloud compute reset-windows-password: This command allows you to reset the Windows password for a VM instance and retrieve the new password. It is a secure way to access the VM without needing to store or transmit passwords in an insecure manner.
		
		Options A, C, and D are not suitable for retrieving the login credentials or logging in to a Windows VM:

			Option A (Use your Google Account credentials): Google Account credentials are not used for logging in to Windows VMs. Windows VMs typically require a username and password specific to the Windows operating system.

		Option C (Add metadata with 'windows-password'): While it's possible to set up a password during VM creation using metadata, it's not considered a best practice, and using gcloud compute reset-windows-password is a more secure and recommended approach.

		Option D (Download the JSON private key for the Compute Engine service account): Downloading the JSON private key for the default Compute Engine service account does not provide the necessary credentials for logging in to a Windows VM. Windows VMs typically require a username and password.

		Therefore, option B is the recommended choice for securely retrieving the login credentials for a Windows VM on Compute Engine.

	-------

	Bard:

		The most appropriate approach to securely set up a Windows VM on Compute Engine and ensure RDP access is C. When creating the VM, add metadata to the instance using 'windows-password' as the key and a password as the value.

			By adding the password as metadata to the VM instance during creation, you can securely store the password and retrieve it programmatically without requiring manual configuration or the use of your Google Account credentials. This method adheres to the principle of least privilege by granting only the necessary access to the VM.

		Option A, using your Google Account credentials, is not secure and can pose a security risk.

		Option B, using the gcloud compute reset-windows-password command, is not recommended as it requires retrieving the password from the VM, which can be vulnerable to exposure.

		Option D, using the JSON private key for the default Compute Engine service account, is not appropriate for setting up RDP access as it grants broad access to the VM and its resources.

		Therefore, setting a password as metadata during VM creation and retrieving it programmatically is the most secure and efficient approach to configure RDP access for a Windows VM on Compute Engine.

-------

* You built an application on Google Cloud that uses Cloud Spanner. Your support team needs to monitor the environment but should not have access to table data.
You need a streamlined solution to grant the correct permissions to your support team, and you want to follow Google-recommended practices. What should you do? [Question 67]
	
	A. Add the support team group to the roles/monitoring.viewer role. [✔️]
	
	B. Add the support team group to the roles/spanner.databaseUser role. [Suggested]
	
	C. Add the support team group to the roles/spanner.databaseReader role.
	
	D. Add the support team group to the roles/stackdriver.accounts.viewer role.

	-------

	The appropriate answer is A. Add the support team group to the roles/monitoring.viewer role.

	Explanation:

		roles/monitoring.viewer role: This role grants read-only access to monitoring data, which includes metrics, dashboards, and other monitoring-related information. It does not provide access to the actual data stored in Cloud Spanner tables, aligning with the requirement to monitor the environment without accessing table data.
		
		Options B, C, and D grant permissions that go beyond the monitoring needs and may expose sensitive data:

			Option B (roles/spanner.databaseUser role): This role is designed for users who need to connect to Cloud Spanner databases and execute SQL queries. It includes the ability to read data from tables, which is not suitable for the support team's requirement to only monitor the environment.

			Option C (roles/spanner.databaseReader role): Similar to the databaseUser role, the databaseReader role includes permissions to read data from tables, which is more than what is needed for monitoring purposes.

			Option D (roles/stackdriver.accounts.viewer role): This role provides visibility into Stackdriver accounts, but it is broader than what is necessary for monitoring Cloud Spanner specifically. It may grant access to additional resources and data that are not relevant to the support team's monitoring needs.

		Therefore, option A is the recommended choice for granting the correct permissions to the support team to monitor the Cloud Spanner environment without accessing table data.

	-------

	Bard:

		The most appropriate approach to grant monitoring permissions to the support team without providing table data access, aligning with Google-recommended practices, is A. Add the support team group to the roles/monitoring.viewer role.

		By assigning the roles/monitoring.viewer role to the support team group, you provide them with the necessary permissions to view Cloud Monitoring metrics and alerts related to the application, ensuring that they can effectively monitor its health and performance without accessing sensitive table data.

		Option B, adding the support team group to the roles/spanner.databaseUser role, grants excessive access, allowing them to modify and delete data in the database.

		Option C, adding the support team group to the roles/spanner.databaseReader role, allows them to read table data, exceeding the intended monitoring scope.

		Option D, adding the support team group to the roles/stackdriver.accounts.viewer role, provides access to account-level information, not specifically related to monitoring the application.

		Therefore, granting the roles/monitoring.viewer role to the support team ensures they have the appropriate permissions to monitor the application without compromising data security and adhering to Google's recommended practices.

-------

** You have a Linux VM that must connect to Cloud SQL. You created a service account with the appropriate access rights. You want to make sure that the VM uses this service account instead of the default Compute Engine service account. What should you do? [Question 19]
	
	A. When creating the VM via the web console, specify the service account under the 'Identity and API Access' section. [✔️]
	
	B. Download a JSON Private Key for the service account. On the Project Metadata, add that JSON as the value for the key compute-engine-service- account.
	
	C. Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine- service-account. [Bard Suggested and Another Most Voted]
	
	D. Download a JSON Private Key for the service account. After creating the VM, ssh into the VM and save the JSON under ~/.gcloud/compute-engine-service- account.json.

	-------

	The appropriate answer is A. When creating the VM via the web console, specify the service account under the 'Identity and API Access' section.

	Explanation:

		Option A (When creating the VM via the web console, specify the service account): When creating a VM in the Google Cloud Console, you can specify the service account under the 'Identity and API Access' section. This is a straightforward and user-friendly way to assign a service account to a VM during the creation process.
		
		Options B, C, and D are not the recommended approaches or are incorrect for the following reasons:

			Option B (Download a JSON Private Key for the service account): This option suggests adding a JSON Private Key to the Project Metadata, but this is not the standard or secure method of specifying a service account for a VM. It's also not directly related to the 'compute-engine-service-account' key.

		Option C (Download a JSON Private Key for the service account): Similar to option B, adding a JSON Private Key to the Custom Metadata of the VM is not a standard or recommended approach for specifying the service account for a VM.

		Option D (Download a JSON Private Key for the service account): Manually saving a JSON Private Key on the VM itself is not a recommended practice. It's preferable to specify the service account during VM creation to ensure proper association.

		Therefore, option A is the recommended choice for associating a service account with a VM when creating it via the web console.

	-------

	Bard:

		The most appropriate approach to ensure that the Linux VM uses the specifically created service account for Cloud SQL access is C. Download a JSON Private Key for the service account. On the Custom Metadata of the VM, add that JSON as the value for the key compute-engine-service-account.

			By adding the service account's JSON key to the Custom Metadata of the VM, you can dynamically load the credentials of the service account into the VM's environment. This method ensures that the VM uses the intended service account for Cloud SQL authentication without the need to explicitly configure the service account on the VM.

		Option A, specifying the service account under the 'Identity and API Access' section when creating the VM, provides static configuration and doesn't allow for dynamic updates.

		Option B, adding the JSON key to the Project Metadata, can be used to distribute the service account credentials across multiple VMs, but it doesn't provide fine-grained control over individual VMs.

		Option D, downloading the JSON key and manually adding it to the VM, requires manual configuration for each VM and doesn't scale well for multiple instances.

		Therefore, adding the service account's JSON key to the Custom Metadata of the VM is the most secure and flexible approach to ensure that the Linux VM uses the intended service account for Cloud SQL access without compromising security or manual intervention.

-------

You have an object in a Cloud Storage bucket that you want to share with an external company. The object contains sensitive data. You want access to the content to be removed after four hours. The external company does not have a Google account to which you can grant specific user-based access privileges. You want to use the most secure method that requires the fewest steps. What should you do? [Question 29]

	A. Create a signed URL with a four-hour expiration and share the URL with the company. [✔️]
	
	B. Set object access to 'public' and use object lifecycle management to remove the object after four hours.
	
	C. Configure the storage bucket as a static website and furnish the object's URL to the company. Delete the object from the storage bucket after four hours.
	
	D. Create a new Cloud Storage bucket specifically for the external company to access. Copy the object to that bucket. Delete the bucket after four hours have passed.

	-------

	The correct answer is A. Create a signed URL with a four-hour expiration and share the URL with the company.

	Explanation:

		Option A (Create a signed URL): This is the recommended and secure method for granting temporary access to an object in a Cloud Storage bucket. A signed URL includes a cryptographic signature that allows temporary access to the specified resource. By setting an expiration time, you can control how long the URL remains valid. This method is secure and does not require creating additional buckets or managing lifecycle policies.
		
		Options B, C, and D have significant drawbacks or are not suitable for secure, temporary access:

			Option B (Set object access to 'public' and use object lifecycle management): Making the object 'public' exposes it to anyone on the internet, which is not secure for sensitive data. Additionally, using object lifecycle management to delete the object after four hours might not be immediate or reliable.

			Option C (Configure the storage bucket as a static website): Configuring the storage bucket as a static website is not designed for secure, temporary access control. Also, deleting the object from the bucket after four hours may not be immediate or reliable.

			Option D (Create a new Cloud Storage bucket): Creating a new bucket and copying the object introduces unnecessary complexity. Additionally, deleting an entire bucket might have unintended consequences, and it's not a recommended method for providing temporary access to a single object.

		Therefore, option A is the recommended and secure method for providing temporary access to an object in a Cloud Storage bucket while maintaining control over access duration.

	-------

	Bard:

		The most appropriate approach to share the sensitive object securely with the external company while allowing access only for a limited duration (four hours) is A. Create a signed URL with a four-hour expiration and share the URL with the company.

			Signed URLs provide a time-bound and secure method for granting access to specific objects without compromising the object's overall security. By creating a signed URL with a four-hour expiration, the company can access the object within that timeframe, and the URL will become invalid after four hours, preventing unauthorized access to the sensitive data.

		Option B, setting object access to 'public' and using object lifecycle management to remove the object after four hours, is not secure as it allows anyone to access the object for the entire four hours.

		Option C, configuring the storage bucket as a static website and furnishing the object's URL to the company, defeats the purpose of protecting sensitive data as the object becomes publicly accessible through the static website.

		Option D, creating a new Cloud Storage bucket specifically for the external company and copying the object to that bucket, is not the most secure or efficient approach as it involves managing an additional bucket that needs to be deleted after four hours.

		Therefore, creating a signed URL with a four-hour expiration and sharing the URL with the external company is the most secure and streamlined method for granting temporary access to the sensitive object without compromising its confidentiality or requiring additional infrastructure management.

-------

You need to verify that a Google Cloud Platform service account was created at a particular time. What should you do?
[Question 86]
	
	A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.
	[✔️]
	
	B. Filter the Activity log to view the Configuration category. Filter the Resource type to Google Project.
	
	C. Filter the Activity log to view the Data Access category. Filter the Resource type to Service Account.
	
	D. Filter the Activity log to view the Data Access category. Filter the Resource type to Google Project.

	-------

	The correct answer is A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.

	Explanation:

		Option A (Filter the Activity log to view the Configuration category): The creation of a service account is considered a configuration event. By filtering the Activity log to the Configuration category and specifying the Resource type as Service Account, you can identify the creation event for the service account.
		
		Options B, C, and D are not the most appropriate for verifying the creation time of a service account:

		Option B (Filter the Activity log to view the Configuration category. Filter the Resource type to Google Project): This option is more focused on project-level configuration changes, and it may not provide specific details about service account creation.

		Option C (Filter the Activity log to view the Data Access category. Filter the Resource type to Service Account): The Data Access category is generally related to actions involving data, not service account creation. Filtering by Data Access may not yield relevant information about service account creation.

		Option D (Filter the Activity log to view the Data Access category. Filter the Resource type to Google Project): Similar to option C, filtering by Data Access and Google Project is not likely to provide information specifically about the creation time of a service account.

		Therefore, option A is the recommended choice for verifying the creation time of a Google Cloud Platform service account by filtering the Activity log to the Configuration category and specifying the Resource type as Service Account.

	-------

	Bard:

		The most appropriate approach to verify that a Google Cloud Platform (GCP) service account was created at a particular time is A. Filter the Activity log to view the Configuration category. Filter the Resource type to Service Account.

			The Activity log provides a history of events related to your GCP projects, including service account creation. By filtering the Activity log to the Configuration category and the Resource type to Service Account, you can view all events related to service account creation, including the time of creation.

		Option B, filtering the Activity log to the Configuration category and the Resource type to Google Project, would not provide the specific information needed to determine the creation time of a service account.

		Option C, filtering the Activity log to the Data Access category and the Resource type to Service Account, would provide information about data access events related to service accounts, but it would not specifically indicate the creation time of a service account.

		Option D, filtering the Activity log to the Data Access category and the Resource type to Google Project, would provide a broader view of data access events related to the project, but it would not specifically focus on service account creation.

		Therefore, filtering the Activity log to the Configuration category and the Resource type to Service Account is the most direct and accurate method for verifying the creation time of a GCP service account.

-------

** Your company runs its Linux workloads on Compute Engine instances. Your company will be working with a new operations partner that does not use Google
Accounts. You need to grant access to the instances to your operations partner so they can maintain the installed tooling. What should you do? [Question 152]
	
	A. Enable Cloud IAP for the Compute Engine instances, and add the operations partner as a Cloud IAP Tunnel User.
	[✔️]
	
	B. Tag all the instances with the same network tag. Create a firewall rule in the VPC to grant TCP access on port 22 for traffic from the operations partner to instances with the network tag. [Suggested and chatGPT suggested]
	
	C. Set up Cloud VPN between your Google Cloud VPC and the internal network of the operations partner.
	
	D. Ask the operations partner to generate SSH key pairs, and add the public keys to the VM instances.

	-------

	Bard:

		The most appropriate approach to grant access to the Compute Engine instances to your operations partner for maintaining the installed tooling is A. Enable Cloud IAP for the Compute Engine instances, and add the operations partner as a Cloud IAP Tunnel User.

			Cloud Identity-Aware Proxy (Cloud IAP) Tunnel provides secure and controlled access to Compute Engine instances from external networks, including those of operations partners that don't use Google Accounts. By enabling Cloud IAP and adding the operations partner as a Cloud IAP Tunnel User, you can grant granular access to specific instances without exposing them to the public internet.

		Option B, tagging all the instances with the same network tag and creating a firewall rule to grant access to those instances, is less secure and doesn't provide centralized management of access permissions.

		Option C, setting up Cloud VPN between your Google Cloud VPC and the internal network of the operations partner, is not the most suitable option in this scenario as it involves configuring VPN gateways and routing between networks, which can be complex and time-consuming.

		Option D, asking the operations partner to generate SSH key pairs and add the public keys to the VM instances, is a less secure approach as it involves sharing SSH keys with external partners.

		Therefore, enabling Cloud IAP for the Compute Engine instances and adding the operations partner as a Cloud IAP Tunnel User provides a secure, controlled, and centralized method for granting access to the instances while maintaining compliance with security best practices.

-------

** You are migrating a production-critical on-premises application that requires 96 vCPUs to perform its task. You want to make sure the application runs in a similar environment on GCP. What should you do? [Question 89]
	
	A. When creating the VM, use machine type n1-standard-96. [✔️]
	
	B. When creating the VM, use Intel Skylake as the CPU platform. [Bard Suggested]
	
	C. Create the VM using Compute Engine default settings. Use gcloud to modify the running instance to have 96 vCPUs.
	
	D. Start the VM using Compute Engine default settings, and adjust as you go based on Rightsizing Recommendations.
	[Bard Suggested]

	-------

	Option A (Use machine type n1-standard-96): When creating a VM, you can specify the machine type, and using n1-standard-96 directly sets the VM to have 96 vCPUs. This is a straightforward and explicit way to ensure that the VM has the required number of vCPUs.
	
	Options B, C, and D have drawbacks or are not the most suitable for ensuring the application runs with 96 vCPUs:

		Option B (Use Intel Skylake as the CPU platform): Specifying the CPU platform may not guarantee the exact number of vCPUs, and it might limit flexibility. Also, the specific CPU platform may not be necessary for achieving the desired vCPU count.

	Option C (Create the VM using default settings and modify with gcloud): While it's possible to modify the VM with gcloud after creation, it's generally more straightforward and recommended to set the required configuration during the VM creation process.

	Option D (Start the VM with default settings and adjust based on Rightsizing Recommendations): While Rightsizing Recommendations can provide insights into optimizing resources, if the requirement is known in advance (96 vCPUs), it's more efficient to specify the machine type directly during VM creation.

	Therefore, option A is the recommended choice for ensuring that the application runs with 96 vCPUs in a GCP environment.

	-------

	Bard:

		The most appropriate approach to ensure that the production-critical on-premises application runs in a similar environment on GCP with 96 vCPUs is B. When creating the VM, use Intel Skylake as the CPU platform.

			Using Intel Skylake as the CPU platform for the Compute Engine instance ensures that the instance has the same CPU architecture as the on-premises environment, which can minimize performance differences and potential compatibility issues. Selecting a different CPU platform, such as AMD or newer Intel generations, might introduce performance variations or require additional tuning.

		Option A, using machine type n1-standard-96, provides 96 vCPUs but doesn't guarantee compatibility with the on-premises CPU architecture.

		Option C, creating the VM with default settings and modifying the running instance to have 96 vCPUs, can be error-prone and potentially disrupt the application's performance.

		Option D, starting the VM with default settings and adjusting based on Rightsizing Recommendations, is a reactive approach that might not guarantee the application's performance requirements are met.

		Therefore, using Intel Skylake as the CPU platform when creating the Compute Engine instance provides a more proactive approach to ensuring the application runs in a similar environment to the on-premises setup.

-------

* You need to host an application on a Compute Engine instance in a project shared with other teams. You want to prevent the other teams from accidentally causing downtime on that application. Which feature should you use?
[Question 101]
	
	A. Use a Shielded VM.
	
	B. Use a Preemptible VM.
	
	C. Use a sole-tenant node. [✔️]
	
	D. Enable deletion protection on the instance. [Bard Suggested]

	-------

	Option D (Enable deletion protection on the instance): Enabling deletion protection on the instance prevents accidental deletion of the Compute Engine instance. This can help avoid downtime caused by inadvertent actions from other teams or users. With deletion protection enabled, additional confirmation is required before the instance can be deleted.
	
	Options A, B, and C have different use cases or are not as directly related to preventing accidental downtime:

	Option A (Use a Shielded VM): Shielded VMs provide additional security features, but they are more focused on protecting the VM's integrity and ensuring that it runs trusted code. While this enhances security, it may not directly prevent accidental downtime.

	Option B (Use a Preemptible VM): Preemptible VMs are short-lived, and they are not suitable for hosting critical applications that require continuous availability. They are cost-effective for certain use cases but are not designed for sustained, reliable application hosting.

	Option C (Use a sole-tenant node): Sole-tenant nodes provide dedicated hardware for VMs, but they are more about isolation and performance. They may not directly prevent accidental downtime caused by actions such as deletion.

	Therefore, option D (Enable deletion protection on the instance) is the recommended choice for preventing accidental downtime on a Compute Engine instance in a shared project.

	-------

	Bard:

		The most appropriate feature to prevent other teams from accidentally causing downtime on an application hosted on a Compute Engine instance in a shared project is D. Enable deletion protection on the instance.

			Enabling deletion protection on a Compute Engine instance prevents other users in the shared project from deleting the instance, which would cause the application to become unavailable. This feature helps to ensure that the application's uptime is protected from accidental or unauthorized actions.

		Option A, using a Shielded VM, enhances the security of the instance by providing protection against hardware-based attacks. However, it doesn't prevent accidental deletion by other users.

		Option B, using a Preemptible VM, offers cost-savings but can be terminated at any time by Google to accommodate other users' workloads. This could lead to downtime for the application.

		Option C, using a sole-tenant node, isolates the instance from other users in the project, providing a higher level of isolation and protection. However, it is more expensive than using a regular Compute Engine instance.

		Therefore, enabling deletion protection on the Compute Engine instance is the most effective and straightforward method to safeguard the application from accidental deletion by other users and ensure its continued availability.

-------

** You create a Deployment with 2 replicas in a Google Kubernetes Engine cluster that has a single preemptible node pool. After a few minutes, you use kubectl to examine the status of your Pod and observe that one of them is still in Pending status: [Question 94]

What is the most likely cause?
	
	A. The pending Pod's resource requests are too large to fit on a single node of the cluster.
	
	B. Too many Pods are already running in the cluster, and there are not enough resources left to schedule the pending Pod. [✔️]
	
	C. The node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod.
	
	D. The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node.
	[Bard Suggested and chatGPT Suggested] [✔️]

	-------

	Bard:

		The most likely cause of the pending Pod being in Pending status is D. The pending Pod was originally scheduled on a node that has been preempted between the creation of the Deployment and your verification of the Pods' status. It is currently being rescheduled on a new node.

			Preemptible nodes are subject to termination at any time by Google to accommodate other users' workloads. If the node that the pending Pod was originally scheduled on was preempted, then the Pod would be evicted and Kubernetes would attempt to reschedule it on another node. This process can take a few minutes, and during this time, the Pod will be in Pending status.

		Option A, the resource requests are too large to fit on a single node of the cluster, is less likely because the Deployment has only 2 replicas and the preemptible node pools are typically provisioned with enough resources to accommodate multiple Pods.

		Option B, too many Pods are already running in the cluster, is also less likely because the preemptible node pools are designed to handle a burst of traffic and should be able to accommodate the additional Pod.

		Option C, the node pool is configured with a service account that does not have permission to pull the container image used by the pending Pod, is also less likely because the service account is typically configured with the necessary permissions to pull container images.

		Therefore, the most probable explanation for the pending Pod being in Pending status is that it was originally scheduled on a preempted node and is currently being rescheduled on a new node.

-------

You have a workload running on Compute Engine that is critical to your business. You want to ensure that the data on the boot disk of this workload is backed up regularly. You need to be able to restore a backup as quickly as possible in case of disaster. You also want older backups to be cleaned automatically to save on cost. You want to follow Google-recommended practices. What should you do? [Question 112]
	
	A. Create a Cloud Function to create an instance template.
	
	B. Create a snapshot schedule for the disk using the desired interval. [✔️]
	
	C. Create a cron job to create a new disk from the disk using gcloud.
	
	D. Create a Cloud Task to create an image and export it to Cloud Storage
	
	-------

	The correct answer is B. Create a snapshot schedule for the disk using the desired interval.

	Explanation:

		Option B (Create a snapshot schedule): Google Cloud Platform provides the ability to create scheduled snapshots for persistent disks. Snapshots are a point-in-time copy of the disk, providing a backup that can be used for disaster recovery. Creating a snapshot schedule allows you to automate the process of regular backups, ensuring that you have recent copies of your data. Additionally, snapshots can be used to create new disks or images.
	
	Options A, C, and D are not the most suitable for creating and managing backups:

		Option A (Create a Cloud Function to create an instance template): While Cloud Functions can be used for automation, creating an instance template may not directly address the backup needs. Instance templates are more focused on defining the properties of virtual machine instances.

		Option C (Create a cron job to create a new disk): Manually creating new disks on a schedule using a cron job may not be as efficient and automated as using scheduled snapshots. Snapshots are a more streamlined and integrated solution for backup purposes.

		Option D (Create a Cloud Task to create an image and export it to Cloud Storage): While Cloud Tasks can be used for managing tasks, using it for the specific purpose of creating an image and exporting it to Cloud Storage may involve more manual steps compared to using scheduled snapshots.

	Therefore, option B is the recommended choice for creating and managing backups of the boot disk on Compute Engine to ensure disaster recovery and cost-effective backup management.

-------

** You are using Deployment Manager to create a Google Kubernetes Engine cluster. Using the same Deployment Manager deployment, you also want to create a DaemonSet in the kube-system namespace of the cluster. You want a solution that uses the fewest possible services. What should you do? [Question 69]
	
	A. Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet.
	[✔️]
	
	B. Use the Deployment Manager Runtime Configurator to create a new Config resource that contains the DaemonSet definition. [Bard Suggested]
	
	C. With Deployment Manager, create a Compute Engine instance with a startup script that uses kubectl to create the DaemonSet.
	
	D. In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value. [chatGPT Suggested]

	-------

	Bard:

		The correct answer is B. Use the Deployment Manager Runtime Configurator to create a new Config resource that contains the DaemonSet definition.

			This is the most efficient approach because it utilizes the Runtime Configurator's ability to define and execute arbitrary configurations, allowing you to embed the DaemonSet definition directly within the Deployment Manager deployment. This eliminates the need to create an additional service, such as a Compute Engine instance or a custom Type Provider, and simplifies the deployment process.

		The other options are less efficient or require additional services:

			A. Add the cluster's API as a new Type Provider in Deployment Manager, and use the new type to create the DaemonSet. While this would allow you to manage the DaemonSet within the Deployment Manager framework, it would involve creating and maintaining a custom Type Provider, which adds complexity and maintenance overhead.

			C. With Deployment Manager, create a Compute Engine instance with a startup script that uses kubectl to create the DaemonSet. This approach requires creating an additional Compute Engine instance and maintaining the startup script, adding more resources and complexity to the deployment.

			D. In the cluster's definition in Deployment Manager, add a metadata that has kube-system as key and the DaemonSet manifest as value. This approach simply embeds the DaemonSet manifest directly in the cluster definition, but it does not take advantage of the Runtime Configurator's ability to define and execute configurations. This can lead to challenges in managing and updating the DaemonSet.

		Therefore, using the Deployment Manager Runtime Configurator to encapsulate the DaemonSet definition within the Deployment Manager deployment provides the most efficient and streamlined approach for managing both the cluster and the DaemonSet within a single deployment unit.

-------

** You created a Google Cloud Platform project with an App Engine application inside the project. You initially configured the application to be served from the us- central region. Now you want the application to be served from the asia-northeast1 region. What should you do? [Question 37]
	
	A. Change the default region property setting in the existing GCP project to asia-northeast1.
	
	B. Change the region property setting in the existing App Engine application from us-central to asia-northeast1.
	
	C. Create a second App Engine application in the existing GCP project and specify asia-northeast1 as the region to serve your application. [chatGPT Suggested]
	
	D. Create a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application. [✔️]

	-------

	Bard:

		The correct answer is D. Create a new GCP project and create an App Engine application inside this new project. Specify asia-northeast1 as the region to serve your application.

		App Engine applications are tightly bound to the region they are created in, and it is not currently possible to change the region after an application has been created. Therefore, the only way to serve your application from the asia-northeast1 region is to create a new App Engine application in a new GCP project that is located in the asia-northeast1 region.

		The other options are incorrect:

			A. Change the default region property setting in the existing GCP project to asia-northeast1. This will only change the default region for new resources created in the project, and it will not affect the existing App Engine application.

			B. Change the region property setting in the existing App Engine application from us-central to asia-northeast1. This is not possible, as App Engine applications cannot be moved to a different region once they have been created.

			C. Create a second App Engine application in the existing GCP project and specify asia-northeast1 as the region to serve your application. This will create a second App Engine application in the same project, but it will still be served from the us-central1 region.

		Therefore, the only option that will allow you to serve your application from the asia-northeast1 region is to create a new App Engine application in a new GCP project that is located in the asia-northeast1 region.

-------

You have a Compute Engine instance hosting an application used between 9 AM and 6 PM on weekdays. You want to back up this instance daily for disaster recovery purposes. You want to keep the backups for 30 days. You want the Google-recommended solution with the least management overhead and the least number of services. What should you do? [Question 144]
	
	A. 1. Update your instances' metadata to add the following value: snapshot schedule: 0 1 * * * 2. Update your instances' metadata to add the following value: snapshot retention: 30
	
	B. 1. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. 2. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: - Schedule frequency: Daily - Start time: 1:00 AM 2:00 AM - Autodelete snapshots after: 30 days [✔️]
	
	C. 1. Create a Cloud Function that creates a snapshot of your instance's disk. 2. Create a Cloud Function that deletes snapshots that are older than 30 days. 3. Use Cloud Scheduler to trigger both Cloud Functions daily at 1:00 AM.
	
	D. 1. Create a bash script in the instance that copies the content of the disk to Cloud Storage. 2. Create a bash script in the instance that deletes data older than 30 days in the backup Cloud Storage bucket. 3. Configure the instance's crontab to execute these scripts daily at 1:00 AM.

	-------

	The appropriate answer is B. In the Cloud Console, go to the Compute Engine Disks page and select your instance's disk. In the Snapshot Schedule section, select Create Schedule and configure the following parameters: - Schedule frequency: Daily - Start time: 1:00 AM - Autodelete snapshots after: 30 days.

	Explanation:

		Option B (Create Snapshot Schedule): This option uses the native snapshot scheduling feature available in Google Cloud Platform. It involves configuring a snapshot schedule for the Compute Engine disk directly from the Cloud Console. This approach requires the least management overhead, as it is a built-in feature of Google Cloud, and it allows you to specify the frequency and retention period for automatic snapshots.
		
		Options A, C, and D introduce more manual or custom scripting and may involve more management overhead:

			Option A (Update instances' metadata): This option suggests updating the instances' metadata to create snapshots and set retention. While metadata can be used for custom configurations, using the built-in snapshot scheduling feature is a more straightforward and recommended approach.

			Option C (Create a Cloud Function): Creating a custom Cloud Function to handle snapshot creation and deletion introduces additional complexity. While Cloud Functions can be powerful, using native features is often more straightforward.

			Option D (Create a bash script): Using custom bash scripts on the instance for copying content to Cloud Storage and managing backups introduces more manual effort and maintenance. This approach also depends on the instance's local configurations.

		Therefore, option B is the recommended solution for creating daily snapshots with a retention period of 30 days, offering the least management overhead and utilizing a built-in feature of Google Cloud Platform.

-------

** Your organization has three existing Google Cloud projects. You need to bill the Marketing department for only their Google Cloud services for a new initiative within their group. What should you do? [Question 197]
	
	A. 1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud Project for the Marketing department. 2. Link the new project to a Marketing Billing Account. [✔️]
	
	B. 1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project. [Bard Suggested] [✔️]
	
	C. 1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Link the new project to a Marketing Billing Account. [Another Most Voted and chatGPT suggested]
	
	D. 1. Verify that you are assigned the Organization Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project.

	-------

	The most appropriate answer to bill the Marketing department separately for their Google Cloud services is 

		B. 1. Verify that you are assigned the Billing Administrator IAM role for your organization's Google Cloud account. 2. Create a new Google Cloud Project for the Marketing department. 3. Set the default key-value project labels to department:marketing for all services in this project.

	Here's a breakdown of why this option is the most suitable:

		Verify Billing Administrator Role: 

			Ensuring you have the Billing Administrator IAM role ensures you have the necessary permissions to manage billing accounts, projects, and billing-related resources. This is crucial for setting up separate billing for the Marketing department.

		Create a Dedicated Project: 

			Creating a new Google Cloud project specifically for the Marketing department provides them with a dedicated space to manage and deploy resources related to their initiative. This isolation prevents their usage from being mixed with other departments, enabling separate billing.

		Apply Default Project Labels: 

			Setting the default key-value project label 'department:marketing' associates all usage within this project with the 'Marketing' department. This ensures that the usage is correctly attributed to the appropriate department for accurate cost allocation.

	The other options either lack the necessary steps or involve unnecessary or less efficient methods:

		Option A: This option suggests linking the new project to a Marketing Billing Account. However, this step is unnecessary if you already have a designated billing account for the Marketing department.

		Option C: This option suggests creating a new project for the Marketing department and then linking it to a separate billing account. While this approach works, it involves creating the project twice, which is redundant.

		Option D: This option suggests creating a new project for the Marketing department and applying the label 'department:marketing' to all services. However, it fails to address the crucial step of verifying the Billing Administrator role, which is essential for managing billing accounts.

	Therefore, Option B, which includes all the necessary steps and ensures proper billing attribution, is the most appropriate choice for billing the Marketing department separately for their Google Cloud services.

-------

* Every employee of your company has a Google account. Your operational team needs to manage a large number of instances on Compute Engine. Each member of this team needs only administrative access to the servers. Your security team wants to ensure that the deployment of credentials is operationally efficient and must be able to determine who accessed a given instance. What should you do? [Question 1]
	
	A. Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance.
	
	B. Ask each member of the team to generate a new SSH key pair and to send you their public key. Use a configuration management tool to deploy those keys on each instance.
	
	C. Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the "compute.osAdminLogin" role to the Google group corresponding to this team. [✔️]
	
	D. Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance. [Suggested]
	
	-------

	The recommended approach for managing SSH access on Compute Engine instances is to use Google Cloud Identity-Aware Proxy (IAP) along with Cloud IAM roles. Option C closely aligns with this best practice:

	Option C (Ask each member of the team to generate a new SSH key pair and add the public key to their Google account. Grant the "compute.osAdminLogin" role to the Google group corresponding to this team): This option is aligned with Google IAP and IAM best practices. By associating SSH keys with Google accounts and using the "compute.osAdminLogin" role, you can provide administrative access while allowing IAP to control access to the instances.
	
	Explanation of why other options are not ideal:

		Option A (Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key in the metadata of each instance): This approach involves manually distributing private keys, which is not recommended for security reasons. It lacks the centralized control provided by IAM roles and IAP.

		Option B (Ask each member of the team to generate a new SSH key pair and send you their public key. Use a configuration management tool to deploy those keys on each instance): While using a configuration management tool is a step in the right direction for automation, managing SSH keys directly on instances is less secure and less scalable compared to using IAM roles and IAP.

		Option D (Generate a new SSH key pair. Give the private key to each member of your team. Configure the public key as a project-wide public SSH key in your Cloud Platform project and allow project-wide public SSH keys on each instance): Similar to Option A, this approach involves manually distributing private keys and lacks the centralized control provided by IAM roles and IAP.

	In summary, Option C is the recommended choice as it leverages Google IAP and IAM for secure and centralized access management.

	-------

	Bard:

		The most appropriate solution to manage SSH keys for the operational team while ensuring operational efficiency and auditability is C. Ask each member of the team to generate a new SSH key pair and to add the public key to their Google account. Grant the "compute.osAdminLogin" role to the Google group corresponding to this team.

		This approach offers several advantages:

			Operational Efficiency: 

				It allows each team member to manage their own SSH keys securely within their Google account. This simplifies key management and reduces the risk of mishandling or losing private keys.

			Auditability: 

				Adding the public keys to Google accounts enables audit trails of who accessed each instance. This fulfills the security team's requirement to track user access.

			Granular Access Control: 

				Granting the "compute.osAdminLogin" role to the Google group associated with the operational team ensures that only authorized members have administrative access to the instances.

			Centralized Management: 

				Managing public keys centrally within Google accounts eliminates the need to deploy keys manually to each instance, simplifying credential management.

		In contrast, the other options have limitations:

			Option A: Distributing private keys to team members directly poses a security risk due to the potential loss or misuse of these sensitive keys.

			Option B: Relying on a configuration management tool to deploy public keys from each team member introduces the complexity of managing multiple public keys and adds another layer of potential error.

			Option D: Using project-wide public SSH keys grants broader access permissions to all members of the project, which may not be desirable for administrative access to instances.

		Therefore, option C, which combines user-managed keys with granular role-based access control and centralized management, aligns with the operational efficiency, auditability, and security requirements of the situation.

-------

** You received a JSON file that contained a private key of a Service Account in order to get access to several resources in a Google Cloud project. You downloaded and installed the Cloud SDK and want to use this private key for authentication and authorization when performing gcloud commands. What should you do? [Question 170]
	
	A. Use the command gcloud auth login and point it to the private key.
	
	B. Use the command gcloud auth activate-service-account and point it to the private key. [✔️]
	
	C. Place the private key file in the installation directory of the Cloud SDK and rename it to credentials.json.
	
	D. Place the private key file in your home directory and rename it to GOOGLE_APPLICATION_CREDENTIALS. 
	[Bard Suggested] [✔️]

	-------

	Option B (Use the command gcloud auth activate-service-account and point it to the private key): This is the correct command to activate a service account with a private key for authentication and authorization when using gcloud commands.
	
	Explanation:

		Option A (Use the command gcloud auth login and point it to the private key): The gcloud auth login command is used for user account authentication, not service account authentication. It interacts with the user's browser for interactive login.

		Option C (Place the private key file in the installation directory of the Cloud SDK and rename it to credentials.json): Placing the private key directly in the installation directory and renaming it to credentials.json is not the standard approach for service account authentication. The recommended method is to specify the path to the private key using the gcloud auth activate-service-account command.

		Option D (Place the private key file in your home directory and rename it to GOOGLE_APPLICATION_CREDENTIALS): The GOOGLE_APPLICATION_CREDENTIALS environment variable is used to specify the path to the service account key file. While this is a valid option, it doesn't involve renaming the file to GOOGLE_APPLICATION_CREDENTIALS; rather, you set the environment variable to the path of the private key file.

	In summary, Option B is the correct and standard way to activate a service account with a private key for gcloud commands.

	-------

	Bard:

		The correct answer is D. Place the private key file in your home directory and rename it to GOOGLE_APPLICATION_CREDENTIALS.

			To use a service account private key for authentication and authorization when performing gcloud commands, you can place the private key file in your home directory and rename it to GOOGLE_APPLICATION_CREDENTIALS. This will automatically load the private key when you run gcloud commands.

		The other options are incorrect:

			A. Use the command gcloud auth login and point it to the private key. This command is used to login to Google Cloud with a user account, not to authenticate with a service account.

			B. Use the command gcloud auth activate-service-account and point it to the private key. This command is used to activate a service account, but it does not automatically load the private key. You would still need to call the gcloud auth login command or set the environment variable GOOGLE_APPLICATION_CREDENTIALS.

			C. Place the private key file in the installation directory of the Cloud SDK and rename it to credentials.json. This will also work, but it is not the recommended way to do it. The GOOGLE_APPLICATION_CREDENTIALS environment variable is more secure because it is not exposed to the filesystem.

		Therefore, the best way to use a service account private key for authentication and authorization with gcloud commands is to place the private key file in your home directory and rename it to GOOGLE_APPLICATION_CREDENTIALS.

-------

** You want to deploy an application on Cloud Run that processes messages from a Cloud Pub/Sub topic. You want to follow Google-recommended practices. What should you do? [Question 64]
	
	A. 1. Create a Cloud Function that uses a Cloud Pub/Sub trigger on that topic. 2. Call your application on Cloud Run from the Cloud Function for every message.
	
	B. 1. Grant the Pub/Sub Subscriber role to the service account used by Cloud Run. 2. Create a Cloud Pub/Sub subscription for that topic. 3. Make your application pull messages from that subscription. [chatGPT Suggested]
	
	C. 1. Create a service account. 2. Give the Cloud Run Invoker role to that service account for your Cloud Run application. 3. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint. [✔️]
	
	D. 1. Deploy your application on Cloud Run on GKE with the connectivity set to Internal. 2. Create a Cloud Pub/Sub subscription for that topic. 3. In the same Google Kubernetes Engine cluster as your application, deploy a container that takes the messages and sends them to your application. [Suggested]

	-------

	Bard:

		The most appropriate and Google-recommended approach for deploying an application on Cloud Run that processes messages from a Cloud Pub/Sub topic is C. Create a service account. 2. Give the Cloud Run Invoker role to that service account for your Cloud Run application. 3. Create a Cloud Pub/Sub subscription that uses that service account and uses your Cloud Run application as the push endpoint.

		This approach offers several advantages:

			Efficient Processing: 

				The Cloud Pub/Sub service will push messages directly to your Cloud Run application using a dedicated service account, eliminating the need for your application to actively pull messages from the subscription. This improves efficiency and reduces the load on your application.

			Reduced Operational Overhead: 

				This approach simplifies deployment and management, as you don't need to maintain a separate script or service to invoke your Cloud Run application for each message.

			Granular Access Control: 

				By granting the Cloud Run Invoker role to the service account, you ensure that only authorized messages can trigger your Cloud Run application, enhancing security and preventing unauthorized access.

		The other options have limitations:

			Option A: Using a Cloud Function to trigger your Cloud Run application for each message introduces additional overhead and latency compared to the direct push approach.

			Option B: Granting the Pub/Sub Subscriber role to the Cloud Run service account requires your application to pull messages from the subscription, which can increase load and complexity.

			Option D: Deploying your application on Cloud Run on GKE with internal connectivity limits the scalability and flexibility of your deployment. It also requires managing an additional container for message forwarding.

		Therefore, option C, which utilizes Cloud Pub/Sub's push mechanism and leverages a dedicated service account with the Cloud Run Invoker role, aligns with the Google recommended practices for efficient, secure, and manageable message processing on Cloud Run.

-------

** You are given a project with a single Virtual Private Cloud (VPC) and a single subnetwork in the us-central1 region. There is a Compute Engine instance hosting an application in this subnetwork. You need to deploy a new instance in the same project in the europe-west1 region. This new instance needs access to the application. You want to follow Google-recommended practices. What should you do? [Question 54]
	
	A. 1. Create a subnetwork in the same VPC, in europe-west1. 2. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint. [✔️]
	
	B. 1. Create a VPC and a subnetwork in europe-west1. 2. Expose the application with an internal load balancer. 3. Create the new instance in the new subnetwork and use the load balancer's address as the endpoint. 
	[Bard and chatGPT suggested] [✔️]
	
	C. 1. Create a subnetwork in the same VPC, in europe-west1. 2. Use Cloud VPN to connect the two subnetworks. 3. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.
	
	D. 1. Create a VPC and a subnetwork in europe-west1. 2. Peer the 2 VPCs. 3. Create the new instance in the new subnetwork and use the first instance's private address as the endpoint.

	-------

	Bard:

		The most appropriate and Google-recommended approach for allowing the new instance in europe-west1 to access the application in us-central1 is B. 1. Create a VPC and a subnetwork in europe-west1. 2. Expose the application with an internal load balancer. 3. Create the new instance in the new subnetwork and use the load balancer's address as the endpoint.

		This approach offers several advantages:

			Secure Communication: 

				By exposing the application with an internal load balancer, you can restrict its access to authorized instances within the VPC. This enhances security and prevents unauthorized connections.

			Scalability and Flexibility: 

				Using an internal load balancer allows you to scale the application horizontally by adding more instances behind the load balancer. It also provides flexibility in managing the application's traffic distribution.

			Simplified Deployment: 

				By deploying the new instance in a separate VPC and subnetwork, you avoid the complexities of peering or VPN connections, which can be more difficult to manage and troubleshoot.

		The other options have limitations:

			Option A: Creating a subnetwork in the same VPC but different region introduces unnecessary complexity and potential security risks.

			Option C: Using Cloud VPN to connect the two subnetworks adds additional overhead and introduces latency due to the VPN tunnel.

			Option D: Peering the two VPCs is a secure method but requires configuring and managing the peering connection. It may be more complex and less scalable than using an internal load balancer.

		Therefore, option B, which utilizes an internal load balancer to expose the application securely and deploys the new instance in a separate VPC for simplicity and scalability, aligns with the Google recommended practices for VPC interconnection and application accessibility.

-------

You have a Google Cloud Platform account with access to both production and development projects. You need to create an automated process to list all compute instances in development and production projects on a daily basis. What should you do?
[Question 84]
	
	A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources. [✔️]
	
	B. Create two configurations using gsutil config. Write a script that sets configurations as active, individually. For each configuration, use gsutil compute instances list to get a list of compute resources.
	
	C. Go to Cloud Shell and export this information to Cloud Storage on a daily basis.
	
	D. Go to GCP Console and export this information to Cloud SQL on a daily basis.

	-------

	Option A (Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources): This is the most appropriate method for achieving the desired outcome. You can use gcloud configurations to manage settings for different environments (such as development and production) and then use gcloud commands to list compute instances for each configuration.
	
	Explanation:

		Option A (Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources): This option correctly uses gcloud configurations to manage different environments and provides a scriptable way to list compute instances for both development and production projects.

		Option B (Create two configurations using gsutil config. Write a script that sets configurations as active, individually. For each configuration, use gsutil compute instances list to get a list of compute resources): gsutil is primarily used for interacting with Cloud Storage, and using it for compute instances might not work as intended. The correct command is gcloud compute instances list for listing compute instances.

		Option C (Go to Cloud Shell and export this information to Cloud Storage on a daily basis): While Cloud Shell is a valid environment for running gcloud commands, exporting the information to Cloud Storage may not be necessary for simply listing compute instances.

		Option D (Go to GCP Console and export this information to Cloud SQL on a daily basis): Cloud SQL is a managed relational database service, and exporting compute instance information to Cloud SQL is not a suitable approach for this task.

	In summary, Option A is the recommended approach, utilizing gcloud configurations and gcloud commands to list compute instances for both development and production projects.

	-------

	Bard:

		The most appropriate and efficient approach to automate the process of listing all compute instances in development and production projects on a daily basis is A. Create two configurations using gcloud config. Write a script that sets configurations as active, individually. For each configuration, use gcloud compute instances list to get a list of compute resources.

		This approach offers several advantages:

			Script-driven Automation: 

				Creating a script allows for automating the process of setting configurations and retrieving instance information, eliminating manual intervention and ensuring consistent execution.

			Flexibility and Reusability: 

				The script can be easily modified to include additional commands or logic, making it adaptable to future requirements.

			Reduced Complexity: 

				Managing configurations using gcloud config is simpler compared to using gsutil config, which is primarily designed for managing Cloud Storage resources.

			Granular Access Control: 

				Utilizing separate configurations for development and production projects ensures that the script operates within the appropriate environment, preventing unauthorized access to resources.

		The other options are less suitable for this task:

			Option C: Exporting the instance information to Cloud Storage requires additional steps and manual intervention to process the exported data.

			Option D: Exporting the instance information to Cloud SQL involves creating and maintaining a database schema, which is unnecessary for simply listing instances.

		Therefore, option A, which utilizes a script-driven approach with gcloud config to automate the instance listing process, is the most efficient and streamlined solution.

-------

* Your auditor wants to view your organization's use of data in Google Cloud. The auditor is most interested in auditing who accessed data in Cloud Storage buckets. You need to help the auditor access the data they need. What should you do?
[Question 169]
	
	A. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage. [✔️]
	
	B. Assign the appropriate permissions, and then create a Data Studio report on Admin Activity Audit Logs.
	
	C. Assign the appropriate permissions, and then use Cloud Monitoring to review metrics.
	
	D. Use the export logs API to provide the Admin Activity Audit Logs in the format they want. [Suggested]

	-------

	Option A (Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage): This is the recommended approach for auditing who accessed data in Cloud Storage buckets. Enabling Data Access Logs allows you to track and review operations performed on Cloud Storage objects, and you can use the log viewer to filter and query the logs.
	
	Explanation:

		Option A (Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage): Enabling Data Access Logs for Cloud Storage buckets will provide detailed audit logs capturing who accessed the data. Using the log viewer, you can build queries to filter and analyze these logs specifically for Cloud Storage operations.

		Option B (Assign the appropriate permissions, and then create a Data Studio report on Admin Activity Audit Logs): Admin Activity Audit Logs are more focused on administrative actions and changes in Google Cloud, and they may not provide the level of detail needed for tracking data access in Cloud Storage.

		Option C (Assign the appropriate permissions, and then use Cloud Monitoring to review metrics): Cloud Monitoring is more focused on metrics and performance data, and it may not provide the granular details required for auditing who accessed data in Cloud Storage.

		Option D (Use the export logs API to provide the Admin Activity Audit Logs in the format they want): While exporting logs can be done using the export logs API, Admin Activity Audit Logs might not capture the specific data access details needed for Cloud Storage operations.

	In summary, Option A is the recommended approach for auditing data access in Cloud Storage by enabling Data Access Logs and using the log viewer for detailed queries and analysis.

	-------

	Bard:

		The most appropriate approach to help the auditor access the data they need is A. Turn on Data Access Logs for the buckets they want to audit, and then build a query in the log viewer that filters on Cloud Storage.

		This approach offers several advantages:

			Focused Auditing: 

				Turning on Data Access Logs for specific buckets provides a granular view of data access activities within those buckets, aligning with the auditor's specific interest.

			Direct Access to Logs: 

				Building a query in the log viewer enables the auditor to directly access and analyze the audit logs related to data access, facilitating in-depth scrutiny.

			Flexibility and Customization: 

				The log viewer allows for flexible filtering and data manipulation, enabling the auditor to tailor the analysis to their specific requirements.

			Real-time Data: 

				Data Access Logs are continuously updated, ensuring that the auditor has access to the most recent data access activities.

		The other options are less suitable for this task:

			Option B: Creating a Data Studio report on Admin Activity Audit Logs would provide broader information about user activity, but it may not be as focused on data access specifically within Cloud Storage buckets as the auditor needs.

			Option C: Using Cloud Monitoring to review metrics would not provide the granular level of detail about data access activities that the auditor requires.

			Option D: Exporting Admin Activity Audit Logs would provide the auditor with the raw data, but it would require them to manually analyze and process the logs, which could be time-consuming and error-prone.

		Therefore, option A, which focuses on Data Access Logs specifically for the buckets of interest and enables direct query and analysis in the log viewer, is the most efficient and effective solution for assisting the auditor in their data access auditing task.

-------

You want to verify the IAM users and roles assigned within a GCP project named my-project. What should you do?
[Question 34]
	
	A. Run gcloud iam roles list. Review the output section.
	
	B. Run gcloud iam service-accounts list. Review the output section.
	
	C. Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles. [✔️]
	
	D. Navigate to the project and then to the Roles section in the GCP Console. Review the roles and status.

	-------

	The correct option is:

		Option C (Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles): This is the most direct and user-friendly way to review IAM (Identity and Access Management) users and roles within a GCP project.
	
	Explanation:

		Option C (Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles): The IAM section in the GCP Console provides a graphical interface where you can easily review the members (users, groups, and service accounts) and their assigned roles within a GCP project.

		Option A (Run gcloud iam roles list. Review the output section): While gcloud commands can be used to list IAM roles, running gcloud iam roles list is focused on listing roles rather than directly showing the users assigned to those roles. It's more suitable for viewing available roles rather than reviewing specific user assignments.

		Option B (Run gcloud iam service-accounts list. Review the output section): This command is specific to listing service accounts, which are special types of accounts used by applications and services to access GCP resources. It does not provide a comprehensive view of all IAM users and their roles.

		Option D (Navigate to the project and then to the Roles section in the GCP Console. Review the roles and status): While navigating to the Roles section is useful for reviewing available roles, it doesn't directly show the members assigned to those roles. The IAM section provides a more user-friendly view of both roles and members.

	In summary, Option C is the recommended approach for reviewing IAM users and their assigned roles within a GCP project using the GCP Console.

	-------

	Bard:

		The most appropriate and efficient approach to verify the IAM users and roles assigned within a GCP project named my-project is C. Navigate to the project and then to the IAM section in the GCP Console. Review the members and roles.

		This approach offers several advantages:

			Visual Representation: 

				The IAM section provides a clear and organized visual representation of the IAM users, their associated roles, and the project resources they have access to.

			Granular Detail: 

				The IAM section allows for detailed inspection of individual users and their assigned roles, ensuring comprehensive visibility into the IAM configuration.

			Context-Aware Access: 

				The IAM section is within the context of the specific project, my-project, providing a focused view of the IAM structure for that project.

			Real-time Updates: 

				The IAM section reflects the current IAM configuration, ensuring that you are always viewing the most up-to-date information.

			Integrated Management: 

				The IAM section is part of the overall GCP Console, enabling you to manage other project aspects alongside IAM configuration.

		The other options are less suitable for this task:

			Option A: Running gcloud iam roles list provides a list of available roles in the project but does not show the specific users assigned to those roles.

			Option B: Running gcloud iam service-accounts list shows a list of service accounts in the project but does not provide information about the IAM roles assigned to those accounts.

			Option D: Navigate to the project and then to the Roles section in the GCP Console. Review the roles and status. While this does provide a view of roles, it does not show the specific users assigned to those roles.

		Therefore, option C, which utilizes the IAM section in the GCP Console to provide a visual and comprehensive overview of IAM users, roles, and project resources, is the most effective way to verify the IAM configuration for the project my-project.

-------

* Your company uses a large number of Google Cloud services centralized in a single project. All teams have specific projects for testing and development. The DevOps team needs access to all of the production services in order to perform their job. You want to prevent Google Cloud product changes from broadening their permissions in the future. You want to follow Google-recommended practices. What should you do? [Question 158]
	
	A. Grant all members of the DevOps team the role of Project Editor on the organization level.
	
	B. Grant all members of the DevOps team the role of Project Editor on the production project. [Suggested]
	
	C. Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the production project. [✔️]
	
	D. Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the organization level.

	-------

	Option C (Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the production project): Creating a custom role allows you to define specific permissions needed for the DevOps team without granting unnecessary privileges. Assigning the custom role at the production project level ensures that permissions are scoped appropriately.
	
	Explanation:

		Option C (Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the production project): This approach follows the principle of least privilege, allowing you to define a custom role tailored to the specific needs of the DevOps team. By assigning the custom role at the production project level, you ensure that the permissions are scoped to the necessary resources without granting broader access.

		Option A (Grant all members of the DevOps team the role of Project Editor on the organization level): This approach grants broad permissions across the entire organization, which may lead to unintended consequences and increases the risk of security issues. It's generally not recommended to assign overly permissive roles at the organization level.

		Option B (Grant all members of the DevOps team the role of Project Editor on the production project): While this approach is more scoped than Option A, granting the Project Editor role may still provide more permissions than needed for the DevOps team. Custom roles offer more granularity.

		Option D (Create a custom role that combines the required permissions. Grant the DevOps team the custom role on the organization level): Similar to Option A, assigning custom roles at the organization level provides broad permissions and may lead to unintended consequences. It's preferable to scope permissions at the project level.

	In summary, Option C is recommended as it allows for the creation of a custom role tailored to the DevOps team's specific needs and ensures that permissions are granted at the project level, minimizing the risk of unintended access.

	-------

	Bard:

		This approach offers the following advantages:

			Granular Access Control: 

				Creating a custom role ensures that the DevOps team only has the specific permissions they need to perform their job, preventing them from accidentally modifying or accessing unauthorized resources.

			Prevent Unintended Permissions Expansion: 

				By granting the custom role on the production project, you limit the impact of any future Google Cloud product changes that could inadvertently broaden permissions. This ensures that the DevOps team's access remains controlled.

			Centralized Management: 

				Managing permissions at the project level provides a centralized approach to control access to production resources, making it easier to track and audit permissions.

			Scalability and Flexibility: 

				As the DevOps team's requirements evolve, the custom role can be modified or new roles can be created to accommodate changes without affecting the broader organization-level permissions.

		The other options are less suitable for this task:

			Option A: Granting all members of the DevOps team the role of Project Editor on the organization level grants them excessive permissions, which could pose security risks.

			Option B: Granting all members of the DevOps team the role of Project Editor on the production project grants them excessive permissions, which could pose security risks and make it difficult to track their activities.

			Option D: Granting the DevOps team the custom role on the organization level allows them to access production resources from any project within the organization, which could increase the risk of unauthorized access.

		Therefore, option C, which utilizes a custom role with restricted permissions granted at the production project level, aligns with Google's recommended practices for granting secure and controlled access to production resources.

-------

** You need to create a new billing account and then link it with an existing Google Cloud Platform project. What should you do? [Question 35]
	
	A. Verify that you are Project Billing Manager for the GCP project. Update the existing project to link it to the existing billing account. [Another Most Voted]
	
	B. Verify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project. [✔️]
	
	C. Verify that you are Billing Administrator for the billing account. Create a new project and link the new project to the existing billing account.
	
	D. Verify that you are Billing Administrator for the billing account. Update the existing project to link it to the existing billing account.

	-------

	Bard:

		The most appropriate approach to create a new billing account and then link it with an existing Google Cloud Platform project is B. Verify that you are Project Billing Manager for the GCP project. Create a new billing account and link the new billing account to the existing project.

		This approach offers the following advantages:

			Proper Role Requirements: 

				Ensuring that you are the Project Billing Manager for the existing project grants you the necessary permissions to manage billing associations.

				New Billing Account Creation: 

					Creating a new billing account allows you to separate billing responsibilities and track costs associated with specific projects.

				Project-Billing Account Linking: 

					Linking the new billing account to the existing project connects the project's usage to the new billing account, ensuring accurate billing for project resources.

				Controlled Billing Association: 

					Linking the project to the new billing account maintains control over billing associations and prevents unauthorized access to billing information.

		The other options are less suitable for this task:

			Option A: Updating the existing project to link it to the existing billing account would not create a new billing account and would only associate the project with the existing billing account.

			Option C: Creating a new project and linking it to the existing billing account would not assign the new project to the new billing account.

			Option D: Updating the existing project to link it to the existing billing account would not create a new billing account and would only associate the project with the existing billing account.

		Therefore, option B, which involves creating a new billing account, linking it to the existing project, and ensuring the appropriate role permissions, is the most accurate and efficient approach to achieve the desired outcome.

-------

* Your organization needs to grant users access to query datasets in BigQuery but prevent them from accidentally deleting the datasets. You want a solution that follows Google-recommended practices. What should you do? [Question 102]
	
	A. Add users to roles/bigquery user role only, instead of roles/bigquery dataOwner.
	
	B. Add users to roles/bigquery dataEditor role only, instead of roles/bigquery dataOwner. [Suggested]
	
	C. Create a custom role by removing delete permissions, and add users to that role only.
	
	D. Create a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role. [✔️]

	-------

	A. Add users to roles/bigquery user role only, instead of roles/bigquery dataOwner.

		Reasoning: The roles/bigquery.user role provides basic permissions for querying datasets, but it does not specifically control delete actions. Users with this role might still have the ability to delete datasets if they have other roles with delete permissions.
	
	B. Add users to roles/bigquery dataEditor role only, instead of roles/bigquery dataOwner.

		Reasoning: While the roles/bigquery.dataEditor role allows users to edit and modify data, it doesn't explicitly prevent deletion. Users with this role could still delete datasets, and it might provide more permissions than necessary.
	
	C. Create a custom role by removing delete permissions, and add users to that role only.

		Reasoning: Creating a custom role with only the necessary permissions (excluding delete) is a good practice for fine-grained access control. Users assigned to this custom role would be able to query datasets but not delete them.
	
	D. Create a custom role by removing delete permissions. Add users to the group, and then add the group to the custom role.

		Reasoning: This is a valid approach and is similar to option C. Using a group can simplify management if you need to assign the same custom role to multiple users. However, for simplicity, directly adding users to the custom role might be more straightforward unless group management is beneficial.
	
	In summary, while options C and D are both suitable, option C might be more straightforward unless the use of groups for user management is specifically desired. Options A and B are less suitable because they do not address the prevention of dataset deletion effectively.

	-------

	Bard:

		The most appropriate and secure approach to grant users access to query datasets in BigQuery while preventing accidental dataset deletion is D. Create a custom role by removing delete permissions, add users to the group, and then add the group to the custom role.

		This approach offers several advantages:

			Granular Access Control: 

				Creating a custom role allows for fine-grained control over permissions, ensuring that users only have the ability to query datasets without deleting them.

				Prevent Accidental Deletions: 

					Removing delete permissions from the custom role prevents users from accidentally deleting datasets, safeguarding sensitive data.

				Centralized Management: 

					Managing permissions through a custom role simplifies the process of granting and revoking access, making it easier to maintain control over dataset access.

				Scalability and Flexibility: 

					As user requirements evolve, the custom role can be modified to adjust permissions without affecting the broader organization's access policies.

				Group-Based Management: 

					Adding users to a group and then granting the group the custom role simplifies the management of user permissions and reduces administrative overhead.

		The other options are less suitable for this task:

			Option A: Adding users to the roles/bigquery user role would allow them to query datasets but also potentially delete them, which is not the desired outcome.

			Option B: Adding users to the roles/bigquery dataEditor role would allow them to edit datasets, which includes the ability to delete them.

			Option C: While creating a custom role and removing delete permissions is a step in the right direction, directly adding users to the custom role can be cumbersome and inefficient. Using a group to manage user permissions simplifies the process.

		Therefore, option D, which involves creating a custom role with limited permissions, managing user access through a group, and adding users to the group, is the most secure and manageable approach to grant user access to BigQuery datasets while preventing accidental deletions.

-------

* You have a project for your App Engine application that serves a development environment. The required testing has succeeded and you want to create a new project to serve as your production environment. What should you do?
[Question 24]
	
	A. Use gcloud to create the new project, and then deploy your application to the new project. [✔️]
	
	B. Use gcloud to create the new project and to copy the deployed application to the new project.
	
	C. Create a Deployment Manager configuration file that copies the current App Engine deployment into a new project. 
	[Bard and chatGPT Suggested] [✔️]
	
	D. Deploy your application again using gcloud and specify the project parameter with the new project name to create the new project.

	-------

	Bard:

		The most appropriate approach to create a new project for your production environment is C. Create a Deployment Manager configuration file that copies the current App Engine deployment into a new project.

		This approach offers several advantages:

			Infrastructure Automation: 

				Deployment Manager simplifies the process of deploying and managing infrastructure resources, including App Engine deployments.

			Consistent Environment: 

				Deployment Manager ensures that the production environment is an exact replica of the development environment, ensuring consistency and reliability.

			Reusable Configuration: 

				The Deployment Manager configuration file can be reused for future deployments, saving time and effort.

			Version Control: 

				The Deployment Manager configuration file can be stored in a version control system, enabling traceability and rollbacks.

			Scalability and Flexibility: 

				Deployment Manager supports scaling and modifications to the production environment as needed.

		The other options are less suitable for this task:

			Option A: Creating a new project and manually deploying the application is inefficient and error-prone.

			Option B: Copying the deployed application manually is time-consuming and may lead to discrepancies between the development and production environments.

			Option D: Deploying the application again using gcloud and specifying the project parameter with the new project name does not address the need for a consistent and automated deployment process.

		Therefore, option C, which leverages Deployment Manager for a consistent, automated, and reproducible deployment of the App Engine application to the production environment, is the most effective and efficient approach.

-------

* You have a Compute Engine instance hosting a production application. You want to receive an email if the instance consumes more than 90% of its CPU resources for more than 15 minutes. You want to use Google services. What should you do?
[Question 123]
	
	A. 1. Create a consumer Gmail account. 2. Write a script that monitors the CPU usage. 3. When the CPU usage exceeds the threshold, have that script send an email using the Gmail account and smtp.gmail.com on port 25 as SMTP server.
	
	B. 1. Create a Stackdriver Workspace, and associate your Google Cloud Platform (GCP) project with it. 2. Create an Alerting Policy in Stackdriver that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel. [✔️]
	
	C. 1. Create a Stackdriver Workspace, and associate your GCP project with it. 2. Write a script that monitors the CPU usage and sends it as a custom metric to Stackdriver. 3. Create an uptime check for the instance in Stackdriver.
	
	D. 1. In Stackdriver Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9] {1,3})% 2. In Stackdriver Monitoring, create an Alerting Policy based on this metric. 3. Configure your email address in the notification channel. [Suggested]

	-------

	B. Create a Stackdriver Workspace, and associate your Google Cloud Platform (GCP) project with it. Create an Alerting Policy in Stackdriver that uses the threshold as a trigger condition. Configure your email address in the notification channel.

		Recommended: This option leverages Google Cloud's native monitoring and alerting service, Stackdriver, to set up alerting policies based on CPU usage. It allows you to receive email notifications when the specified threshold is breached.
		
	A. Create a consumer Gmail account. Write a script that monitors the CPU usage. When the CPU usage exceeds the threshold, have that script send an email using the Gmail account and smtp.gmail.com on port 25 as SMTP server.

		Not recommended: Using a personal Gmail account for automated alerting is not best practice, and using SMTP to send emails is less integrated and potentially less reliable than using a native monitoring service.
	
	C. Create a Stackdriver Workspace, and associate your GCP project with it. Write a script that monitors the CPU usage and sends it as a custom metric to Stackdriver. Create an uptime check for the instance in Stackdriver.

		Not the most direct solution: While Stackdriver can be used to create custom metrics and alerts, setting up an uptime check might not directly address the CPU usage monitoring requirement.
	
	D. In Stackdriver Logging, create a logs-based metric to extract the CPU usage by using this regular expression: CPU Usage: ([0-9] {1,3})%. In Stackdriver Monitoring, create an Alerting Policy based on this metric. Configure your email address in the notification channel.

	Not as direct: This option involves creating a logs-based metric, which might be less straightforward than setting up an alert directly based on resource metrics.
	
	Therefore, option B is the most appropriate choice for setting up email alerts based on CPU usage in a Google Cloud project. It utilizes native Google Cloud services for monitoring and alerting.

	-------

	Bard:

		The most appropriate approach to receive an email if the instance consumes more than 90% of its CPU resources for more than 15 minutes using Google services is B. 1. Create a Stackdriver Workspace, and associate your Google Cloud Platform (GCP) project with it. 2. Create an Alerting Policy in Stackdriver that uses the threshold as a trigger condition. 3. Configure your email address in the notification channel.

		This approach offers several advantages:

			Centralized Monitoring: 

				Stackdriver Monitoring provides a centralized platform for monitoring CPU usage and other instance metrics.

			Real-time alerting: 

				Stackdriver Monitoring's alerting functionality triggers an email notification when the CPU usage exceeds the threshold, ensuring prompt action.

			Threshold-based alerting: 

				Specifying a threshold of 90% CPU usage for more than 15 minutes allows for targeted alerting and avoids unnecessary notifications.

			Comprehensive monitoring: 

				Stackdriver Monitoring provides a holistic view of the instance's health and performance, enabling proactive identification of potential issues.

			Integrated notification: 

				Stackdriver seamlessly integrates with Gmail to send email notifications, streamlining the alerting process.

		The other options are less suitable for this task:

			Option A: Using a consumer Gmail account for alerting is not recommended due to security concerns and lack of scalability.

			Option C: Sending custom metrics to Stackdriver and relying on uptime checks for alerting are less efficient and less granular than using Stackdriver Monitoring's alerting capabilities.

			Option D: Creating a logs-based metric and alerting on it requires additional complexity and may not provide the same level of flexibility as using Stackdriver Monitoring directly.

		Therefore, option B, which leverages Stackdriver Monitoring's built-in alerting functionality to monitor CPU usage and trigger email notifications when the threshold is exceeded, is the most effective and secure approach for this scenario.

-------

You have one project called proj-sa where you manage all your service accounts. You want to be able to use a service account from this project to take snapshots of VMs running in another project called proj-vm. What should you do?
[Question 36]
	
	A. Download the private key from the service account, and add it to each VMs custom metadata.
	
	B. Download the private key from the service account, and add the private key to each VM's SSH keys.
	
	C. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm. [✔️]
	
	D. When creating the VMs, set the service account's API scope for Compute Engine to read/write.
	
	C. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.

	-------

	Recommended: By granting the service account in the proj-sa project the IAM role of Compute Storage Admin in the proj-vm project, you provide the necessary permissions for the service account to take snapshots of VMs in the specified project.
	
	A. Download the private key from the service account and add it to each VM's custom metadata.

		Not recommended: Embedding the private key directly in VM metadata is a security risk, and it's generally not a good practice. Service accounts are designed to manage access, and granting roles is a better approach.
	
	B. Download the private key from the service account and add the private key to each VM's SSH keys.

		Not applicable: SSH keys are used for authentication, but for service accounts and VM snapshots, IAM roles are more relevant. SSH keys are not directly related to the ability to take VM snapshots.
	
	D. When creating the VMs, set the service account's API scope for Compute Engine to read/write.

		Not as relevant: API scopes are used to restrict the permissions of the default service account on a per-instance basis. However, since you are dealing with a specific service account, granting IAM roles is a more appropriate approach.
	
	Therefore, option C is the most appropriate choice for allowing a service account from proj-sa to take snapshots of VMs in proj-vm. It provides a controlled and granular way to manage permissions.

	-------

	Bard:

		The most appropriate approach to enable a service account from project proj-sa to take snapshots of VMs running in project proj-vm is C. Grant the service account the IAM Role of Compute Storage Admin in the project called proj-vm.

			This approach ensures that the service account has the necessary permissions to access and manage snapshots of VMs in project proj-vm. Granting the IAM Role of Compute Storage Admin provides the service account with the following capabilities:

				Create, read, update, and delete snapshots of Google Compute Engine instances.
				
				List snapshots in the project.
				
				Set the snapshot description.
		
		The other options are less suitable for this task:

			Option A: Downloading the private key and adding it to each VM's custom metadata is not recommended for security reasons. Exposing the private key in the metadata could compromise the security of the service account.

			Option B: Adding the private key to each VM's SSH keys is not the correct approach for granting permissions to take snapshots. The SSH keys are used for remote login and file transfer, not for administrative tasks like taking snapshots.

			Option D: Setting the service account's API scope for Compute Engine to read/write is not sufficient. The IAM Role of Compute Storage Admin provides more granular permissions for managing snapshots.

		Therefore, option C, which grants the service account the IAM Role of Compute Storage Admin in the project called proj-vm, is the most secure and efficient approach for enabling the service account to take snapshots of VMs in project proj-vm.

-------

Your finance team wants to view the billing report for your projects. You want to make sure that the finance team does not get additional permissions to the project. What should you do? [Question 99]

	A. Add the group for the finance team to roles/billing user role.
	
	B. Add the group for the finance team to roles/billing admin role.
	
	C. Add the group for the finance team to roles/billing viewer role. [✔️]
	
	D. Add the group for the finance team to roles/billing project/Manager role.

	-------

	C. Add the group for the finance team to roles/billing viewer role.

		Recommended: The roles/billing viewer role allows users to view the billing information without granting additional permissions that could affect the resources in the project.
	
	A. Add the group for the finance team to roles/billing user role.

		Not as suitable: The roles/billing user role includes permissions to view billing information, but it also includes additional permissions that might not be necessary for a team that only needs to view billing reports.
	
	B. Add the group for the finance team to roles/billing admin role.

		Not recommended: The roles/billing admin role provides full control over billing settings, including the ability to modify budgets and link and unlink projects, which might be more access than the finance team needs.
	
	D. Add the group for the finance team to roles/billing project/Manager role.

		Not a valid role: The role roles/billing project/Manager does not exist.
	
	Therefore, option C is the most appropriate choice if you want to grant the finance team the ability to view billing reports without giving them additional permissions that could impact other aspects of the project.

	-------

	Bard:

		The most appropriate approach for allowing the finance team to view the billing report for your projects while restricting their access to other project resources is C. Add the group for the finance team to roles/billing viewer role.

			Granting the finance team the IAM Role of Billing Viewer provides them with the following capabilities:

				View billing reports for projects in their billing account.
				
				See the billing details for specific projects.
				
				Analyze billing data to identify trends and optimize costs.
			
			This role ensures that the finance team has the necessary permissions to view billing information without granting them any administrative or resource-management capabilities.

		The other options are less suitable for this task:

			Option A: Adding the group to the roles/billing user role provides broader access to billing data and administrative capabilities.

			Option B: Adding the group to the roles/billing admin role grants excessive permissions, including the ability to modify billing settings and manage billing accounts.

			Option D: Adding the group to roles/billing project/Manager role provides access to project-level billing data and administrative capabilities, which is not necessary for simply viewing billing reports.

		Therefore, option C, which limits the finance team's permissions to the roles/billing viewer role, is the most secure and appropriate approach for fulfilling their access requirements.

-------

* You create a new Google Kubernetes Engine (GKE) cluster and want to make sure that it always runs a supported and stable version of Kubernetes. What should you do? [Question 39]
	
	A. Enable the Node Auto-Repair feature for your GKE cluster.
	
	B. Enable the Node Auto-Upgrades feature for your GKE cluster. [✔️]
	
	C. Select the latest available cluster version for your GKE cluster.
	
	D. Select Container-Optimized OS (cos) as a node image for your GKE cluster. [Bard Suggested] [✔️]

	-------

	B. Enable the Node Auto-Upgrades feature for your GKE cluster.

		Recommended: Enabling Node Auto-Upgrades ensures that the nodes in your GKE cluster are automatically upgraded to the latest supported and stable version of Kubernetes. This helps in keeping your cluster up-to-date with security patches and new features.
	
	A. Enable the Node Auto-Repair feature for your GKE cluster.

		Not directly related: Node Auto-Repair is focused on automatically repairing nodes that have been marked as unhealthy, rather than ensuring the Kubernetes version is up-to-date.
	
	C. Select the latest available cluster version for your GKE cluster.

		Not as automatic: While manually selecting the latest cluster version is an option, it requires manual intervention and may not be as hands-off as using Node Auto-Upgrades.
	
	D. Select "Container-Optimized OS (cos)" as a node image for your GKE cluster.

		Not directly related: The choice of the underlying OS image for nodes is not the primary mechanism for ensuring the Kubernetes version is stable and up-to-date.
	
	Therefore, option B is the most appropriate for ensuring that your GKE cluster runs a supported and stable version of Kubernetes by automatically upgrading the nodes.

	-------

	Bard:

		The most appropriate approach to ensure that a new Google Kubernetes Engine (GKE) cluster always runs a supported and stable version of Kubernetes is D. Select Container-Optimized OS (cos) as a node image for your GKE cluster.

			Container-Optimized OS (cos) is a lightweight, pre-configured Linux distribution optimized for running containerized applications on Google Kubernetes Engine (GKE). It comes with a guaranteed Kubernetes release cadence, ensuring that the cluster always runs a supported and stable version of Kubernetes.

		The other options are less suitable for this task:

			A. Enable the Node Auto-Repair feature for your GKE cluster: While Node Auto-Repair can help maintain the health of your cluster, it does not guarantee that the cluster will always run a supported and stable version of Kubernetes.

			B. Enable the Node Auto-Upgrades feature for your GKE cluster: Node Auto-Upgrades automates the upgrade process for GKE nodes, including kernel upgrades and container runtime upgrades. However, it does not guarantee that the upgraded version of Kubernetes will be supported and stable.

			C. Select the latest available cluster version for your GKE cluster: Selecting the latest available cluster version may not always ensure that the cluster is running a supported and stable version of Kubernetes. It is better to rely on a stable operating system like cos, which provides a guaranteed Kubernetes release cadence.

-------

** You need to set a budget alert for use of Compute Engineer services on one of the three Google Cloud Platform projects that you manage. All three projects are linked to a single billing account. What should you do? [Question 88]
	
	A. Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project. [✔️]
	
	B. Verify that you are the project billing administrator. Select the associated billing account and create a budget and a custom alert. [Suggested and chatGPT Suggested]
	
	C. Verify that you are the project administrator. Select the associated billing account and create a budget for the appropriate project.
	
	D. Verify that you are project administrator. Select the associated billing account and create a budget and a custom alert.

	-------

	C. Verify that you are the project administrator. Select the associated billing account and create a budget for the appropriate project.

		Not recommended: Project administrators may not have the necessary permissions to set billing-related budgets and alerts; it's typically a responsibility of the project billing administrator.

	D. Verify that you are the project administrator. Select the associated billing account and create a budget and a custom alert.

		Not recommended: Similar to option C, project administrators may not have the necessary permissions for setting billing-related budgets and alerts.

	-------

	Bard:

		The most appropriate approach for creating a budget alert for Compute Engine services on one of three projects in a single billing account is A. Verify that you are the project billing administrator. Select the associated billing account and create a budget and alert for the appropriate project.

			Setting a budget alert requires the necessary permissions to manage billing for the specific project. Verifying that you are the project billing administrator ensures that you have the authority to create and configure budgets for the project.

			After verifying your permissions, you can select the associated billing account and create a budget specifically for the project where you want to monitor Compute Engine usage. This ensures that the budget and alert are applied only to the intended project.

		The other options are not as appropriate for this task:

			B. Verify that you are the project billing administrator. Select the associated billing account and create a budget and a custom alert.

				Creating a custom alert requires additional configuration beyond setting a simple budget alert.

			C. Verify that you are the project administrator. Select the associated billing account and create a budget for the appropriate project.

				While being a project administrator grants you access to certain billing-related tasks, it may not be sufficient for creating a budget alert.

			D. Verify that you are project administrator. Select the associated billing account and create a budget and a custom alert.

				This option is similar to B, but it also includes creating a custom alert, which may not be necessary for this specific task.

		Therefore, option A, which involves verifying billing permissions, selecting the appropriate billing account, and creating a budget alert specifically for the target project, is the most straightforward and accurate approach to achieve the desired outcome.

-------

* You have a virtual machine that is currently configured with 2 vCPUs and 4 GB of memory. It is running out of memory. You want to upgrade the virtual machine to have 8 GB of memory. What should you do? [Question 49]
	
	A. Rely on live migration to move the workload to a machine with more memory.
	
	B. Use gcloud to add metadata to the VM. Set the key to required-memory-size and the value to 8 GB.
	
	C. Stop the VM, change the machine type to n1-standard-8, and start the VM. [Bard and chatGPT recommended] [✔️]
	
	D. Stop the VM, increase the memory to 8 GB, and start the VM. [✔️]

	-------

	Option C Recommended: This method allows you to change the machine type, including both the number of vCPUs and the amount of memory, without affecting the data on the VM. It ensures that the VM is allocated the desired resources when restarted.
	
	A. Rely on live migration to move the workload to a machine with more memory.

		Not applicable: Live migration can be used for certain scenarios, but it doesn't allow for changing the allocated resources (such as memory) on the current machine. It's more about moving the workload to another host without downtime.
	
	B. Use gcloud to add metadata to the VM. Set the key to required-memory-size and the value to 8 GB.

		Incorrect: Adding metadata doesn't change the actual allocated resources of the VM. It might be used for informational purposes but doesn't affect the underlying configuration.
	
	D. Stop the VM, increase the memory to 8 GB, and start the VM.

		Not directly possible: Changing the memory of a VM typically involves changing its machine type, which requires stopping and starting the VM. Option C provides a more straightforward way to do this.
	
	Therefore, option C is the recommended approach to upgrade the virtual machine to have 8 GB of memory.

	-------

	Bard:

		The most appropriate approach to upgrade the virtual machine's memory to 8 GB is C. Stop the VM, change the machine type to n1-standard-8, and start the VM.

			Stopping the VM before modifying its memory ensures that any data modifications or processes in progress are not interrupted, preventing potential data loss or application errors.

			Changing the machine type to n1-standard-8 updates the VM's configuration to allocate 8 GB of memory, effectively increasing its capacity.

			Starting the VM after completing the configuration changes ensures that the VM boots up with the updated memory allocation and can resume running applications and processes smoothly.

		The other options are not as appropriate for this task:

			A. Relying on live migration: Live migration is a technique for transferring a running virtual machine from one physical machine to another without interruption. However, it is not applicable for simply increasing a VM's memory allocation.

			B. Adding metadata: Adding metadata to the VM with the key required-memory-size and the value 8 GB does not directly modify the VM's memory allocation.

			D. Directly increasing the memory: Directly increasing the memory to 8 GB without stopping the VM could lead to data corruption or application instability.

-------

You are the team lead of a group of 10 developers. You provided each developer with an individual Google Cloud Project that they can use as their personal sandbox to experiment with different Google Cloud solutions. You want to be notified if any of the developers are spending above $500 per month on their sandbox environment. What should you do? [Question 156]
	
	A. Create a single budget for all projects and configure budget alerts on this budget.
	
	B. Create a separate billing account per sandbox project and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per billing account.
	
	C. Create a budget per project and configure budget alerts on all of these budgets. [✔️]
	
	D. Create a single billing account for all sandbox projects and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per project. [Bard Suggested]

	-------

	C. Create a budget per project and configure budget alerts on all of these budgets.

		Recommended: This approach allows you to set individual budgets and alerts for each developer's sandbox project. It provides fine-grained control and notifications tailored to the spending patterns of each developer.
	
	A. Create a single budget for all projects and configure budget alerts on this budget.

		Not recommended: This approach creates a collective budget for all projects, making it difficult to track individual spending patterns. The alerts would be triggered based on the cumulative spending across all sandbox projects.
	
	B. Create a separate billing account per sandbox project and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per billing account.

		Complex: While this approach allows for detailed analysis, it involves creating multiple billing accounts, which might be an overcomplication for managing individual sandbox projects. It also involves additional steps with BigQuery and Data Studio.
	
	D. Create a single billing account for all sandbox projects and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per project.

		Complex and less granular: Similar to option B, this approach involves unnecessary complexity with multiple billing accounts and might not provide the fine-grained control needed for individual sandbox projects.
		Therefore, option C is the most straightforward and effective solution for monitoring and setting budget alerts on individual spending for each developer's sandbox project.

	-------

	Bard:

		The most appropriate approach to monitor and alert on the spending of each developer's sandbox project is D. Create a single billing account for all sandbox projects and enable BigQuery billing exports. Create a Data Studio dashboard to plot the spending per project.

		This approach offers several advantages:

			Centralized Billing Management: 

				Creating a single billing account for all sandbox projects simplifies billing management and ensures that all usage is aggregated together.

			Detailed Billing Exports: 

				Enabling BigQuery billing exports generates detailed billing data for each project, providing granular insights into individual developer spending.

			Data Studio Dashboard: 

				Creating a Data Studio dashboard allows for visualizing the spending data from BigQuery, making it easy to identify developers who exceed the $500 threshold.

			Scalability and Flexibility: 

				As the team grows or new projects are added, the single billing account and Data Studio dashboard can accommodate the increased usage and provide consistent monitoring.

		The other options are less suitable for this scenario:

			Option A: Creating a single budget for all projects may not be effective for identifying individual developer spending.

			Option B: Creating separate billing accounts and Data Studio dashboards for each project would be overly complex and redundant.

			Option C: Creating a budget per project would require setting up multiple budgets and alerts, which can be time-consuming and error-prone.

-------

** You have designed a solution on Google Cloud Platform (GCP) that uses multiple GCP products. Your company has asked you to estimate the costs of the solution. You need to provide estimates for the monthly total cost. What should you do?
[Question 15]
	
	A. For each GCP product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each GCP product. [✔️]
	
	B. For each GCP product in the solution, review the pricing details on the products pricing page. Create a Google Sheet that summarizes the expected monthly costs for each product. [chatGPT suggested]
	
	C. Provision the solution on GCP. Leave the solution provisioned for 1 week. Navigate to the Billing Report page in the Google Cloud Platform Console. Multiply the 1 week cost to determine the monthly costs. [Suggested]
	
	D. Provision the solution on GCP. Leave the solution provisioned for 1 week. Use Stackdriver to determine the provisioned and used resource amounts. Multiply the 1 week cost to determine the monthly costs.

	-------

	Bard:

		The most appropriate approach to estimate the costs of a GCP solution that uses multiple GCP products is A. For each GCP product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each GCP product.

		This approach offers several advantages:

			Accuracy: 

				Estimating costs based on pricing details and the pricing calculator provides a reliable and accurate assessment of monthly costs.

			Granularity: 

				Evaluating each GCP product individually ensures that all potential costs are considered and accounted for.

			Scalability: 

				This approach can be easily adapted to different pricing models and usage scenarios.

			Cost-Effectiveness: 

				Estimating costs before provisioning resources helps avoid unexpected expenses.

		The other options are less suitable for this task:

			Option B: Creating a Google Sheet summarizing expected costs is a manual process that may not capture all potential costs or accurately reflect real-world usage patterns.

			Option C: Provisioning the solution and waiting for a week to generate billing data is time-consuming and may not provide accurate estimates for fluctuating costs.

			Option D: Using Stackdriver to determine provisioned and used resource amounts is not sufficient for estimating costs without considering pricing details and usage patterns.

		Therefore, option A, which involves using the pricing calculator to estimate monthly costs based on product pricing details, is the most efficient and accurate method for this scenario.

-------

You are storing sensitive information in a Cloud Storage bucket. For legal reasons, you need to be able to record all requests that read any of the stored data. You want to make sure you comply with these requirements. What should you do?
[Question 155]
	
	A. Enable the Identity Aware Proxy API on the project.
	
	B. Scan the bucket using the Data Loss Prevention API.
	
	C. Allow only a single Service Account access to read the data.
	
	D. Enable Data Access audit logs for the Cloud Storage API. [✔️]

	-------

	D. Enable Data Access audit logs for the Cloud Storage API.

		Recommended: Enabling Data Access audit logs for the Cloud Storage API allows you to record and track all requests that read the stored data. This helps you comply with legal requirements and provides an audit trail for data access.
	
	A. Enable the Identity Aware Proxy API on the project.

		Not directly related: The Identity Aware Proxy (IAP) is more focused on controlling access to applications rather than logging and auditing data access. While IAP provides identity-based access controls, it may not fulfill the specific requirement of recording all data access requests.
	
	B. Scan the bucket using the Data Loss Prevention API.

		Not directly related: The Data Loss Prevention (DLP) API is designed for identifying and redacting sensitive data in your content. It is not primarily intended for logging and auditing data access requests.
	
	C. Allow only a single Service Account access to read the data.

		Limited approach: While restricting access to a single Service Account can control who can read the data, it doesn't provide detailed logs of every access attempt. Enabling audit logs for the Cloud Storage API is a more comprehensive solution for tracking all access requests.
	
	In summary, option D is the most appropriate for meeting the requirement of recording all requests that read the stored data in the Cloud Storage bucket.

	-------

	Bard:

		The most appropriate approach to enable recording of all requests that read sensitive information stored in a Cloud Storage bucket is D. Enable Data Access audit logs for the Cloud Storage API.

			Data Access audit logs provide detailed information about all requests that access Cloud Storage resources, including read, write, and delete operations. This information includes the user or service account that initiated the request, the timestamp of the request, the resource being accessed, and the specific operation performed. By enabling data access audit logs, you can effectively record all requests that read the sensitive information stored in the Cloud Storage bucket.

		The other options are not as appropriate for this task:

			Option A: Enabling the Identity Aware Proxy API (IAP) enhances security by requiring users to authenticate before accessing resources. However, it does not specifically address the need to record data access requests.

			Option B: Scanning the bucket using the Data Loss Prevention (DLP) API is used to identify and classify sensitive data, but it does not provide detailed logs of data access requests.

			Option C: Allowing only a single Service Account access to read the data can restrict access, but it does not guarantee that all data access requests are recorded.

		Therefore, option D, which involves enabling data access audit logs for the Cloud Storage API, is the most suitable and effective approach to fulfill the requirement of recording all requests that read sensitive information stored in the bucket.

-------

You have an application that looks for its licensing server on the IP 10.0.3.21. You need to deploy the licensing server on Compute Engine. You do not want to change the configuration of the application and want the application to be able to reach the licensing server. What should you do? [Question 8]
	
	A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.
	[✔️]
	
	B. Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.
	
	C. Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.
	
	D. Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address

	-------

	A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.

		Recommended: This option allows you to reserve the specific internal IP address (10.0.3.21) as a static IP address and assign it to the licensing server. This ensures that the IP address remains constant, meeting the requirement of not changing the configuration of the application.
	
	B. Reserve the IP 10.0.3.21 as a static public IP address using gcloud and assign it to the licensing server.

		Not recommended: If the application is configured to look for the licensing server on an internal IP (10.0.3.21), assigning a public IP would likely not match the application's expectations. Additionally, it may introduce unnecessary exposure to the public internet.
	
	C. Use the IP 10.0.3.21 as a custom ephemeral IP address and assign it to the licensing server.

		Not recommended: Ephemeral IP addresses are dynamic and may change if the instance is stopped and started. This could lead to the application being unable to reach the licensing server if the IP address changes.
	
	D. Start the licensing server with an automatic ephemeral IP address, and then promote it to a static internal IP address.

		Not optimal: While promoting an ephemeral IP to a static IP is possible, it may involve changes to the instance's configuration. Directly reserving the desired internal IP address as static is a more straightforward solution.
	
	In summary, option A is the most appropriate for ensuring that the application can reach the licensing server without changing its configuration by reserving 10.0.3.21 as a static internal IP address.

	-------

	Bard:

		The most appropriate approach to deploy the licensing server on Compute Engine and ensure that the application can reach it without modifying its configuration is A. Reserve the IP 10.0.3.21 as a static internal IP address using gcloud and assign it to the licensing server.

			By reserving the IP 10.0.3.21 as a static internal IP address, you can guarantee that the licensing server will always have access to this specific IP address, allowing the application to connect to it consistently without requiring any changes to its configuration. This approach provides the most reliable and secure method for ensuring uninterrupted communication between the application and the licensing server.

		The other options are less suitable for this task:

			Option B: Reserving the IP 10.0.3.21 as a static public IP address would expose the licensing server to the Internet, which is not recommended for security reasons.

			Option C: Using the IP 10.0.3.21 as a custom ephemeral IP address would not provide the same level of consistency as a static IP address, potentially causing connection issues for the application.

			Option D: Starting the licensing server with an automatic ephemeral IP address and then promoting it to a static internal IP address is an unnecessarily complex approach that does not guarantee consistent connectivity for the application.

		Therefore, option A, which involves reserving the IP 10.0.3.21 as a static internal IP address and assigning it to the licensing server, is the most secure, reliable, and efficient method for fulfilling the requirements of this scenario.

-------

You need to configure IAM access audit logging in BigQuery for external auditors. You want to follow Google-recommended practices. What should you do? [Question 25]
	
	A. Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles. [✔️]
	
	B. Add the auditors group to two new custom IAM roles.
	
	C. Add the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.
	
	D. Add the auditor user accounts to two new custom IAM roles. [Bard Suggested] [✔️]

	-------

	A. Add the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.

		Recommended: This option is aligned with Google-recommended practices. By adding the auditors group to the 'logging.viewer' role, members will have access to view logs, and by adding them to the 'bigQuery.dataViewer' role, they will have read access to BigQuery datasets, enabling them to audit access.
	
	B. Add the auditors group to two new custom IAM roles.

		Not necessary: While creating custom roles is an option, using predefined roles like 'logging.viewer' and 'bigQuery.dataViewer' is generally more straightforward and aligns with best practices.
	
	C. Add the auditor user accounts to the 'logging.viewer' and 'bigQuery.dataViewer' predefined IAM roles.

		Valid but less scalable: If you have a small number of auditors and prefer individual assignments, this could work, but using a group (Option A) is more scalable and easier to manage.
	
	D. Add the auditor user accounts to two new custom IAM roles.

		Not necessary: Using predefined roles is recommended, and creating custom roles might introduce unnecessary complexity.
	
	In summary, option A, adding the auditors group to the 'logging.viewer' and 'bigQuery.dataViewer' predefined roles, is the most straightforward and aligns with Google-recommended practices.

	-------

	Bard:

		The most appropriate approach to configure IAM access audit logging in BigQuery for external auditors is D. Add the auditor user accounts to two new custom IAM roles.

		This approach offers several advantages:

			Granular Access Control: 

				Creating custom IAM roles allows for fine-grained control over the access granted to the auditors, enabling them to only view specific audit logs related to BigQuery activity.

			Audit Trail Visibility: 

				By granting the roles 'logging.viewer' and 'bigQuery.dataViewer', the auditors can effectively track access to BigQuery resources and identify any unauthorized or suspicious activity.

			Security and Compliance: 

				Creating custom IAM roles and granting them only the necessary permissions aligns with Google's recommended practices for security and compliance purposes.

		The other options are less suitable for this task:

			Option A: Adding the auditors group to the predefined IAM roles 'logging.viewer' and 'bigQuery.dataViewer' grants them broader access to audit logs and BigQuery data, which may not be appropriate for external auditors.

			Option B: Adding the auditors group to two new custom IAM roles is not as secure as adding individual user accounts, as it could allow unauthorized access if the group membership is compromised.

			Option C: Adding the auditor user accounts directly to the predefined IAM roles 'logging.viewer' and 'bigQuery.dataViewer' grants them excessive access to audit logs and BigQuery data, which is not recommended for external auditors.

		Therefore, option D, which involves creating custom IAM roles for individual auditor user accounts and granting them only the necessary permissions, is the most secure, compliant, and appropriate approach for this scenario.

-------

You deployed an App Engine application using gcloud app deploy, but it did not deploy to the intended project. You want to find out why this happened and where the application deployed. What should you do? [Question 46]
	
	A. Check the app.yaml file for your application and check project settings.
	
	B. Check the web-application.xml file for your application and check project settings.
	
	C. Go to Deployment Manager and review settings for deployment of applications.
	
	D. Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment.
	[✔️]

	-------

	D. Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment.

		Recommended: Running gcloud config list in Cloud Shell will show the current configuration settings, including the active project. This will help you identify which project is currently set as the default for deployments.
	
	A. Check the app.yaml file for your application and check project settings.

		Partially relevant: Checking the app.yaml file is a good practice to ensure that the intended settings are specified for the App Engine application. However, it might not directly show where the application was deployed.
	
	B. Check the web-application.xml file for your application and check project settings.

		Not relevant: web-application.xml is not a standard configuration file for App Engine deployments. The standard configuration file is app.yaml.
	
	C. Go to Deployment Manager and review settings for deployment of applications.

		Not relevant: Deployment Manager is generally used for managing the deployment of infrastructure as code and might not directly provide information about the location of an App Engine deployment.
	
	In summary, option D is the most relevant for quickly checking the current configuration settings and the active project, which will help identify where the application was deployed.

	-------

	Bard:

		The most appropriate approach to determine why the App Engine application did not deploy to the intended project and where it was deployed is D. Go to Cloud Shell and run gcloud config list to review the Google Cloud configuration used for deployment.

			The gcloud config list command provides a comprehensive overview of the current Google Cloud configuration, including the project ID that was used for the most recent deployment. This information can be used to identify the project where the application is deployed and to troubleshoot any configuration issues that may have caused the deployment to fail.

		The other options are less suitable for this task:

			Option A: Checking the app.yaml file for project settings may not provide accurate information about the actual project used for deployment, as the deployment configuration may have overridden the project settings in the app.yaml file.

			Option B: Checking the web-application.xml file for project settings is not relevant for App Engine applications, as this file is used for deploying web applications to Java-based platforms like Google App Engine Flex and Google Container Engine (GKE).

			Option C: Reviewing deployment settings in Deployment Manager is not applicable for App Engine applications, as Deployment Manager is used for deploying infrastructure-as-code (IaC) templates, not for deploying web applications.

		Therefore, option D, which involves using the gcloud config list command to determine the project used for the recent deployment, is the most effective and reliable method for identifying the deployed application's location.

-------

** You have a web application deployed as a managed instance group. You have a new version of the application to gradually deploy. Your web application is currently receiving live web traffic. You want to ensure that the available capacity does not decrease during the deployment. What should you do? [Question 57]
	
	A. Perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1. [Bard and chatGPT suggested] [✔️]
	
	B. Perform a rolling-action start-update with maxSurge set to 1 and maxUnavailable set to 0. [✔️]
	
	C. Create a new managed instance group with an updated instance template. Add the group to the backend service for the load balancer. When all instances in the new managed instance group are healthy, delete the old managed instance group. [Suggested]
	
	D. Create a new instance template with the new application version. Update the existing managed instance group with the new instance template. Delete the instances in the managed instance group to allow the managed instance group to recreate the instance using the new instance template.

	-------

	D. Create a new instance template with the new application version. Update the existing managed instance group with the new instance template. Delete the instances in the managed instance group to allow the managed instance group to recreate the instance using the new instance template.

		Not recommended: This option involves deleting instances, which could lead to a temporary decrease in capacity and might not be suitable for scenarios where maintaining consistent capacity is crucial.

	-------

	Bard:

		The correct answer is A. Perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.

			Rolling updates are a way to gradually deploy new versions of an application without disrupting traffic. With a rolling update, you can update a small number of instances at a time, and the load balancer will automatically distribute traffic to the healthy instances. This ensures that the available capacity does not decrease during the deployment.

			The maxSurge parameter specifies the maximum number of new instances that can be created during the update. Setting maxSurge to 0 means that no new instances will be created until all of the old instances have been updated and are healthy.

			The maxUnavailable parameter specifies the maximum number of instances that can be unavailable during the update. Setting maxUnavailable to 1 means that one instance will be unavailable at a time. This means that the load balancer will always have at least one instance available to serve traffic.

		Option B is incorrect because it would allow the number of instances to increase during the update, which could overload the load balancer.

		Option C is incorrect because it would create a new managed instance group and then delete the old one. This would disrupt traffic for the duration of the deployment, which is not what we want.

		Option D is incorrect because deleting the instances in the managed instance group would disrupt traffic. Instead, we should wait for the rolling update to complete before deleting the old instances.

		Therefore, the best way to gradually deploy a new version of a web application deployed as a managed instance group without disrupting traffic is to perform a rolling-action start-update with maxSurge set to 0 and maxUnavailable set to 1.

-------

You host a static website on Cloud Storage. Recently, you began to include links to PDF files on this site. Currently, when users click on the links to these PDF files, their browsers prompt them to save the file onto their local system. Instead, you want the clicked PDF files to be displayed within the browser window directly, without prompting the user to save the file locally. What should you do? [Question 48]
	
	A. Enable Cloud CDN on the website frontend.
	
	B. Enable 'Share publicly' on the PDF file objects.
	
	C. Set Content-Type metadata to application/pdf on the PDF file objects. [✔️]
	
	D. Add a label to the storage bucket with a key of Content-Type and value of application/pdf

	-------

	C. Set Content-Type metadata to application/pdf on the PDF file objects.

		Recommended: By setting the Content-Type metadata to application/pdf, you specify the correct MIME type for the PDF files. This helps the browser recognize the file type and display it directly within the browser window instead of prompting the user to save it locally.
	
	A. Enable Cloud CDN on the website frontend.

		Not directly related: Cloud CDN (Content Delivery Network) helps in delivering content quickly to users by caching it at locations close to them. While it can improve performance, it's not the primary solution for controlling how browsers handle file types.
	
	B. Enable 'Share publicly' on the PDF file objects.

		Not directly related: Enabling public sharing doesn't control how the browser displays the file; it only affects who can access the file.
	
	D. Add a label to the storage bucket with a key of Content-Type and value of application/pdf.

		Not necessary: Labels are not used to control how browsers interpret the content type. Setting the correct Content-Type metadata directly on the file objects is the appropriate approach.
	
	In summary, option C is the most appropriate solution for ensuring that the clicked PDF files are displayed within the browser window without prompting the user to save the file locally.

	-------

	Bard:

		The correct answer is C. Set Content-Type metadata to application/pdf on the PDF file objects.

			To display PDF files directly within the browser window, you need to set the Content-Type metadata to application/pdf on the PDF file objects. This will tell the browser that the file is a PDF and that it should be displayed inline, rather than saved locally.

		Option A is incorrect because Cloud CDN is a content delivery network that can improve the performance of your website by caching static content closer to users. It will not directly affect how PDF files are displayed.

		Option B is incorrect because enabling 'Share publicly' on the PDF file objects will make the files publicly accessible, but it will not change how they are displayed in the browser.

		Option D is incorrect because adding a label to the storage bucket will not affect the metadata of individual objects within the bucket.

		Therefore, the only way to ensure that PDF files are displayed directly within the browser window is to set the Content-Type metadata to application/pdf on the PDF file objects themselves.

-------

You have 32 GB of data in a single file that you need to upload to a Nearline Storage bucket. The WAN connection you are using is rated at 1 Gbps, and you are the only one on the connection. You want to use as much of the rated 1 Gbps as possible to transfer the file rapidly. How should you upload the file? [Question 41]
	
	A. Use the GCP Console to transfer the file instead of gsutil.
	
	B. Enable parallel composite uploads using gsutil on the file transfer. [✔️]
	
	C. Decrease the TCP window size on the machine initiating the transfer.
	
	D. Change the storage class of the bucket from Nearline to Multi-Regional.

	-------

	B. Enable parallel composite uploads using gsutil on the file transfer.

		Recommended: Using gsutil with parallel composite uploads allows the file to be split into smaller components that can be uploaded concurrently, maximizing the use of the available bandwidth. This helps in achieving faster transfers.
	
	A. Use the GCP Console to transfer the file instead of gsutil.

		Not efficient: The GCP Console may not optimize the transfer process as effectively as gsutil, especially for large files. Using gsutil with parallel composite uploads is a more efficient option.
	
	C. Decrease the TCP window size on the machine initiating the transfer.

		Not recommended: Decreasing the TCP window size could potentially limit the amount of data in flight and may not be the best approach for maximizing the utilization of the available bandwidth.
	
	D. Change the storage class of the bucket from Nearline to Multi-Regional.

		Not relevant: The storage class of the bucket does not directly impact the speed of file uploads. It determines the storage cost and retrieval characteristics but does not affect the transfer rate.
	
	In summary, option B, enabling parallel composite uploads using gsutil, is the most appropriate choice for maximizing the use of the available 1 Gbps bandwidth and achieving a rapid file transfer.

	-------

	Bard:

		The correct answer is B. Enable parallel composite uploads using gsutil on the file transfer.

			Nearline Storage is a cost-effective storage class for infrequently accessed data. However, it is not designed for high-performance workloads. When uploading a large file to Nearline Storage, you should use parallel composite uploads to improve the performance of the transfer.

			Parallel composite uploads break the file into smaller chunks and uploads them in parallel. This can significantly reduce the upload time, especially for large files.

		Option A is incorrect because the GCP Console is a web-based tool that is not designed for transferring large amounts of data. It would be much more efficient to use gsutil, which is a command-line tool specifically designed for interacting with Cloud Storage.

		Option C is incorrect because decreasing the TCP window size would limit the amount of data that can be transferred at once. This would actually reduce the performance of the transfer.

		Option D is incorrect because changing the storage class of the bucket from Nearline to Multi-Regional would not improve the performance of the upload. Multi-Regional Storage is a different storage class that is designed for data that needs to be accessed from multiple regions. It is not designed for high-throughput workloads.

		Therefore, the best way to upload a large file to a Nearline Storage bucket using a 1 Gbps WAN connection is to enable parallel composite uploads using gsutil.

-------

* An employee was terminated, but their access to Google Cloud Platform (GCP) was not removed until 2 weeks later. You need to find out this employee accessed any sensitive customer information after their termination. What should you do?
[Question 117]
	
	A. View System Event Logs in Stackdriver. Search for the user's email as the principal.
	
	B. View System Event Logs in Stackdriver. Search for the service account associated with the user. [Suggested]
	
	C. View Data Access audit logs in Stackdriver. Search for the user's email as the principal. [✔️]
	
	D. View the Admin Activity log in Stackdriver. Search for the service account associated with the user.

	-------

	C. View Data Access audit logs in Stackdriver. Search for the user's email as the principal.

		Recommended: Data Access audit logs in Stackdriver provide information about accesses to resources, which includes actions such as reading or modifying data. Searching for the user's email as the principal in Data Access audit logs can help identify any access to sensitive customer information.
	
	A. View System Event Logs in Stackdriver. Search for the user's email as the principal.

		Not the most relevant: System Event Logs may not provide detailed information about data access activities. Data Access audit logs are more appropriate for tracking resource accesses.
	
	B. View System Event Logs in Stackdriver. Search for the service account associated with the user.

		Not the most relevant: System Event Logs may not directly associate with data access, and searching for the service account associated with the user might not provide detailed information about data interactions.
	
	D. View the Admin Activity log in Stackdriver. Search for the service account associated with the user.

		Not the most relevant: The Admin Activity log focuses on administrative actions rather than data access. Searching for the service account associated with the user may not provide information about sensitive data access.
	
	In summary, option C, viewing Data Access audit logs in Stackdriver and searching for the user's email as the principal, is the most appropriate choice for identifying whether the terminated employee accessed sensitive customer information after termination.

	-------

	Bard:

		The correct answer is C. View Data Access audit logs in Stackdriver. Search for the user's email as the principal.

			Data Access Audit Logs in Stackdriver are specifically designed to capture access to sensitive data, including customer information. These logs record all data-access events, including the user or service account that performed the access, the time of the access, and the resource that was accessed.

			To determine if a terminated employee accessed sensitive customer information, you should search the Data Access Audit Logs for events that occurred after the employee's termination date. You should also filter the logs by the user's email address to ensure that you are only looking at events that were initiated by the terminated employee.

		Option A is incorrect because System Event Logs capture a broader range of events, including system-level actions such as service restarts or instance provisioning. They do not specifically record data access events.

		Option B is incorrect because service accounts are typically used to automate tasks or access resources on behalf of users. While a terminated employee may have been associated with a service account, it is the user associated with the service account that would have initiated the access.

		Option D is incorrect because the Admin Activity log captures administrative actions, such as creating users, granting roles, or modifying IAM policies. It does not record data access events.

		Therefore, the best way to determine if a terminated employee accessed sensitive customer information after their termination is to view the Data Access Audit Logs in Stackdriver and search for events that occurred after the termination date and were initiated by the terminated employee's email address.

-------

** You have an application that uses Cloud Spanner as a database backend to keep current state information about users. Cloud Bigtable logs all events triggered by users. You export Cloud Spanner data to Cloud Storage during daily backups. One of your analysts asks you to join data from Cloud Spanner and Cloud
Bigtable for specific users. You want to complete this ad hoc request as efficiently as possible. What should you do?
[Question 146]
	
	A. Create a dataflow job that copies data from Cloud Bigtable and Cloud Storage for specific users.
	
	B. Create a dataflow job that copies data from Cloud Bigtable and Cloud Spanner for specific users. [Suggested and chatGPT Suggested]
	
	C. Create a Cloud Dataproc cluster that runs a Spark job to extract data from Cloud Bigtable and Cloud Storage for specific users.
	
	D. Create two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters. [✔️]

	-------

	C. Create a Cloud Dataproc cluster that runs a Spark job to extract data from Cloud Bigtable and Cloud Storage for specific users.

		Possible but not as efficient: Cloud Dataproc can be used for processing large datasets using Apache Spark, but for this specific task, Dataflow is more suitable as it is a fully managed service for stream and batch processing.

	D. Create two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters.

		Possible but less efficient: While BigQuery can perform joins, creating external tables on Cloud Storage and Cloud Bigtable and joining them in BigQuery may introduce additional latency and be less efficient than using Dataflow for processing and joining the data directly.

	-------

	Bard:

		The correct answer is D. Create two separate BigQuery external tables on Cloud Storage and Cloud Bigtable. Use the BigQuery console to join these tables through user fields, and apply appropriate filters.

			This approach is the most efficient and scalable way to join data from Cloud Spanner and Cloud Bigtable for specific users. By creating separate external tables for each source, you can take advantage of BigQuery's parallel processing capabilities to efficiently join the data. Additionally, using BigQuery's console makes it easy to filter and visualize the results.

		Option A is incorrect because creating a dataflow job to copy the data from both sources would be inefficient and would not scale well for multiple requests.

		Option B is incorrect because creating a dataflow job to copy the data from Cloud Bigtable would be inefficient, as the data is already stored in Cloud Storage.

		Option C is incorrect because creating a Cloud Dataproc cluster and running a Spark job would be more complex and time-consuming than the other options. Additionally, Spark is not as well-suited for ad hoc data analysis as BigQuery.

-------

** A team of data scientists infrequently needs to use a Google Kubernetes Engine (GKE) cluster that you manage. They require GPUs for some long-running, non- restartable jobs. You want to minimize cost. What should you do? [Question 108]
	
	A. Enable node auto-provisioning on the GKE cluster. [✔️]
	
	B. Create a VerticalPodAutscaler for those workloads.
	
	C. Create a node pool with preemptible VMs and GPUs attached to those VMs. [Bard Suggested] [✔️]
	
	D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.
	[Another Most Voted]

	-------

	A. Enable node auto-provisioning on the GKE cluster.

		Recommended: Node auto-provisioning in Google Kubernetes Engine (GKE) allows the cluster to automatically adjust the size of its node pools based on the resource requirements of running and upcoming pods. This helps minimize cost by efficiently scaling the number of nodes up or down based on the actual demand from the workloads.
	
	B. Create a VerticalPodAutoscaler for those workloads.

		Not the best fit: VerticalPodAutoscaler adjusts the CPU and memory resources of existing pods, but it doesn't directly address the need for GPUs. Additionally, enabling node auto-provisioning is a more comprehensive solution.
	
	C. Create a node pool with preemptible VMs and GPUs attached to those VMs.

		Possible but with caveats: Preemptible VMs are cost-effective but come with the risk of being preempted. If the data scientists' jobs are non-restartable, using preemptible VMs may not be suitable, as preempted instances lose their data. Also, preemptible VMs may not be ideal for workloads with GPUs.
	
	D. Create a node pool of instances with GPUs, and enable autoscaling on this node pool with a minimum size of 1.

		Possible but less efficient: Enabling autoscaling with a minimum size of 1 may keep a node running even when there are no active workloads, leading to unnecessary costs. Node auto-provisioning can dynamically adjust the size based on demand, providing a more efficient solution.
	
	In summary, option A, enabling node auto-provisioning, is recommended as it allows the GKE cluster to automatically adjust the size of its node pools based on the resource requirements, minimizing cost while efficiently meeting the demands of the data scientists' workloads.

	-------

	Bard:

		The correct answer is C. Create a node pool with preemptible VMs and GPUs attached to those VMs.

			Preemptible VMs are a cost-effective way to run workloads that can tolerate interruptions, such as long-running, non- restartable jobs. By creating a node pool with preemptible VMs and attaching GPUs to those VMs, you can minimize the cost of running these workloads.

		Option A is incorrect because enabling node auto-provisioning on the GKE cluster would cause the cluster to provision new nodes automatically whenever there is demand. This would be inefficient for infrequently used workloads, as the nodes would be spinning up and down constantly.

		Option B is incorrect because a VerticalPodAutoscaler automatically adjusts the number of CPU and memory resources assigned to pods. This would not help to reduce the cost of running workloads on preemptible VMs, as the VMs are already priced based on their CPU and memory resources.

		Option D is incorrect because creating a node pool of instances with GPUs, and enabling autoscaling on this node pool with a minimum size of 1 would cause the cluster to always have at least one node running, even if there is no demand for the workloads. This would be wasteful of resources and would not minimize the cost of running the workloads.

		Therefore, the best way to minimize the cost of running infrequently used workloads that require GPUs on a GKE cluster is to create a node pool with preemptible VMs and attach GPUs to those VMs. This will allow you to take advantage of the cost-effectiveness of preemptible VMs while still providing the necessary GPU resources for your workloads.

-------

Your organization has user identities in Active Directory. Your organization wants to use Active Directory as their source of truth for identities. Your organization wants to have full control over the Google accounts used by employees for all Google services, including your Google Cloud Platform (GCP) organization. What should you do? [Question 109]
	
	A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity. [✔️]
	
	B. Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity.
	
	C. Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.
	
	D. Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password.

	-------

	A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.

		Recommended: Google Cloud Directory Sync (GCDS) is a tool that synchronizes user and group details from an Active Directory server to Cloud Identity or Google Workspace. It allows you to maintain control over user identities in Active Directory while ensuring synchronization with Google Cloud services, including Google Cloud Platform (GCP).
	
	B. Use the Cloud Identity APIs and write a script to synchronize users to Cloud Identity.

		Possible but more complex: While using APIs is a valid option, Google provides a dedicated tool (GCDS) for this purpose, which is more user-friendly and tailored for directory synchronization. Writing a custom script might be more complex and error-prone.
	
	C. Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.

		Not efficient for ongoing synchronization: This method is manual and not suitable for ongoing synchronization of user identities. GCDS provides automated and scheduled synchronization.
	
	D. Ask each employee to create a Google account using self-signup. Require that each employee use their company email address and password.

		Not recommended: This approach lacks central control and doesn't ensure that user identities are synchronized with the Active Directory source of truth. It may lead to inconsistencies and security concerns.
	
	In summary, option A, using Google Cloud Directory Sync (GCDS), is recommended as it provides a robust and automated solution for synchronizing user identities from Active Directory to Cloud Identity, ensuring that your organization maintains control over Google accounts used for all Google services, including GCP.

	-------

	Bard:

		The correct answer is A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.

			Google Cloud Directory Sync (GCDS) is the recommended solution to synchronize user identities from Active Directory to Cloud Identity. GCDS is a fully managed service that can be used to automate the synchronization of user accounts, groups, and other directory objects. This allows your organization to keep its Active Directory as the source of truth for identities, while still providing full control over the Google accounts used by employees for all Google services.

		Option B is incorrect because writing a script to synchronize users to Cloud Identity manually would be a complex and time-consuming task. GCDS is a more efficient and reliable solution for this purpose.

		Option C is incorrect because exporting users from Active Directory as a CSV file and importing them to Cloud Identity via the Admin Console would be a one-time solution. GCDS is a continuous synchronization solution that can keep your Active Directory and Cloud Identity in sync at all times.

		Option D is incorrect because asking each employee to create a Google account using self signup would lead to a lack of control over the Google accounts used by employees. GCDS ensures that all Google accounts are managed centrally by your organization.

		Therefore, the best way to synchronize user identities from Active Directory to Cloud Identity while maintaining full control over Google accounts is to use Google Cloud Directory Sync (GCDS). GCDS is a fully managed service that can automate the synchronization of user accounts, groups, and other directory objects, keeping your Active Directory as the source of truth for identities and providing centralized control over Google accounts.

-------

* You want to select and configure a cost-effective solution for relational data on Google Cloud Platform. You are working with a small set of operational data in one geographic location. You need to support point-in-time recovery. What should you do? [Question 3]
	
	A. Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected. [✔️]
	
	B. Select Cloud SQL (MySQL). Select the create failover replicas option. [Bard suggested] [✔️]
	
	C. Select Cloud Spanner. Set up your instance with 2 nodes.
	
	D. Select Cloud Spanner. Set up your instance as multi-regional.

	-------

	A. Select Cloud SQL (MySQL). Verify that the enable binary logging option is selected.

		Recommended: This option provides point-in-time recovery capability by enabling binary logging in Cloud SQL (MySQL). Binary logs allow you to restore the database to a specific point in time, supporting data recovery.
	
	B. Select Cloud SQL (MySQL). Select the create failover replicas option.

		Not the best fit for point-in-time recovery: While failover replicas can provide high availability, they may not be the optimal solution for point-in-time recovery. Enabling binary logging is more directly related to this requirement.
	
	C. Select Cloud Spanner. Set up your instance with 2 nodes.

		May be overkill for small sets of operational data: Cloud Spanner is a globally distributed, horizontally scalable database. For small datasets in a single geographic location, it might be considered over-engineering and could be more costly.
	
	D. Select Cloud Spanner. Set up your instance as multi-regional.

		May be overkill for small sets of operational data: Similar to option C, setting up Cloud Spanner as multi-regional is designed for high availability and global distribution, which may not be necessary for a small set of operational data in one geographic location.
	
	In summary, option A, selecting Cloud SQL (MySQL) and enabling binary logging, is recommended for a cost-effective solution that supports point-in-time recovery for a small set of operational data in one geographic location.

	-------

	Bard:

		The correct answer is B. Select Cloud SQL (MySQL). Select the create failover replicas option.

			Cloud SQL is a managed MySQL server that offers cost-effectiveness and point-in-time recovery. The create failover replicas option allows you to create asynchronous replicas of your primary Cloud SQL instance. If the primary instance fails, you can fail over to one of the replicas without data loss.

		Option A is incorrect because enabling binary logging allows you to perform point-in-time recovery, but it does not provide the full functionality of a failover replica. A failover replica is a fully functional copy of your database, whereas binary logging only allows you to restore your database to a specific point in time.

		Option C is incorrect because a Cloud Spanner instance with 2 nodes is not a cost-effective solution for a small set of operational data. Cloud Spanner is a fully managed globally distributed database that is designed for large-scale workloads. It is not well-suited for small, localized applications.

		Option D is incorrect because a Cloud Spanner instance configured as multi-regional is not necessary for a small set of operational data in one geographic location. Multi-regional instances are designed for applications that require high availability and low latency across multiple regions. They are not required for applications that only operate in one region.

		Therefore, the best way to select a cost-effective solution for relational data on Google Cloud Platform with point-in-time recovery for a small set of operational data in one geographic location is to select Cloud SQL (MySQL) and enable the create failover replicas option. This will allow you to create a fully functional replica of your database that can be used for failover if the primary instance fails.

-------

* You have an application running in Google Kubernetes Engine (GKE) with cluster autoscaling enabled. The application exposes a TCP endpoint. There are several replicas of this application. You have a Compute Engine instance in the same region, but in another Virtual Private Cloud (VPC), called gce-network, that has no overlapping IP ranges with the first VPC. This instance needs to connect to the application on GKE. You want to minimize effort. What should you do?
[Question 60]
	
	A. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Set the service's externalTrafficPolicy to Cluster. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created. [Another Most Voted]
	
	B. 1. In GKE, create a Service of type NodePort that uses the application's Pods as backend. 2. Create a Compute Engine instance called proxy with 2 network interfaces, one in each VPC. 3. Use iptables on this instance to forward traffic from gce-network to the GKE nodes. 4. Configure the Compute Engine instance to use the address of proxy in gce-network as endpoint.
	
	C. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal 3. Peer the two VPCs together. 4. Configure the Compute Engine instance to use the address of the load balancer that has been created. [✔️] 
	
	D. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add a Cloud Armor Security Policy to the load balancer that whitelists the internal IPs of the MIG's instances. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.

	-------

	C. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add an annotation to this service: cloud.google.com/load-balancer-type: Internal 3. Peer the two VPCs together. 4. Configure the Compute Engine instance to use the address of the load balancer that has been created.

		Recommended: This option creates an internal LoadBalancer in GKE, which makes the service accessible only within the same VPC. By peering the two VPCs together, you enable communication between them. The Compute Engine instance can then use the address of the internal LoadBalancer.
	
	Explanation for other options:

		A. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Set the service's externalTrafficPolicy to Cluster. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.

			Not recommended: This option exposes the application to the internet, which might not be necessary for internal communication between VPCs. Additionally, using an internal LoadBalancer is more suitable for this scenario.
		
		B. In GKE, create a Service of type NodePort that uses the application's Pods as backend. 2. Create a Compute Engine instance called proxy with 2 network interfaces, one in each VPC. 3. Use iptables on this instance to forward traffic from gce-network to the GKE nodes. 4. Configure the Compute Engine instance to use the address of proxy in gce-network as an endpoint.

			Complex and involves additional components: While technically feasible, this option involves setting up a proxy instance and managing iptables rules, which adds complexity and may not be necessary for achieving the goal.
	
		D. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Add a Cloud Armor Security Policy to the load balancer that whitelists the internal IPs of the MIG's instances. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.

			Not necessary for internal communication: Cloud Armor Security Policy is designed for securing internet-facing 	applications, and in this scenario, you are dealing with internal communication between VPCs. It introduces unnecessary complexity for this use case.

	-------

	Bard:

		The most appropriate approach to allow the Compute Engine instance in gce-network to connect to the application on GKE with minimal effort is A. 1. In GKE, create a Service of type LoadBalancer that uses the application's Pods as backend. 2. Set the service's externalTrafficPolicy to Cluster. 3. Configure the Compute Engine instance to use the address of the load balancer that has been created.

		This approach offers several advantages:

			Simplicity and Scalability: 

				Creating a LoadBalancer service with Cluster externalTrafficPolicy allows the Compute Engine instance to connect to any of the application's replicas without requiring manual intervention or configuration changes.

			Minimal Effort: 

				This method involves the least amount of configuration and setup, making it the most straightforward and efficient solution.

			Automatic Traffic Routing: 

				The load balancer will automatically distribute traffic across the application's replicas, ensuring that the Compute Engine instance can reach any pod that is available.

		The other options are less suitable for this scenario:

			Option B: Using a NodePort service and a proxy instance requires more complexity and manual configuration.

			Option C: Peering the two VPCs is not required to connect the Compute Engine instance to the GKE application, as the LoadBalancer service will route traffic directly from the Compute Engine instance to the application's pods.

			Option D: Using Cloud Armor Security Policy is not necessary for this scenario, as the LoadBalancer service already provides protection against unauthorized access.

		Therefore, option A, which involves creating a LoadBalancer service with Cluster externalTrafficPolicy and configuring the Compute Engine instance to use the load balancer address, is the most efficient and straightforward approach for connecting the Compute Engine instance to the application on GKE with minimal effort.

-------

You have sensitive data stored in three Cloud Storage buckets and have enabled data access logging. You want to verify activities for a particular user for these buckets, using the fewest possible steps. You need to verify the addition of metadata labels and which files have been viewed from those buckets. What should you do? [Question 27]
	
	A. Using the GCP Console, filter the Activity log to view the information. [✔️]
	
	B. Using the GCP Console, filter the Stackdriver log to view the information. [Another Most Voted]
	
	C. View the bucket in the Storage section of the GCP Console.
	
	D. Create a trace in Stackdriver to view the information

	-------

	A. Using the GCP Console, filter the Activity log to view the information.

		Recommended: The Activity log contains detailed information about various actions performed on resources, including Cloud Storage buckets. You can filter the Activity log based on the user and specific activities, such as the addition of metadata labels and file views.
	
	Explanation for other options:

		B. Using the GCP Console, filter the Stackdriver log to view the information.

			Not the primary source: While Stackdriver might contain logs related to certain activities, the Activity log is specifically designed for auditing and monitoring user activities on Google Cloud resources.
	
		C. View the bucket in the Storage section of the GCP Console.

			Limited details: The Storage section provides an overview of the Cloud Storage buckets and their contents but may not provide detailed user activity logs.
		
		D. Create a trace in Stackdriver to view the information.

			Not suitable for this purpose: Stackdriver traces are typically used for performance monitoring and debugging distributed systems. It is not the appropriate tool for auditing user activities in Cloud Storage.

	-------

	Bard:

		The most appropriate approach to verify activities for a particular user for the three Cloud Storage buckets with enabled data access logging, using the fewest possible steps, is A. Using the GCP Console, filter the Activity log to view the information.

		This approach offers several advantages:

			Centralized Logging: 

				The GCP Console provides a centralized view of all Google Cloud logs, including data access logs for Cloud Storage.

			Simplified Filtering: 

				Activity logs can be easily filtered to focus on specific user activities, such as metadata label additions or file viewing actions.

			Integrated Viewing: 

				Filtered activity logs can be directly viewed within the GCP Console without the need for exporting or analyzing separate log files.

		The other options are less suitable for this scenario:

			Option B: Stackdriver logs are more complex and specifically designed for error analysis and debugging. They may not be the most efficient option for verifying user activities.

			Option C: Viewing the buckets directly in the Storage section of the Console will only provide information about the files and their contents, not about user activities related to those files.

			Option D: Creating a trace in Stackdriver is specifically used for tracking application requests and performance, not for analyzing user activities related to data access logs.

		Therefore, option A, which involves using the GCP Console to filter the Activity log to focus on specific user activities and view the relevant information, is the most straightforward and efficient approach for this task.

-------

You are managing several Google Cloud Platform (GCP) projects and need access to all logs for the past 60 days. You want to be able to explore and quickly analyze the log contents. You want to follow Google-recommended practices to obtain the combined logs for all projects. What should you do? [Question 114]
	
	A. Navigate to Stackdriver Logging and select resource.labels.project_id="*"
	
	B. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days. [✔️]
	
	C. Create a Stackdriver Logging Export with a Sink destination to Cloud Storage. Create a lifecycle rule to delete objects after 60 days.
	
	D. Configure a Cloud Scheduler job to read from Stackdriver and store the logs in BigQuery. Configure the table expiration to 60 days

	-------

	B. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.

		Recommended: This option allows you to export logs from Stackdriver Logging to a BigQuery dataset, where you can analyze and explore the log contents efficiently. Configuring the table expiration to 60 days ensures that the logs are retained for the required duration.
	
	Explanation for other options:

		A. Navigate to Stackdriver Logging and select resource.labels.project_id="*":

			Not efficient: Manually navigating and selecting projects one by one is not efficient, especially when managing multiple projects.
		
		C. Create a Stackdriver Logging Export with a Sink destination to Cloud Storage. Create a lifecycle rule to delete objects after 60 days:

			Less suitable for exploration: While exporting logs to Cloud Storage is an option, exploring and analyzing logs directly in BigQuery provides more flexibility and powerful querying capabilities.
		
		D. Configure a Cloud Scheduler job to read from Stackdriver and store the logs in BigQuery. Configure the table expiration to 60 days:

			Overly complex: Using Cloud Scheduler to periodically fetch logs and store them in BigQuery adds unnecessary complexity. Directly exporting logs to BigQuery using Stackdriver Logging is a more straightforward approach.

	-------

	Bard:

		The most appropriate approach to obtain combined logs for all projects using Google-recommended practices is B. Create a Stackdriver Logging Export with a Sink destination to a BigQuery dataset. Configure the table expiration to 60 days.

		This approach offers several advantages:

			Centralized Logging: 

				Stackdriver Logging Export provides a unified mechanism for collecting logs from multiple projects.

			Scalable and Reliable: 

				BigQuery is a highly scalable and reliable data warehouse that can effectively store and analyze large volumes of log data.

			Time-Bound Storage: 

				Configuring table expiration ensures that logs are automatically deleted after 60 days, preventing excessive storage consumption.

			Google-Recommended Practice: 

				This approach aligns with Google's recommended practices for managing and storing log data in a centralized and secure manner.

		The other options are less suitable for this scenario:

			Option A: Filtering log data directly in Stackdriver Logging may not be efficient for managing and analyzing logs from multiple projects.

			Option C: Using Cloud Storage for log storage can be challenging for large volumes of log data and lacks the analysis capabilities of BigQuery.

			Option D: Utilizing Cloud Scheduler to transfer logs to BigQuery adds an additional layer of complexity and may not be as efficient as directly exporting logs using Stackdriver Logging Export.

		Therefore, option B, which involves creating a Stackdriver Logging Export with a Sink destination to a BigQuery dataset and configuring table expiration, is the most appropriate and recommended approach for this task.

-------

* You have downloaded and installed the gcloud command line interface (CLI) and have authenticated with your Google Account. Most of your Compute Engine instances in your project run in the europe-west1-d zone. You want to avoid having to specify this zone with each CLI command when managing these instances.
What should you do? [Question 138]
	
	A. Set the europe-west1-d zone as the default zone using the gcloud config subcommand. [✔️]
	
	B. In the Settings page for Compute Engine under Default location, set the zone to europe west1-d.
	
	C. In the CLI installation directory, create a file called default.conf containing zone=europe west1 d. [Suggested]
	
	D. Create a Metadata entry on the Compute Engine page with key compute/zone and value europe west1 d.

	-------

	A. Set the europe-west1-d zone as the default zone using the gcloud config subcommand.

		Recommended: This option allows you to set the default zone for the gcloud command line interface, ensuring that you don't have to specify the zone with each command when managing instances.
	
	Explanation for other options:

		B. In the Settings page for Compute Engine under Default location, set the zone to europe-west1-d:

		Not a gcloud CLI configuration: This setting is related to the Google Cloud Console and not the gcloud command line interface. It won't affect the behavior of the CLI.
	
	C. In the CLI installation directory, create a file called default.conf containing zone=europe-west1-d:

		Manual approach: This is not the standard way to configure the default zone for the gcloud CLI. Using the gcloud config subcommand is the recommended method.
	
	D. Create a Metadata entry on the Compute Engine page with key compute/zone and value europe-west1-d:

		Not a gcloud CLI configuration: This option involves setting metadata at the project level, which is not the standard way to configure the default zone for the gcloud CLI. The gcloud CLI configuration should be done using the gcloud config command.

	-------

	Bard:

		The most appropriate approach to set the europe-west1-d zone as the default zone using the gcloud config subcommand is A. Set the europe-west1-d zone as the default zone using the gcloud config subcommand.

			This approach involves utilizing the gcloud config set command to specify the default zone for all subsequent CLI commands related to Compute Engine instances. This eliminates the need to explicitly mention the zone with each command, making it more convenient for managing instances in that region.

		The other options are less suitable for this task:

			Option B: Setting the zone in the Compute Engine Settings page only applies to the web interface, not to CLI commands.

			Option C: Creating a default.conf file in the CLI installation directory is an outdated method that may not work with the latest version of gcloud.

			Option D: Creating a Metadata entry on the Compute Engine page with the specified key and value will not affect the default zone for CLI commands.

		Therefore, option A, which involves setting the default zone using the gcloud config subcommand, is the most efficient and reliable method for managing Compute Engine instances in the specified zone without explicitly specifying the zone with each command.

-------

You have a Dockerfile that you need to deploy on Kubernetes Engine. What should you do? [Question 12]
	
	A. Use kubectl app deploy <dockerfilename>.
	
	B. Use gcloud app deploy <dockerfilename>.
	
	C. Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file. [✔️]
	
	D. Create a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.

	-------

	C. Create a docker image from the Dockerfile and upload it to Container Registry. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file.

		Recommended: This is the standard practice for deploying Docker containers on Kubernetes Engine. You build the Docker image, store it in a container registry (such as Google Container Registry), and then create a Kubernetes Deployment YAML file to specify the image and other deployment details.
	
	Explanation for other options:

		A. Use kubectl app deploy <dockerfilename>:

			Incorrect syntax: The correct syntax for deploying with kubectl involves specifying the deployment YAML file or the resource directly, not the Dockerfile.
		
		B. Use gcloud app deploy <dockerfilename>:

			Incorrect command: The gcloud app deploy command is used for deploying applications to Google App Engine, not for deploying containers to Kubernetes Engine.
		
		D. Create a docker image from the Dockerfile and upload it to Cloud Storage. Create a Deployment YAML file to point to that image. Use kubectl to create the deployment with that file:

			Not a standard practice: While it is possible to store Docker images in Cloud Storage, it is not the recommended approach for deploying containers on Kubernetes Engine. Container Registry is the preferred service for hosting container images in a Kubernetes environment.

-------

* You are building an application that will run in your data center. The application will use Google Cloud Platform (GCP) services like AutoML. You created a service account that has appropriate access to AutoML. You need to enable authentication to the APIs from your on-premises environment. What should you do? [Question 70]
	
	A. Use service account credentials in your on-premises application. [Bard and chatGPT Suggested] [✔️]
	
	B. Use gcloud to create a key file for the service account that has appropriate permissions. [✔️]
	
	C. Set up direct interconnect between your data center and Google Cloud Platform to enable authentication for your on-premises applications.
	
	D. Go to the IAM & admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center.

	-------

	Bard:

		The correct answer is A. Use service account credentials in your on-premises application.

			When building an application that will run in your on-premises data center and utilize GCP services, using service account credentials is the preferred method for enabling authentication to the APIs. Service accounts are special Google accounts that are managed programmatically and have a set of predetermined permissions. By creating a service account with appropriate access to AutoML, you can securely grant your on-premises application the necessary permissions to interact with the GCP APIs.

		The other options are not suitable in this scenario:

			B. Use gcloud to create a key file for the service account that has appropriate permissions. This is a valid approach, but it doesn't address the authentication aspect. The key file will be used for authorization, but it doesn't establish a connection between the on-premises application and the GCP APIs.

			C. Set up direct interconnect between your data center and Google Cloud Platform to enable authentication for your on-premises applications. Direct interconnect is a physical connection between your on-premises network and Google Cloud's network. It's primarily used to improve network performance and reduce latency for data transfers. Authentication is handled separately using service account credentials.

			D. Go to the IAM & admin console, grant a user account permissions similar to the service account permissions, and use this user account for authentication from your data center. Granting a user account permissions similar to the service account is not recommended for on-premises applications. User accounts are typically used for human interaction with GCP resources, while service accounts are more suitable for automated applications.

		In summary, using service account credentials directly within the on-premises application is the most secure and efficient method for enabling authentication to GCP APIs from your data center.

-------

** You have a developer laptop with the Cloud SDK installed on Ubuntu. The Cloud SDK was installed from the Google Cloud Ubuntu package repository. You want to test your application locally on your laptop with Cloud Datastore. What should you do? [Question 103]
	
	A. Export Cloud Datastore data using gcloud datastore export.
	
	B. Create a Cloud Datastore index using gcloud datastore indexes create.
	
	C. Install the google-cloud-sdk-datastore-emulator component using the apt get install command.
	[Another Most Voted and chatGPT Suggested]
	
	D. Install the cloud-datastore-emulator component using the gcloud components install command. [✔️]

	-------

	Bard:

		The correct answer is D. Install the cloud-datastore-emulator component using the gcloud components install command.

			To test your application locally on your laptop with Cloud Datastore, you need to install the Datastore emulator component. This component provides a local emulation of the production Cloud Datastore environment, allowing you to develop and test your application without relying on the real Google Cloud platform.

		Option A. Export Cloud Datastore data using gcloud datastore export is not relevant for testing locally. It is used to export data from Cloud Datastore to a local file or another datastore.

		Option B. Create a Cloud Datastore index using gcloud datastore indexes create is also not directly related to testing locally. Index creation is typically done in production environments to optimize data retrieval.

		Option C. Install the google-cloud-sdk-datastore-emulator component using the apt get install command is incorrect. The apt get install command is used to install packages from the Ubuntu package repository, while the Cloud SDK components are installed using the gcloud components install command.

		Therefore, the correct approach for testing your application locally with Cloud Datastore is to install the cloud-datastore-emulator component using the gcloud components install command. This will provide the necessary emulator environment for local testing.

-------

You need to create a custom VPC with a single subnet. The subnet's range must be as large as possible. Which range should you use? [Question 2]
	
	A. 0.0.0.0/0
	
	B. 10.0.0.0/8 [✔️]
	
	C. 172.16.0.0/12
	
	D. 192.168.0.0/16

	-------

	If you need to create a custom VPC with a single subnet, and the subnet's range should be as large as possible, you should choose a private IP address range that provides a large number of available addresses. Among the options provided:

	B. 10.0.0.0/8
	C. 172.16.0.0/12
	D. 192.168.0.0/16
	
	The largest address space is provided by option B. 10.0.0.0/8. This subnet range covers IP addresses from 10.0.0.0 to 10.255.255.255, offering a total of 16,777,216 (2^24) unique IPv4 addresses. Therefore, choosing option B would give you the largest possible subnet range.

-------

Your organization has user identities in Active Directory. Your organization wants to use Active Directory as their source of truth for identities. Your organization wants to have full control over the Google accounts used by employees for all Google services, including your Google Cloud Platform (GCP) organization. What should you do? [Question 109]
	
	A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity. [✔️]
	
	B. Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity.
	
	C. Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.
	
	D. Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password.

	-------

	A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.

		Google Cloud Directory Sync allows you to synchronize user and group details from an Active Directory server to your Cloud Identity or Google Workspace environment. This helps to maintain consistency between your on-premises identity infrastructure and your Google Cloud environment.

		Using GCDS ensures that changes in your Active Directory, such as user additions, modifications, or deletions, are automatically reflected in your Cloud Identity or Google Workspace environment. This integration helps to manage and provision users across both systems efficiently.

	B. Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity.

		This option involves manual scripting and direct usage of Cloud Identity APIs to synchronize users. While it's technically possible, it is not the recommended approach for most organizations. Writing custom scripts introduces complexity, potential errors, and might require ongoing maintenance to keep pace with changes in both Active Directory and Cloud Identity.

	C. Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console.

		Exporting users from Active Directory to a CSV and manually importing them via the Admin Console is a manual and error-prone process. It lacks automation and real-time synchronization, making it less efficient for managing user identity changes over time. It is not scalable and might lead to inconsistencies between the two systems.

	D. Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password.

		This option lacks centralized control and synchronization. Relying on employees to create Google accounts individually can lead to inconsistencies, increased administrative overhead, and security concerns. It doesn't provide a unified and automated approach to manage user identities across Active Directory and Google Cloud services.

	In summary, using Google Cloud Directory Sync (GCDS) is the recommended approach because it provides a robust, automated, and centralized solution for synchronizing user identities between Active Directory and Cloud Identity, ensuring consistency and efficiency in user management.

	-------

	Bard:

		The correct answer is A. Use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity.

			Google Cloud Directory Sync (GCDS) is a service that allows you to synchronize user identities and groups between Active Directory and Cloud Identity. This is the recommended approach for organizations that want to use Active Directory as their source of truth for identities and have full control over the Google accounts used by employees.

		The other options are not as suitable for this scenario:

			B. Use the cloud Identity APIs and write a script to synchronize users to Cloud Identity. This approach is more technical and may not be feasible for all organizations. Additionally, it would require more ongoing maintenance than using GCDS.

			C. Export users from Active Directory as a CSV and import them to Cloud Identity via the Admin Console. This approach is not recommended because it requires manual data entry and is not as scalable as using GCDS.

			D. Ask each employee to create a Google account using self signup. Require that each employee use their company email address and password. This approach is the most decentralized, but it does not give the organization full control over the Google accounts used by employees. Additionally, it can be more difficult to manage and secure user accounts.

		Therefore, the best approach for this organization is to use Google Cloud Directory Sync (GCDS) to synchronize users into Cloud Identity. This will allow the organization to have full control over the Google accounts used by employees while still taking advantage of Active Directory as their source of truth for identities.

-------

You are building an application that stores relational data from users. Users across the globe will use this application. Your CTO is concerned about the scaling requirements because the size of the user base is unknown. You need to implement a database solution that can scale with your user growth with minimum configuration changes. Which storage solution should you use? [Question 58]
	
	A. Cloud SQL
	
	B. Cloud Spanner [✔️]
	
	C. Cloud Firestore
	
	D. Cloud Datastore

	-------

	In this scenario, where the user base is expected to grow globally, and scalability is a concern, Cloud Spanner is the most suitable choice. Here's why:

	B. Cloud Spanner:

		Global Distribution: Cloud Spanner is designed for globally distributed applications. It provides strong consistency and high availability across multiple regions, making it suitable for applications with a worldwide user base.
		
		Horizontal Scalability: Cloud Spanner horizontally scales across multiple nodes and regions automatically. It can handle increased workloads and storage requirements without significant manual configuration changes.
		
		Relational Data Model: Cloud Spanner supports a relational data model, making it suitable for applications that require complex queries and transactions. It provides the benefits of a relational database with the scalability of a globally distributed, horizontally scalable system.
		
		Automatic Sharding: Cloud Spanner automatically shards and distributes data across nodes and regions, ensuring optimal performance and scalability without manual intervention.
	
	Other Options:

		A. Cloud SQL: While Cloud SQL is a managed relational database service, it might require more manual intervention for scaling, and it may not provide the same level of global distribution and automatic sharding as Cloud Spanner.
		
		C. Cloud Firestore: Cloud Firestore is a NoSQL document database, suitable for flexible, scalable applications. However, if your application requires complex queries and transactions associated with a relational data model, Cloud Spanner might be a better fit.
		
		D. Cloud Datastore: Cloud Datastore is a NoSQL database that provides horizontal scalability. However, Cloud Spanner offers a more advanced feature set, including global distribution and automatic sharding, making it a more suitable choice for large-scale, globally distributed applications.
	
	In summary, Cloud Spanner is the recommended option for a globally distributed application with unknown scaling requirements and relational data needs.

	-------

	Bard:

		The best storage solution for this scenario is B. Cloud Spanner.

			Cloud Spanner is a fully managed globally distributed database service that can seamlessly scale to meet the demands of your growing user base. It automatically distributes data across multiple data centers, ensuring high availability and low latency for users worldwide.

			Cloud Spanner's horizontal scalability makes it ideal for unknown user growth. You don't need to manually provision or manage additional resources as your user base expands. Cloud Spanner will automatically add more servers and replicas to handle the increasing load.

		Compared to other options:

			Cloud SQL is a relational database service that runs on Google's infrastructure, but it's not fully distributed like Cloud Spanner. It requires you to manage and scale the database yourself, which can be more complex and less efficient for large-scale applications.

			Cloud Firestore is a NoSQL database service that is well-suited for mobile and web applications with high read and write workloads. However, it's not as suitable for large-scale relational data storage due to its schemaless nature and lack of ACID guarantees.

			Cloud Datastore is another NoSQL database service that offers a balance between flexibility and scalability. It's a good choice for applications with a moderate amount of data and unpredictable growth patterns.

		In summary, Cloud Spanner is the most suitable choice for this scenario due to its ability to seamlessly scale across multiple regions, handle large amounts of relational data, and meet the demands of a growing global user base. It provides the flexibility and resilience needed to support a wide range of applications with unpredictable growth patterns.

-------

* Your management has asked an external auditor to review all the resources in a specific project. The security team has enabled the Organization Policy called Domain Restricted Sharing on the organization node by specifying only your Cloud Identity domain. You want the auditor to only be able to view, but not modify, the resources in that project. What should you do? [Question 111]
	
	A. Ask the auditor for their Google account, and give them the Viewer role on the project.
	
	B. Ask the auditor for their Google account, and give them the Security Reviewer role on the project. [chatGPT Suggested]
	
	C. Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project. [✔️]
	
	D. Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project.

	-------

	Bard:

		The correct answer is C. Create a temporary account for the auditor in Cloud Identity, and give that account the Viewer role on the project.

			This is the recommended approach for granting access to external auditors while maintaining security. Creating a temporary account for the auditor ensures that their access is limited to the specific project and duration of the audit.

		The other options are not as secure or appropriate for this scenario:

			A. Ask the auditor for their Google account, and give them the Viewer role on the project. This is not secure as it exposes the auditor's personal Google account credentials.

			B. Ask the auditor for their Google account, and give them the Security Reviewer role on the project. While this provides more permissions than the Viewer role, it is still not ideal as it allows the auditor to modify some resources.

			D. Create a temporary account for the auditor in Cloud Identity, and give that account the Security Reviewer role on the project. This is not the best choice as it grants the auditor more permissions than necessary for auditing purposes.

		By creating a temporary account for the auditor and granting them the Viewer role, you can effectively limit their access to the project and ensure that they can only view, but not modify, the resources. This approach strikes a balance between security and providing the auditor with the necessary access to perform their duties.

-------

** You are building an application that processes data files uploaded from thousands of suppliers. Your primary goals for the application are data security and the expiration of aged data. You need to design the application to:
* Restrict access so that suppliers can access only their own data.
* Give suppliers write access to data only for 30 minutes.
* Delete data that is over 45 days old.
You have a very short development cycle, and you need to make sure that the application requires minimal maintenance. Which two strategies should you use? (Choose two.) [Question 159]

	A. Build a lifecycle policy to delete Cloud Storage objects after 45 days. [✔️] 
	
	B. Use signed URLs to allow suppliers limited time access to store their objects. [✔️]
	
	C. Set up an SFTP server for your application, and create a separate user for each supplier.
	
	D. Build a Cloud function that triggers a timer of 45 days to delete objects that have expired. [chatGPT Suggested]
	
	E. Develop a script that loops through all Cloud Storage buckets and deletes any buckets that are older than 45 days. [Suggested]

	-------

	C. Set up an SFTP server for your application, and create a separate user for each supplier.

		Explanation: SFTP servers introduce additional complexity and maintenance overhead. This option is not aligned with the goal of minimal maintenance and may not offer the necessary controls for limiting write access to a specific time window.

	-------

	Bard:

		The two most suitable strategies for this scenario are A. Build a lifecycle policy to delete Cloud Storage objects after 45 days and B. Use signed URLs to allow suppliers limited time access to store their objects.

			A. Build a lifecycle policy to delete Cloud Storage objects after 45 days is the most efficient and least error-prone approach for automatically deleting aged data from Cloud Storage. A lifecycle policy is a managed rule that automatically applies to objects in a bucket, specifying actions to be taken based on object age or other conditions. In this case, you can set a lifecycle rule to delete objects older than 45 days. This eliminates the need for manual intervention or scripts to manage data expiration.

			B. Use signed URLs to allow suppliers limited time access to store their objects is a secure and controlled way to grant suppliers write access to data for a specified duration. Signed URLs are temporary URLs that contain an access token and an expiration time. When a supplier uses the signed URL to upload data, Cloud Storage verifies the access token and allows the upload only within the specified timeframe. This ensures that suppliers can upload data without having permanent access to the bucket.

		While options C. Set up an SFTP server for your application, and create a separate user for each supplier and D. Build a Cloud function that triggers a timer of 45 days to delete objects that have expired could potentially fulfill the requirements, they are less efficient and more prone to errors or manual intervention.

			C. Setting up an SFTP server requires additional setup, configuration, and maintenance, and it may not be scalable for thousands of suppliers. Additionally, managing separate user accounts for each supplier could become cumbersome.

			D. Building a Cloud function to trigger a timer for object deletion involves more complexity and ongoing maintenance. It requires writing and deploying a Cloud function, handling potential errors, and monitoring its operation.

		In summary, A. Building a lifecycle policy for automatic data expiration and B. Using signed URLs for controlled supplier data access are the most effective and maintainable strategies for this scenario. They balance security, efficiency, and minimal ongoing maintenance.

-------

Your company implemented BigQuery as an enterprise data warehouse. Users from multiple business units run queries on this data warehouse. However, you notice that query costs for BigQuery are very high, and you need to control costs. Which two methods should you use? (Choose two.) [Question 96]
	
	A. Split the users from business units to multiple projects. [Bard Suggested] [✔️]
	
	B. Apply a user- or project-level custom query quota for BigQuery data warehouse. [✔️]
	
	C. Create separate copies of your BigQuery data warehouse for each business unit.
	
	D. Split your BigQuery data warehouse into multiple data warehouses for each business unit.
	
	E. Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project. [✔️]

	-------

	The two methods you should use to control costs for BigQuery in the given scenario are:

		B. Apply a user- or project-level custom query quota for BigQuery data warehouse.

			Explanation: By applying custom query quotas at the user or project level, you can limit the amount of resources consumed by users or projects. This helps control costs by preventing excessive resource usage.
		
		E. Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project.

			Explanation: Switching from on-demand pricing to flat-rate pricing allows you to allocate a specific number of slots to each project, which can help control costs. With flat-rate pricing, you pay a fixed monthly fee based on the allocated slots, providing more predictable costs.
		
	Other Options:

		A. Split the users from business units to multiple projects.

			Explanation: While splitting users into multiple projects might provide some isolation, it may not directly control costs. Implementing custom query quotas and choosing the appropriate pricing model are more effective for cost control.
		
		C. Create separate copies of your BigQuery data warehouse for each business unit.

			Explanation: Creating separate copies could lead to data redundancy and increased storage costs. It doesn't directly address the query cost control issue.
		
		D. Split your BigQuery data warehouse into multiple data warehouses for each business unit.

			Explanation: Similar to option C, splitting into multiple data warehouses might introduce complexity and data redundancy without directly addressing the query cost control issue.
		
		In summary, applying custom query quotas and choosing the appropriate pricing model (flat rate) are effective methods for controlling costs in BigQuery.

	-------

	Bard:

		The two most suitable methods for controlling costs in this scenario are A. Split the users from business units to multiple projects and E. Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project.

			A. Split the users from business units to multiple projects allows you to isolate the query activity of each business unit, enabling you to set separate query quotas and monitor individual project usage. This granular approach can help you identify and address excessive query costs more effectively.

			E. Change your BigQuery query model from on-demand to flat rate. Apply the appropriate number of slots to each Project. By switching to the flat-rate pricing model, you can pre-purchase a set of query slots for each project. This eliminates the unpredictable costs associated with on-demand queries and provides a fixed monthly fee. By aligning the number of slots to each project's expected query volume, you can optimize costs while ensuring adequate query capacity.

		Option B. Apply a user- or project-level custom query quota for BigQuery data warehouse addresses the need to control query costs but is not the most suitable approach in this scenario. While it allows you to set limits on query usage, it doesn't provide granular control over costs and doesn't address the issue of fluctuating costs with the on-demand model.

		While options C. Create separate copies of your BigQuery data warehouse for each business unit and D. Split your BigQuery data warehouse into multiple data warehouses for each business unit could potentially isolate query costs, they are generally not recommended due to the overhead of managing multiple data warehouses and the potential for data duplication.

		In summary, A. Project-based isolation and E. Flat-rate pricing with slot allocation are the most effective methods for controlling BigQuery query costs in this scenario. They provide granular control over resource utilization, predictable pricing, and optimized resource allocation.

-------

You are running a data warehouse on BigQuery. A partner company is offering a recommendation engine based on the data in your data warehouse. The partner company is also running their application on Google Cloud. They manage the resources in their own project, but they need access to the BigQuery dataset in your project. You want to provide the partner company with access to the dataset. What should you do? [Question 165]
	
	A. Create a Service Account in your own project, and grant this Service Account access to BigQuery in your project.
	
	B. Create a Service Account in your own project, and ask the partner to grant this Service Account access to BigQuery in their project.
	
	C. Ask the partner to create a Service Account in their project, and have them give the Service Account access to BigQuery in their project.
	
	D. Ask the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project. [✔️]

	-------

	D. Ask the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project.

	Explanation:
		
		In a scenario where another project (partner's project) needs access to a BigQuery dataset in your project, it's best to have the partner create a Service Account in their own project.
	
		The partner can then share the Service Account email address with you, and you can grant the necessary permissions (read or specific roles) on your BigQuery dataset to that Service Account.
		
		This way, the access control is managed on the dataset owner's (your) side, and the partner retains control over the Service Account and its permissions in their own project.
	
		It follows the principle of least privilege, as you grant access only to the specific dataset and don't provide more permissions than necessary.
	
	Other Options:

		Option A and Option B suggest creating a Service Account in your own project. However, in this scenario, it's more appropriate to have the partner manage their own Service Account for access to your dataset.
		
		Option C suggests the partner creating a Service Account in their project and giving it access to BigQuery in their project. However, in this scenario, the access needs to be granted to your dataset, so they should have access to your project's dataset.

	-------

	Bard:

		The correct answer is D. Ask the partner to create a Service Account in their project, and grant their Service Account access to the BigQuery dataset in your project.

			This approach is the most secure and recommended way to grant access to your BigQuery dataset to an external partner. By creating a Service Account in their own project, you can tightly control the permissions granted to the partner, ensuring that they only have access to the data they need to perform their tasks.

		The other options are less secure:

			A. Create a Service Account in your own project, and grant this Service Account access to BigQuery in your project. This option would give the partner too much access to your data, as they would be able to see and potentially modify all of the data in your project.

			B. Create a Service Account in your own project, and ask the partner to grant this Service Account access to BigQuery in their project. This option would require the partner to give you their project credentials, which is not a secure practice.

			C. Ask the partner to create a Service Account in their project, and have them give the Service Account access to BigQuery in their project. This option is not the most secure, as the partner would still have full control over the Service Account.

		Therefore, the best approach is to have the partner create a Service Account in their project and grant you access to the BigQuery dataset in their project. This will give you the necessary access to the data while still maintaining a secure separation of permissions.

-------

* You have an application that receives SSL-encrypted TCP traffic on port 443. Clients for this application are located all over the world. You want to minimize latency for the clients. Which load balancing option should you use? [Question 127]
	
	A. HTTPS Load Balancer [Bard and chatGPT Suggested] [✔️]
	
	B. Network Load Balancer
	
	C. SSL Proxy Load Balancer [✔️]
	
	D. Internal TCP/UDP Load Balancer. Add a firewall rule allowing ingress traffic from 0.0.0.0/0 on the target instances.

	-------

	A. HTTPS Load Balancer

		Explanation:

		The HTTPS Load Balancer is designed for SSL-encrypted traffic on port 443 and provides global load balancing, which helps minimize latency for clients distributed worldwide.
		
		It can distribute incoming HTTPS traffic across multiple backend instances and locations to optimize performance and ensure high availability.
		
		HTTPS Load Balancer supports SSL termination at the load balancer, allowing the load balancer to offload SSL processing from the backend instances.
		
		This is suitable for applications that receive SSL-encrypted traffic, especially when low latency is a priority for clients around the world.

	B. Network Load Balancer: Network Load Balancer operates at the transport layer (Layer 4) and doesn't handle SSL termination. It's suitable for non-HTTP(S) traffic and may not be the best choice for SSL-encrypted applications.
	
	C. SSL Proxy Load Balancer: SSL Proxy Load Balancer handles SSL termination but does not support global load balancing. It's designed for SSL-terminated traffic but may not provide optimal latency for clients distributed worldwide.
	
	D. Internal TCP/UDP Load Balancer: This is used for load balancing internal TCP/UDP traffic and is not suitable for handling global SSL-encrypted traffic from clients worldwide.

	-------

	Bard:

		The correct answer is A. HTTPS Load Balancer.

			An HTTPS Load Balancer is specifically designed to handle SSL-encrypted traffic on port 443. It provides a secure and efficient way to distribute traffic to application servers.

		The other options are not as suitable for this scenario:

			B. Network Load Balancer can handle both TCP and UDP traffic, but it is not optimized for SSL-encrypted traffic. It would need to decrypt the traffic before forwarding it to the application servers, which can add latency.

			C. SSL Proxy Load Balancer is a type of proxy that can terminate SSL connections and forward the decrypted traffic to the application servers. However, it is not as efficient as an HTTPS Load Balancer because it requires additional processing to terminate and re-encrypt the traffic.

			D. Internal TCP/UDP Load Balancer is designed for traffic within a private network. It would not be able to accept traffic from clients on the public internet. Additionally, it would require a firewall rule to allow ingress traffic from 0.0.0.0/0, which is a security risk.

		Therefore, an HTTPS Load Balancer is the most suitable option for this scenario because it is designed to handle SSL-encrypted traffic on port 443 efficiently and securely.

-------

* You are deploying a production application on Compute Engine. You want to prevent anyone from accidentally destroying the instance by clicking the wrong button. What should you do? [Question 157]
	
	A. Disable the flag. Delete boot disk when instance is deleted. [Suggested]
	
	B. Enable delete protection on the instance. [✔️]
	
	C. Disable Automatic restart on the instance.
	
	D. Enable Preemptibility on the instance.

	-------

	The appropriate option is:

	B. Enable delete protection on the instance.

		Explanation:
			
			Enabling delete protection on the instance helps prevent accidental deletion of the instance. This is a useful safety measure to avoid unintentional data loss or service disruption.
			
			With delete protection enabled, attempting to delete the instance through the Google Cloud Console or the API will result in an error, reducing the risk of accidental deletions.
			
			This feature adds an additional layer of security by requiring users to explicitly disable delete protection before deleting the instance.
		
	Other Options:

		A. Disable the flag "Delete boot disk when instance is deleted": While this prevents the deletion of the boot disk along with the instance, it does not protect against accidental deletion of the entire instance.
		
		C. Disable Automatic restart on the instance: This setting controls whether the instance should automatically restart when it is terminated. It does not prevent manual deletions.
		
		D. Enable Preemptibility on the instance: Preemptible instances are short-lived and can be terminated by Google Cloud at any time. Enabling preemptibility does not protect against manual deletions but rather makes the instance preemptible.

	-------

	Bard:

		The correct answer is B. Enable delete protection on the instance.

			Delete protection is a security feature that prevents instances from being deleted accidentally. When delete protection is enabled, the instance must be first unprotected before it can be deleted. This prevents anyone from accidentally deleting the instance by clicking the wrong button.

	The other options are not as suitable for this scenario:

		A. Disabling the flag. Delete boot disk when instance is deleted will not prevent the instance from being destroyed. The instance will still be destroyed, but the boot disk will be deleted as well.

		C. Disabling Automatic restart on the instance will not prevent the instance from being destroyed if it is restarted manually.

		D. Enabling Preemptibility on the instance will allow the instance to be preempted by Google Cloud, even if delete protection is enabled.

	Therefore, the best option is to enable delete protection on the instance. This will prevent the instance from being accidentally deleted, even if someone clicks the wrong button.

-------

You significantly changed a complex Deployment Manager template and want to confirm that the dependencies of all defined resources are properly met before committing it to the project. You want the most rapid feedback on your changes. What should you do? [Question 22]
	
	A. Use granular logging statements within a Deployment Manager template authored in Python.
	
	B. Monitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console.
	
	C. Execute the Deployment Manager template against a separate project with the same configuration, and monitor for failures.
	
	D. Execute the Deployment Manager template using the -preview option in the same project, and observe the state of interdependent resources. [✔️]

	-------

	D. Execute the Deployment Manager template using the "-preview" option in the same project, and observe the state of interdependent resources.

	Explanation:
		
		The "-preview" option allows you to execute a Deployment Manager template in a preview mode, which means that the changes are not applied to the actual resources but are validated to ensure that dependencies are met and the deployment will succeed.
		
		By using the "-preview" option, you can quickly validate the changes without making any modifications to the existing resources in the project. It helps you catch any issues related to dependencies or resource conflicts before committing the changes.
		
		This provides a rapid feedback loop, allowing you to identify and address any problems in the Deployment Manager template before applying the changes to the production environment.
	
	Other Options:

		A. Use granular logging statements within a Deployment Manager template authored in Python: While logging statements can help in debugging, they may not provide a comprehensive validation of dependencies.
		
		B. Monitor activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console: This is useful for reviewing logs during and after the deployment but may not provide a preview of the changes.
		
		C. Execute the Deployment Manager template against a separate project with the same configuration, and monitor for failures: This could be a valid approach, but using the "-preview" option in the same project provides a more direct and efficient way to validate changes.

	-------

	Bard:

		The correct answer is D. Execute the Deployment Manager template using the -preview option in the same project, and observe the state of interdependent resources.

			This is the most rapid and direct way to confirm that the dependencies of all defined resources are properly met. The -preview option allows you to run the deployment without actually modifying any resources, so you can quickly identify any errors or conflicts.

		The other options are not as suitable for this scenario:

			A. Using granular logging statements within a Deployment Manager template authored in Python can be helpful for debugging, but it is not the most efficient way to identify resource dependencies.

			B. Monitoring activity of the Deployment Manager execution on the Stackdriver Logging page of the GCP Console is a good way to troubleshoot problems that arise during deployment, but it does not provide real-time feedback on the status of resource dependencies.

			C. Executing the Deployment Manager template against a separate project with the same configuration can be useful for testing, but it is not as efficient as using the -preview option. Additionally, it can be difficult to keep the two projects synchronized.

-------

For analysis purposes, you need to send all the logs from all of your Compute Engine instances to a BigQuery dataset called platform-logs. You have already installed the Stackdriver Logging agent on all the instances. You want to minimize cost. What should you do? [Question 68]
	
	A. 1. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. 2. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs.
	
	B. 1. In Stackdriver Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. 2. Create a Cloud Function that is triggered by messages in the logs topic. 3. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset.
	
	C. 1. In Stackdriver Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination. [✔️]
	
	D. 1. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. 2. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY) 3. Use Cloud Scheduler to trigger this Cloud Function once a day.

	-------

	C. 1. In Stackdriver Logging, create a filter to view only Compute Engine logs. 2. Click Create Export. 3. Choose BigQuery as Sink Service, and the platform-logs dataset as Sink Destination.

	Explanation:
		
		This option utilizes Stackdriver Logging and BigQuery integration to export Compute Engine logs to a BigQuery dataset.
		
		By creating a filter to view only Compute Engine logs in Stackdriver Logging, you can specify the logs you want to export to BigQuery.
		
		The export is set up using Stackdriver Logging, and the destination is chosen as BigQuery with the specified dataset (platform-logs).
		
		This approach is cost-effective and directly exports relevant logs to BigQuery without the need for additional services or Cloud Functions.
	
	Other Options:

		A. Give the BigQuery Data Editor role on the platform-logs dataset to the service accounts used by your instances. Update your instances' metadata to add the following value: logs-destination: bq://platform-logs:

			This involves granting broader permissions (Data Editor role) to service accounts, and the metadata update 	might not be necessary for achieving the log export.
		
		B. In Stackdriver Logging, create a logs export with a Cloud Pub/Sub topic called logs as a sink. Create a Cloud Function that is triggered by messages in the logs topic. Configure that Cloud Function to drop logs that are not from Compute Engine and to insert Compute Engine logs in the platform-logs dataset:

			This involves additional complexity with Cloud Pub/Sub and Cloud Function, which may not be necessary for the given scenario.
		
		D. Create a Cloud Function that has the BigQuery User role on the platform-logs dataset. Configure this Cloud Function to create a BigQuery Job that executes this query: INSERT INTO dataset.platform-logs (timestamp, log) SELECT timestamp, log FROM compute.logs WHERE timestamp > DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY). Use Cloud Scheduler to trigger this Cloud Function once a day:

			This involves additional Cloud Function logic and scheduling to perform periodic inserts, which might not be the most straightforward solution for continuous log export. The continuous export option provided by Stackdriver Logging is often more efficient for this use case.

-------

* You need to configure optimal data storage for files stored in Cloud Storage for minimal cost. The files are used in a mission-critical analytics pipeline that is used continually. The users are in Boston, MA (United States). What should you do?
	
	A. Configure regional storage for the region closest to the users. Configure a Nearline storage class.
	
	B. Configure regional storage for the region closest to the users. Configure a Standard storage class. [✔️]
	
	C. Configure dual-regional storage for the dual region closest to the users. Configure a Nearline storage class.
	
	D. Configure dual-regional storage for the dual region closest to the users. Configure a Standard storage class.
	[Another Most Voted]

	-------

	B. Configure regional storage for the region closest to the users. Configure a Standard storage class.

		Explanation:
			
			In this scenario, you want to balance optimal data access performance and minimal cost for a mission-critical analytics pipeline.
			
			Choosing regional storage in the region closest to the users (Boston, MA) helps minimize latency and provides good performance for data access.
			
			The Standard storage class is suitable for frequently accessed data, and it provides a good balance between performance and cost.
			
			Dual-regional storage might be overkill for this scenario unless there are specific requirements for high availability across multiple regions, and it comes with additional costs.
		
		Other Options:

			A. Configure regional storage for the region closest to the users. Configure a Nearline storage class:

				Nearline storage is designed for infrequently accessed data. Given that the data is used continually, the Standard storage class would be a more appropriate choice for better performance.
			
			C. Configure dual-regional storage for the dual region closest to the users. Configure a Nearline storage class:

				Dual-regional storage might be unnecessary complexity and cost for this scenario unless there are specific requirements for data redundancy across multiple regions.
		
	D. Configure dual-regional storage for the dual region closest to the users. Configure a Standard storage class:

		Similar to option C, dual-regional storage might be overkill, and the Standard storage class is more appropriate for frequently accessed data.

-------

** Your company has an internal application for managing transactional orders. The application is used exclusively by employees in a single physical location. The application requires strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. The first version of the application is implemented in PostgreSQL, and you want to deploy it to the cloud with minimal code changes. Which database is most appropriate for this application?
[Question 188]
	
	A. BigQuery
	
	B. Cloud SQL [✔️]
	
	C. Cloud Spanner [Bard and chatGPT recommended] [✔️]
	
	D. Cloud Datastore

	-------

	The most appropriate database for an application that requires strong consistency, fast queries, ACID guarantees for multi-table transactional updates, and minimal code changes for deployment in the cloud is:

	C. Cloud Spanner

		Explanation:
			
			Cloud Spanner is a globally distributed, strongly consistent, and horizontally scalable database service.
			
			It provides ACID transactions and supports SQL queries, making it suitable for multi-table transactional updates.
			
			Cloud Spanner's global distribution allows it to provide low-latency access to data for users in different physical locations.
		
	Cloud SQL (Option B) is a fully managed relational database service but doesn't provide the same global distribution and scalability features as Cloud Spanner.
	
	BigQuery (Option A) is a serverless, highly scalable, and cost-effective multi-cloud data warehouse, but it's not designed for transactional updates and doesn't provide ACID guarantees.
	
	Cloud Datastore (Option D) is a NoSQL document database, and while it's good for certain use cases, it may not be the best fit for applications requiring strong consistency and multi-table transactional updates.

	-------

	Bard:

		Cloud Spanner is the most suitable database for this application because it provides strong consistency, fast queries, and ACID guarantees for multi-table transactional updates. It is also a good choice for applications that need to be deployed to the cloud with minimal code changes, as it is compatible with PostgreSQL and other relational databases.

		The other options are not as suitable for this application:

			A. BigQuery is a data warehouse, not a transactional database. It is not designed for applications that need strong consistency and fast queries.

			B. Cloud SQL is a managed relational database service. It can provide strong consistency and fast queries, but it is not as scalable as Cloud Spanner. Additionally, Cloud SQL is not compatible with PostgreSQL, so you would need to rewrite your application to use a different database dialect.

			D. Cloud Datastore is a NoSQL database. It does not provide strong consistency, and it is not as efficient for transactional applications as Cloud Spanner. Additionally, Cloud Datastore is not compatible with PostgreSQL, so you would need to rewrite your application to use a different database technology.

		Therefore, Cloud Spanner is the most appropriate database for this application because it provides the features and compatibility that the application needs.

-------

* You need to create an autoscaling managed instance group for an HTTPS web application. You want to make sure that unhealthy VMs are recreated. What should you do? [Question 51]
	
	A. Create a health check on port 443 and use that when creating the Managed Instance Group. [✔️]
	
	B. Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group.
	
	C. In the Instance Template, add the label 'health-check'. [Suggested]
	
	D. In the Instance Template, add a startup script that sends a heartbeat to the metadata server.

	-------

	To create an autoscaling managed instance group for an HTTPS web application and ensure that unhealthy VMs are recreated, you should:

	A. Create a health check on port 443 and use that when creating the Managed Instance Group.

	Explanation:
	
		Health checks are used to determine the health of instances within a managed instance group.
		
		By creating a health check on port 443 (HTTPS), the load balancer can monitor the health of instances serving the HTTPS web application.
		
		If an instance fails the health check, the managed instance group can automatically recreate the instance to maintain the desired number of healthy instances.
		
	Selecting Multi-Zone (Option B) or adding a label (Option C) doesn't directly address the requirement for health checks and recreation of unhealthy VMs.
		
	Using a startup script (Option D) for sending a heartbeat to the metadata server is not a typical or recommended approach for health checking and autoscaling managed instance groups.
		
	Therefore, creating a health check on port 443 is the appropriate choice to ensure that unhealthy VMs are recreated in response to failures.

	-------

	Bard:

		The correct answer is A. Create a health check on port 443 and use that when creating the Managed Instance Group.

			A health check is a mechanism for determining whether an instance is healthy and can serve traffic. When creating an autoscaling managed instance group for an HTTPS web application, you should create a health check that verifies that the instance can respond to HTTPS requests on port 443. This will ensure that unhealthy VMs are removed from the instance group and replaced with new ones.

		The other options are not as suitable for this scenario:

			B. Selecting Multi-Zone instead of Single-Zone when creating the Managed Instance Group would increase the availability of the instance group, but it would not automatically remove unhealthy instances.

			C. Adding the label 'health-check' to the Instance Template is not a valid method for creating a health check.

			D. Adding a startup script that sends a heartbeat to the metadata server would require you to manually monitor the health of the instances.

		Therefore, creating a health check on port 443 and using that when creating the Managed Instance Group is the most effective way to ensure that unhealthy VMs are recreated.

-------

You need to enable traffic between multiple groups of Compute Engine instances that are currently running two different GCP projects. Each group of Compute Engine instances is running in its own VPC. What should you do? [Question 79]
	
	A. Verify that both projects are in a GCP Organization. Create a new VPC and add all instances.
	
	B. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC. [✔️]
	
	C. Verify that you are the Project Administrator of both projects. Create two new VPCs and add all instances.
	
	D. Verify that you are the Project Administrator of both projects. Create a new VPC and add all instances.

	-------

	B. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC.

	Explanation:
		
		The best option for enabling traffic between multiple groups of Compute Engine instances running in different projects and VPCs is to use Shared VPC.
		
		Shared VPC allows a host project to share its VPC network with other service projects. This helps in connecting instances across projects.
		
		If both projects are in a GCP Organization, you can set up a Shared VPC to enable communication between instances in different projects.
	
	Options A, C, and D don't explicitly address the need for instances in different projects to communicate. Creating new VPCs in each project (Options A and C) or creating a new VPC (Option D) doesn't provide the cross-project connectivity required for the scenario.
	
	Therefore, using Shared VPC (Option B) is the recommended approach for enabling traffic between multiple groups of Compute Engine instances in different projects.

	-------

	Bard:

		The correct answer is B. Verify that both projects are in a GCP Organization. Share the VPC from one project and request that the Compute Engine instances in the other project use this shared VPC.

			This is the most efficient and scalable solution for enabling traffic between Compute Engine instances in two different projects. By sharing the VPC from one project, you can effectively extend the network topology to include the instances in the other project. This allows them to communicate with each other as if they were in the same VPC.

		The other options are not as suitable for this scenario:

			A. Creating a new VPC and adding all instances would require you to manage two separate VPC networks, which can be more complex and less scalable.

			C. Creating two new VPCs and adding all instances would create two separate network segments, which would make it more difficult for the instances to communicate with each other.

			D. Verifying that you are the Project Administrator of both projects is a necessary step, but it is not sufficient to enable traffic between the instances. You also need to share the VPC from one project with the other.

		Therefore, sharing the VPC from one project and requesting that the Compute Engine instances in the other project use this shared VPC is the most effective way to enable traffic between the instances.