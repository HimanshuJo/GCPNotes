* Your company publishes large files on an Apache web server that runs on a Compute Engine instance. The Apache web server is not the only application running in the project. You want to receive an email when the egress network costs for the server exceed 100 dollars for the current month as measured by Google Cloud.
What should you do? [Question 14]

	A. Set up a budget alert on the project with an amount of 100 dollars, a threshold of 100%, and notification type of email. [Suggested]

	B. Set up a budget alert on the billing account with an amount of 100 dollars, a threshold of 100%, and notification type of email.

	C. Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly. [✔️]

	D. Use the Cloud Logging Agent to export the Apache web server logs to Cloud Logging. Create a Cloud Function that uses BigQuery to parse the HTTP response log data in Cloud Logging for the current month and sends an email if the size of all HTTP responses, multiplied by current Google Cloud egress prices, totals over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly.

	-------

	A. Set up a budget alert on the project with an amount of 100 dollars, a threshold of 100%, and notification type of "email."

		This option involves setting up a budget alert directly on the project. However, budget alerts in GCP may not be real-time and could have a delay. Also, budget alerts are usually for monitoring overall project spending and may not provide detailed insights into specific resources like egress costs for the Apache web server.
	
	B. Set up a budget alert on the billing account with an amount of 100 dollars, a threshold of 100%, and notification type of "email."

		This option is more appropriate than Option A. Setting up a budget alert on the billing account would give a broader view of the costs, and it's likely to capture egress costs effectively. However, it still might not be real-time and could have some delay.
	
	C. Export the billing data to BigQuery. Create a Cloud Function that uses BigQuery to sum the egress network costs of the exported billing data for the Apache web server for the current month and sends an email if it is over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly.

		This option provides a more detailed and customizable solution. Exporting billing data to BigQuery allows for more granular analysis of costs. The Cloud Function can perform specific calculations for the Apache web server's egress costs, and it can run hourly for near-real-time monitoring.
	
	D. Use the Stackdriver Logging Agent to export the Apache web server logs to Stackdriver Logging. Create a Cloud Function that uses BigQuery to parse the HTTP response log data in Stackdriver for the current month and sends an email if the size of all HTTP responses, multiplied by current GCP egress prices, totals over 100 dollars. Schedule the Cloud Function using Cloud Scheduler to run hourly.

		This option involves monitoring Apache web server logs, but it may not be the most straightforward way to monitor egress costs. It introduces additional complexity by parsing logs and calculating costs based on HTTP response size. It might not be as accurate and reliable as directly analyzing billing data.
	
	Conclusion:
		
		Option C seems to be the most appropriate choice. Exporting billing data to BigQuery allows for detailed analysis, and scheduling a Cloud Function hourly provides a near-real-time monitoring solution. Option B is a reasonable alternative, but it might not be as detailed and customizable as Option C. Options A and D, while addressing monitoring, have limitations and complexities that make them less suitable for this specific use case.

-------

* You have designed a solution on Google Cloud that uses multiple Google Cloud products. Your company has asked you to estimate the costs of the solution. You need to provide estimates for the monthly total cost. What should you do? [Question 15]
	
	A. For each Google Cloud product in the solution, review the pricing details on the products pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product. [✔️]

	B. For each Google Cloud product in the solution, review the pricing details on the products pricing page. Create a Google Sheet that summarizes the expected monthly costs for each product.
	
	C. Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Navigate to the Billing Report page in the Cloud Console. Multiply the 1 week cost to determine the monthly costs.
	[Suggested]
	
	D. Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Use Cloud Monitoring to determine the provisioned and used resource amounts. Multiply the 1 week cost to determine the monthly costs.

	-------

	Let's evaluate each option:

	A. For each Google Cloud product in the solution, review the pricing details on the product's pricing page. Use the pricing calculator to total the monthly costs for each Google Cloud product.

		This option is a common and recommended practice. Reviewing pricing details and using the pricing calculator provides a precise and reliable estimate based on the expected usage of each service.
	
	B. For each Google Cloud product in the solution, review the pricing details on the product's pricing page. Create a Google Sheet that summarizes the expected monthly costs for each product.

		This option is similar to Option A but involves using a Google Sheet for summarizing costs. While this can be a helpful way to organize information, it adds an extra step and may not provide any significant advantage over using the pricing calculator directly.
	
	C. Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Navigate to the Billing Report page in the Cloud Console. Multiply the 1 week cost to determine the monthly costs.

		This option is not recommended. Provisioning the solution for only one week and then extrapolating the monthly cost might not accurately represent the expected usage patterns. It can lead to inaccurate estimates, especially if the usage fluctuates during different times of the month.
	
	D. Provision the solution on Google Cloud. Leave the solution provisioned for 1 week. Use Cloud Monitoring to determine the provisioned and used resource amounts. Multiply the 1 week cost to determine the monthly costs.

		This option involves using Cloud Monitoring to determine resource usage. While it's a more detailed approach than Option C, it may still not capture all nuances of resource usage patterns over a month. Additionally, Cloud Monitoring costs may not be fully reflective of the overall Google Cloud service costs.
	
	Conclusion:
		
		Option A is the most reliable and commonly used method for estimating costs. It allows you to review the pricing details for each service and use the pricing calculator to get a precise estimate based on expected usage. Option B adds unnecessary complexity without significant benefits. Options C and D involve provisioning resources for a short time and extrapolating costs, which is not recommended for accurate cost estimation.

-------

* You have an application that receives SSL-encrypted TCP traffic on port 443. Clients for this application are located all over the world. You want to minimize latency for the clients. Which load balancing option should you use? 
[Question 127]
	
	A. HTTPS Load Balancer [Bard and chatGPT Suggested] [✔️]
	
	B. Network Load Balancer
	
	C. SSL Proxy Load Balancer [✔️]
	
	D. Internal TCP/UDP Load Balancer. Add a firewall rule allowing ingress traffic from 0.0.0.0/0 on the target instances.

	-------

	A. HTTPS Load Balancer

		Explanation:

		The HTTPS Load Balancer is designed for SSL-encrypted traffic on port 443 and provides global load balancing, which helps minimize latency for clients distributed worldwide.
		
		It can distribute incoming HTTPS traffic across multiple backend instances and locations to optimize performance and ensure high availability.
		
		HTTPS Load Balancer supports SSL termination at the load balancer, allowing the load balancer to offload SSL processing from the backend instances.
		
		This is suitable for applications that receive SSL-encrypted traffic, especially when low latency is a priority for clients around the world.

	B. Network Load Balancer: Network Load Balancer operates at the transport layer (Layer 4) and doesn't handle SSL termination. It's suitable for non-HTTP(S) traffic and may not be the best choice for SSL-encrypted applications.

	C. SSL Proxy Load Balancer: SSL Proxy Load Balancer handles SSL termination but does not support global load balancing. It's designed for SSL-terminated traffic but may not provide optimal latency for clients distributed worldwide.

	D. Internal TCP/UDP Load Balancer: This is used for load balancing internal TCP/UDP traffic and is not suitable for handling global SSL-encrypted traffic from clients worldwide.

	-------

	Bard:

		The correct answer is A. HTTPS Load Balancer.

			An HTTPS Load Balancer is specifically designed to handle SSL-encrypted traffic on port 443. It provides a secure and efficient way to distribute traffic to application servers.

		The other options are not as suitable for this scenario:

			B. Network Load Balancer can handle both TCP and UDP traffic, but it is not optimized for SSL-encrypted traffic. It would need to decrypt the traffic before forwarding it to the application servers, which can add latency.

			C. SSL Proxy Load Balancer is a type of proxy that can terminate SSL connections and forward the decrypted traffic to the application servers. However, it is not as efficient as an HTTPS Load Balancer because it requires additional processing to terminate and re-encrypt the traffic.

			D. Internal TCP/UDP Load Balancer is designed for traffic within a private network. It would not be able to accept traffic from clients on the public internet. Additionally, it would require a firewall rule to allow ingress traffic from 0.0.0.0/0, which is a security risk.

		Therefore, an HTTPS Load Balancer is the most suitable option for this scenario because it is designed to handle SSL-encrypted traffic on port 443 efficiently and securely.

-------

* You have an application on a general-purpose Compute Engine instance that is experiencing excessive disk read throttling on its Zonal SSD Persistent Disk. The application primarily reads large files from disk. The disk size is currently 350 GB. You want to provide the maximum amount of throughput while minimizing costs.
What should you do? [Question 128]

	A. Increase the size of the disk to 1 TB.
	
	B. Increase the allocated CPU to the instance.
	
	C. Migrate to use a Local SSD on the instance. [✔️]
	
	D. Migrate to use a Regional SSD on the instance. [Bard and ChatGPT Suggested] [✔️]

	-------

	In the scenario where an application on a general-purpose Compute Engine instance is experiencing excessive disk read throttling on its Zonal SSD Persistent Disk, and the primary operation involves reading large files from disk, let's evaluate each option:

		A. Increase the size of the disk to 1 TB.

			Increasing the disk size can potentially improve throughput because larger disks generally offer higher I/O performance. However, this might not be the most cost-effective solution, and it doesn't guarantee that the disk read throttling will be fully addressed.
	
		B. Increase the allocated CPU to the instance.

			Increasing CPU may help if the bottleneck is related to CPU processing power. However, if the primary issue is disk read throttling, addressing the CPU alone might not be the most efficient solution.
	
		* C. Migrate to use a Local SSD on the instance.

			Local SSDs are physically attached to the host machine and can provide very high I/O performance. However, they come with limitations such as data loss when the instance is stopped or terminated. Additionally, local SSDs are ephemeral and may not be suitable for all use cases, especially if data persistence is crucial.
	
		* D. Migrate to use a Regional SSD on the instance.

			Regional SSDs provide higher throughput and IOPS compared to Zonal SSDs. Migrating to a Regional SSD can be a more effective solution for improving disk performance. However, it's essential to consider the associated costs, as regional resources may be more expensive than zonal resources.
	
	Conclusion:
		
		Considering the goal of providing the maximum amount of throughput while minimizing costs, the most suitable option might be D. Migrate to use a Regional SSD on the instance. This option leverages the higher throughput capabilities of Regional SSDs, which could address the disk read throttling issue, while still being a more cost-effective solution compared to increasing the disk size to 1 TB. However, it's crucial to review the cost implications and performance characteristics of regional resources in the specific use case.

	-------

	Bard:

		The correct answer is D. Migrate to use a Regional SSD on the instance.

			Regional SSDs provide higher throughput than Zonal SSDs, and they are also more resilient to failures. This makes them a better choice for applications that experience excessive disk read throttling.

		The other options are not as suitable for this scenario:

			A. Increasing the size of the disk to 1 TB will not necessarily increase the throughput of the disk. The throughput of a Zonal SSD is limited by the performance of the underlying hardware, and increasing the disk size will not change the performance of the hardware.
		
			B. Increasing the allocated CPU to the instance will not help to improve disk read throughput. The CPU is not the bottleneck for disk read operations. The bottleneck is the storage hardware itself.
			
			C. Migrating to use a Local SSD on the instance will not provide the same level of throughput as a Regional SSD. Local SSDs are designed for low-latency access, not high throughput.
		
		Therefore, the best option is to migrate to use a Regional SSD on the instance. This will provide the maximum amount of throughput while minimizing costs.

-------

* Your Dataproc cluster runs in a single Virtual Private Cloud (VPC) network in a single subnet with range 172.16.20.128/25. There are no private IP addresses available in the VPC network. You want to add new VMs to communicate with your cluster using the minimum number of steps. What should you do? [Question 129]
	
	A. Modify the existing subnet range to 172.16.20.0/24. [Another Most Voted]
	
	B. Create a new Secondary IP Range in the VPC and configure the VMs to use that range.
	
	C. Create a new VPC network for the VMs. Enable VPC Peering between the VMs' VPC network and the Dataproc cluster VPC network. [✔️]
	
	D. Create a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange.

	-------

	A. Modify the existing subnet range to 172.16.20.0/24.

		Modifying the existing subnet range might cause disruptions to the existing resources and is generally not recommended. Changing the subnet range would also affect the Dataproc cluster, and it may not be the most efficient solution.
	
	B. Create a new Secondary IP Range in the VPC and configure the VMs to use that range.

		This option involves adding a secondary IP range to the existing subnet. While this can provide additional IP addresses, it may not be the most scalable solution, and it doesn't guarantee that the VMs will get unique IP addresses, especially if the original subnet is fully utilized.
	
	C. Create a new VPC network for the VMs. Enable VPC Peering between the VMs' VPC network and the Dataproc cluster VPC network.

		This option is a valid approach. Creating a new VPC network for the VMs and enabling VPC peering allows communication between the Dataproc cluster VPC and the VMs' VPC. This solution provides isolation and allows for independent scaling of resources.
	
	D. Create a new VPC network for the VMs with a subnet of 172.32.0.0/16. Enable VPC network Peering between the Dataproc VPC network and the VMs VPC network. Configure a custom Route exchange.

		While this option technically could work, it introduces a new VPC network with a different subnet. This may add unnecessary complexity, and VPC peering alone (Option C) should be sufficient for achieving the desired communication.
	
	Conclusion:
		
		Option C, "Create a new VPC network for the VMs. Enable VPC Peering between the VMs' VPC network and the Dataproc cluster VPC network," seems to be the most straightforward and scalable solution. It allows for independent scaling of resources, provides isolation, and enables communication between the Dataproc cluster and the new VMs' VPC.

	-------

	Bard:

		The correct answer is C. Create a new VPC network for the VMs. Enable VPC Peering between the VMs' VPC network and the Dataproc cluster VPC network.

			This is the most efficient and straightforward way to add new VMs to communicate with your cluster. By creating a new VPC network for the VMs, you can isolate the new VMs from the Dataproc cluster's network and control access between them. Then, you can enable VPC Peering between the two VPC networks to allow the VMs to communicate with the Dataproc cluster.

		The other options are not as suitable for this scenario:

			A. Modifying the existing subnet range to 172.16.20.0/24 would exhaust the available private IP addresses in the VPC network.

			B. Creating a new Secondary IP Range in the VPC would not provide enough private IP addresses for the new VMs.

			D. Creating a new VPC network for the VMs with a subnet of 172.32.0.0/16 would require configuring a custom Route exchange, which is more complex than simply enabling VPC Peering.

		Therefore, creating a new VPC network for the VMs and enabling VPC Peering with the Dataproc cluster's VPC network is the most effective way to add new VMs and maintain network isolation.

-------

* You manage an App Engine Service that aggregates and visualizes data from BigQuery. The application is deployed with the default App Engine Service account. The data that needs to be visualized resides in a different project managed by another team. You do not have access to this project, but you want your application to be able to read data from the BigQuery dataset. What should you do? [Question 130]
	
	A. Ask the other team to grant your default App Engine Service account the role of BigQuery Job User. [✔️]
	(Option A (BigQuery Job User) provides broader access than necessary, allowing users to run queries and potentially interact with the dataset in ways beyond just viewing. It is always advisable to grant the least privilege required for the task.)
	
	B. Ask the other team to grant your default App Engine Service account the role of BigQuery Data Viewer.
	[Bard and ChatGPT Suggested] [✔️]
	
	C. In Cloud IAM of your project, ensure that the default App Engine service account has the role of BigQuery Data Viewer.
	
	D. In Cloud IAM of your project, grant a newly created service account from the other team the role of BigQuery Job User in your project.

	-------

	Bard:

		The correct answer is B. Ask the other team to grant your default App Engine Service account the role of BigQuery Data Viewer.

			This is the most secure and appropriate approach for accessing the data from the other team's project. By granting the default App Engine Service account the role of BigQuery Data Viewer, you are providing the application with the least amount of access necessary to complete the task. The Data Viewer role allows the application to read data from the BigQuery dataset, but it does not grant the ability to modify the dataset or run queries.

		The other options are not as suitable for this scenario:

			A. Granting the role of BigQuery Job User provides broader access than necessary, allowing users to run queries and potentially interact with the dataset in ways beyond just viewing. It is always advisable to grant the least privilege required for the task.

			C. Granting the role of BigQuery Data Viewer directly in your project would grant the default App Engine Service account access to the dataset stored in the other team's project, which is not the desired outcome.

			D. Granting the role of BigQuery Job User to a newly created service account from the other team would require the other team to create and manage a service account, which could be a burden for them.

		Therefore, requesting the other team to grant the default App Engine Service account the role of BigQuery Data Viewer is the most secure and appropriate way to access the data from the other team's project.

-------

You need to create a copy of a custom Compute Engine virtual machine (VM) to facilitate an expected increase in application traffic due to a business acquisition. What should you do? [Question 131]
	
	A. Create a Compute Engine snapshot of your base VM. Create your images from that snapshot.
	
	B. Create a Compute Engine snapshot of your base VM. Create your instances from that snapshot. [Another Most Voted]
	(While creating VMs directly from a snapshot is quicker and more straightforward, using an intermediate custom image offers enhanced management capabilities, version control, and the ability to maintain consistent configurations across multiple instances. The choice between the two approaches depends on the specific requirements of the deployment scenario and the need for customization and control.)
	
	C. Create a custom Compute Engine image from a snapshot. Create your images from that image.
	
	D. Create a custom Compute Engine image from a snapshot. Create your instances from that image. [✔️]
	(Snapshot->Image->Instances)

	-------

	When you need to create a copy of a custom Compute Engine virtual machine (VM) to accommodate an expected increase in application traffic, the recommended approach involves using snapshots and custom images. Let's evaluate each option:

		A. Create a Compute Engine snapshot of your base VM. Create your images from that snapshot.

			While creating a snapshot is a good practice for backing up data, creating images directly from a snapshot might not be as flexible and doesn't provide as many customization options.
		
		B. Create a Compute Engine snapshot of your base VM. Create your instances from that snapshot.

			This option is closer to the recommended approach. Snapshots can be used to capture the state of a disk, and you can use these snapshots to create new instances. However, for more customization and control, using custom images might be a better option.
		
		C. Create a custom Compute Engine image from a snapshot. Create your images from that image.

			This is a more precise option. Creating a custom image from a snapshot allows you to have a predefined image that includes not only the disk content but also the machine configuration, allowing for more customization when creating new instances.
		
		D. Create a custom Compute Engine image from a snapshot. Create your instances from that image.

			This is the most suitable option. Creating a custom image from a snapshot and then creating instances from that custom image provides a more streamlined process. Custom images include both the disk content and the machine configuration, offering a comprehensive solution for copying a VM.
	
	Conclusion:
		
		Option D, "Create a custom Compute Engine image from a snapshot. Create your instances from that image," is the recommended approach. It allows you to capture both the disk content and machine configuration in a custom image, providing flexibility and control when creating new instances.

-------

* You have deployed an application on a single Compute Engine instance. The application writes logs to disk. Users start reporting errors with the application. You want to diagnose the problem. What should you do?
[Question 231]
	
	A. Navigate to Cloud Logging and view the application logs. [Bard and ChatGPT Suggested] [✔️]
	
	B. Connect to the instance's serial console and read the application logs.
	
	C. Configure a Health Check on the instance and set a Low Healthy Threshold value.
	
	D. Install and configure the Cloud Logging Agent and view the logs from Cloud Logging. [✔️]

	-------

	The correct answer is A. Navigate to Cloud Logging and view the application logs.

		When an application writes logs to disk, Cloud Logging can automatically collect and centralize those logs for easy access and analysis. This makes it the most efficient way to diagnose application errors and identify the root cause of the problem.

	Option B, connecting to the instance's serial console, is a less efficient approach as it requires manually accessing the instance and manually reading the logs. It also doesn't provide a central repository for logs to be stored and analyzed.

	Option C, configuring a Health Check on the instance, is used to monitor the instance's health and determine whether it is functioning correctly. It doesn't directly address the issue of analyzing application logs to identify errors.

	Option D, installing and configuring the Cloud Logging Agent, is an alternative method of collecting logs from the instance. However, it is still necessary to view the logs in Cloud Logging to analyze the errors.

	Therefore, the most effective way to diagnose application errors in this scenario is to navigate to Cloud Logging and view the application logs. This provides a centralized and easily searchable repository of logs for efficient analysis.

-------

* An application generates daily reports in a Compute Engine virtual machine (VM). The VM is in the project corp-iot-insights. Your team operates only in the project corp-aggregate-reports and needs a copy of the daily exports in the bucket corp-aggregate-reports-storage. You want to configure access so that the daily reports from the VM are available in the bucket corp-aggregate-reports-storage and use as few steps as possible while following Google-recommended practices. What should you do? [Question 133]
	
	A. Move both projects under the same folder. [Suggested]
	
	B. Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage. [✔️]
	
	C. Create a Shared VPC network between both projects. Grant the VM Service Account the role Storage Object Creator on corp-iot-insights.
	
	D. Make corp-aggregate-reports-storage public and create a folder with a pseudo-randomized suffix name. Share the folder with the IoT team.

	-------

	To configure access so that the daily reports from the Compute Engine virtual machine (VM) in the project corp-iot-insights are available in the bucket corp-aggregate-reports-storage in the project corp-aggregate-reports while following Google-recommended practices, you should consider granting appropriate roles to the service account and managing access securely. Let's evaluate each option:

	A. Move both projects under the same folder.

		Moving projects under the same folder is not a recommended approach for managing cross-project access to resources. Folders are more for organizational purposes, and moving projects into the same folder doesn't inherently grant access between them.
	
	B. Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage.

		This is a reasonable option. Granting the VM's service account the role of Storage Object Creator on the target bucket allows the VM to write objects (daily reports) to the specified bucket.
	
	C. Create a Shared VPC network between both projects. Grant the VM Service Account the role Storage Object Creator on corp-iot-insights.

		Shared VPCs are primarily for sharing a network between projects, and it might be an unnecessary step for simply writing objects to a Cloud Storage bucket. Granting the VM's service account the necessary role directly on the bucket (Option B) is a more straightforward solution.
	
	D. Make corp-aggregate-reports-storage public and create a folder with a pseudo-randomized suffix name. Share the folder with the IoT team.

		Making the storage bucket public is generally not recommended for secure data access. Additionally, sharing a folder with the IoT team may not be necessary, as access controls at the bucket level are typically sufficient.
	
	Conclusion:
		
		Option B, "Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage," is a suitable and straightforward approach. It provides the necessary permissions for the VM to write daily reports to the specified Cloud Storage bucket without introducing unnecessary complexity.

	-------

	Bard:

		The correct answer is B. Grant the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage. This is the most secure and efficient way to grant the necessary permissions to the VM while adhering to Google's recommended practices.

		Option A, moving both projects under the same folder, would unnecessarily complicate the organization and not directly address the access issue.

		Option C, creating a Shared VPC network between both projects, would introduce additional complexity and potential security risks. It's not necessary to grant the entire Shared VPC network access to the bucket. Instead, it's more secure to grant specific permissions to the VM Service Account, which is directly associated with the VM.

		Option D, making corp-aggregate-reports-storage public and creating a folder to share with the IoT team, is a less secure approach as it exposes the bucket to unauthorized access. Additionally, creating a pseudo-randomized folder suffix doesn't provide effective security measures.

		Therefore, granting the VM Service Account the role Storage Object Creator on corp-aggregate-reports-storage is the most appropriate and secure solution to manage access to the daily reports from the VM. This approach minimizes the number of steps, adheres to Google's recommended practices, and provides granular control over permissions.

-------

* You built an application on your development laptop that uses Google Cloud services. Your application uses Application Default Credentials for authentication and works fine on your development laptop. You want to migrate this application to a Compute Engine virtual machine (VM) and set up authentication using Google-recommended practices and minimal changes. What should you do? [Question 134]
	
	A. Assign appropriate access for Google services to the service account used by the Compute Engine VM. [✔️]
	
	B. Create a service account with appropriate access for Google services, and configure the application to use this account. [Another Most Voted and ChatGPT suggested]
	
	C. Store credentials for service accounts with appropriate access for Google services in a config file, and deploy this config file with your application.
	
	D. Store credentials for your user account with appropriate access for Google services in a config file, and deploy this config file with your application.

	-------

	For migrating your application to a Compute Engine virtual machine (VM) and setting up authentication using Google-recommended practices with minimal changes, you should consider the use of service accounts and avoid storing user credentials directly in your application. Let's evaluate each option:

	A. Assign appropriate access for Google services to the service account used by the Compute Engine VM.

		This is a valid and recommended practice. You can assign specific roles to the service account associated with your Compute Engine VM, granting it the necessary permissions to access Google Cloud services. The application running on the VM can then use the Application Default Credentials associated with this service account.
	
	B. Create a service account with appropriate access for Google services, and configure the application to use this account.

		This is a recommended practice. Creating a service account specifically for your application allows you to manage access more granularly. You can assign specific roles to this service account based on the principle of least privilege.
	
	C. Store credentials for service accounts with appropriate access for Google services in a config file, and deploy this config file with your application.

		While it's technically possible, storing service account credentials in a config file is generally not recommended. Google Cloud provides Application Default Credentials (ADC) that can automatically fetch the credentials associated with the environment where your application is running. This helps avoid hardcoding credentials in your code.
	
	D. Store credentials for your user account with appropriate access for Google services in a config file, and deploy this config file with your application.

		Storing user account credentials in a config file is not recommended for security reasons. User account credentials are intended for interactive use and should not be embedded in deployed applications.
	
	Conclusion:
		
		Option B, "Create a service account with appropriate access for Google services, and configure the application to use this account," is the recommended approach. It aligns with Google Cloud's best practices, allowing you to manage access control more effectively and securely.

	--------

	Bard:

		The correct answer is A. Assign appropriate access for Google services to the service account used by the Compute Engine VM.

			Using Application Default Credentials on a Compute Engine VM is not recommended as it exposes the service account credentials to the VM environment, making them vulnerable to security breaches. Instead, it's preferable to create a dedicated service account for the VM and assign it the necessary permissions to access Google Cloud services. This approach provides better isolation and security for the service account credentials.

		Option B, creating a service account and configuring the application directly, is technically correct but involves more changes to the application's code. Using the VM Service Account simplifies the process by leveraging the pre-existing service account associated with the VM.

		Option C, storing service account credentials in a config file, is not secure as it exposes the credentials to the application's deployment environment. This could allow unauthorized access to the service account and the resources it controls.

		Option D, storing user account credentials in a config file, is even more insecure as it grants the application direct access to the user's Google Cloud account. This poses a significant security risk, making it a major violation of Google's recommended practices.

		Therefore, the most secure and recommended approach is to assign appropriate access for Google services to the service account used by the Compute Engine VM. This approach maintains the application's functionality while preserving the security of the service account credentials.

-------

** You need to create a Compute Engine instance in a new project that doesn't exist yet. What should you do?
[Question 135]

	A. Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project. [✔️]
	
	B. Enable the Compute Engine API in the Cloud Console, use the Cloud SDK to create the instance, and then use the --project flag to specify a new project. [Suggested and ChatGPT Suggested]
	
	C. Using the Cloud SDK, create the new instance, and use the --project flag to specify the new project. Answer yes when prompted by Cloud SDK to enable the Compute Engine API.
	
	D. Enable the Compute Engine API in the Cloud Console. Go to the Compute Engine section of the Console to create a new instance, and look for the Create In A New Project option in the creation form.

	-------

	To create a Compute Engine instance in a new project that doesn't exist yet, you would typically follow these steps:

	A. Using the Cloud SDK, create a new project, enable the Compute Engine API in that project, and then create the instance specifying your new project.

		This is partially correct. You would need to create a new project, but enabling the Compute Engine API is not explicitly mentioned. Enabling the API is a necessary step for working with Compute Engine.
	
	B. Enable the Compute Engine API in the Cloud Console, use the Cloud SDK to create the instance, and then use the --project flag to specify a new project.

		This is a valid approach. Enabling the Compute Engine API is a prerequisite, and using the Cloud SDK with the --project flag allows you to specify the new project when creating the instance.
	
	C. Using the Cloud SDK, create the new instance, and use the --project flag to specify the new project. Answer yes when prompted by Cloud SDK to enable the Compute Engine API.

		This is a plausible option. Using the Cloud SDK to create the instance and specifying the new project with the --project flag is correct. The Cloud SDK might prompt you to enable the Compute Engine API if it's not already enabled.
	
	D. Enable the Compute Engine API in the Cloud Console. Go to the Compute Engine section of the Console to create a new instance, and look for the Create In A New Project option in the creation form.

		This option is not entirely accurate. While you can enable the Compute Engine API in the Cloud Console, the option to create an instance directly in a new project may not be available within the Compute Engine section. You typically create instances within projects.
	
	Conclusion:
		
		Option B, "Enable the Compute Engine API in the Cloud Console, use the Cloud SDK to create the instance, and then use the --project flag to specify a new project," is a recommended and accurate approach. It ensures that the necessary API is enabled and allows you to specify the new project when creating the instance using the Cloud SDK.

	-------

	Bard:

		The correct answer is D. Enable the Compute Engine API in the Cloud Console. Go to the Compute Engine section of the Console to create a new instance, and look for the Create In A New Project option in the creation form.

			This approach offers the most straightforward and user-friendly method for creating a Compute Engine instance in a new project that doesn't exist yet. It directly utilizes the Cloud Console, eliminating the need to manage projects and API configurations using the Cloud SDK.

		Option A, creating a new project using the Cloud SDK, enabling the Compute Engine API, and then creating the instance, involves multiple steps and requires familiarity with the Cloud SDK tools.

		Option B, enabling the Compute Engine API in the Cloud Console and using the Cloud SDK to create the instance, introduces an additional step of switching between the Cloud Console and the Cloud SDK. It's more efficient to manage the project and API configurations directly in the Cloud Console.

		Option C, creating the instance using the Cloud SDK and specifying the new project, requires the user to manage the project and API configurations manually. The Cloud Console provides a more intuitive and guided process for creating instances in new projects.

		Therefore, creating the instance directly in the Cloud Console, enabling the Compute Engine API if necessary, and selecting the "Create In A New Project" option provides a streamlined and efficient approach for establishing a new project and deploying a Compute Engine instance.

-------

** Your company runs one batch process in an on-premises server that takes around 30 hours to complete. The task runs monthly, can be performed offline, and must be restarted if interrupted. You want to migrate this workload to the cloud while minimizing cost. What should you do?
[Question 136]
	
	A. Migrate the workload to a Compute Engine Preemptible VM. [Bard Suggested] [✔️]
	
	B. Migrate the workload to a Google Kubernetes Engine cluster with Preemptible nodes. [Suggested]
	
	C. Migrate the workload to a Compute Engine VM. Start and stop the instance as needed. [✔️]
	
	D. Create an Instance Template with Preemptible VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload. [ChatGPT Suggested]

	-------

	Given the scenario where the batch process takes around 30 hours, runs monthly, can be performed offline, and must be restarted if interrupted, let's evaluate each option:

	A. Migrate the workload to a Compute Engine Preemptible VM.

		Preemptible VMs are cost-effective but come with the limitation that they can be terminated by Google Cloud with very little notice (up to 30 seconds). If the batch process can be restarted from the beginning if interrupted and the potential loss of a Preemptible VM is acceptable, this could be a cost-effective option.
	
	B. Migrate the workload to a Google Kubernetes Engine cluster with Preemptible nodes.

		Google Kubernetes Engine (GKE) with Preemptible nodes is similar to option A but provides additional flexibility and scalability. However, it adds complexity compared to using individual Compute Engine instances.
	
	C. Migrate the workload to a Compute Engine VM. Start and stop the instance as needed.

		This option allows you to manually start and stop the instance. While this provides flexibility, it might not be the most efficient or automated solution for a monthly batch process that can run for an extended period.
	
	D. Create an Instance Template with Preemptible VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload.

		This option combines the benefits of Preemptible VMs with the scalability and automation of a Managed Instance Group. Adjusting Target CPU Utilization allows you to automatically adjust the number of instances based on the workload, providing a good balance of cost-effectiveness and efficiency.
	
	Conclusion:
		
		Option D, "Create an Instance Template with Preemptible VMs On. Create a Managed Instance Group from the template and adjust Target CPU Utilization. Migrate the workload," seems to be a well-balanced solution. It leverages the cost advantages of Preemptible VMs, provides automation through a Managed Instance Group, and allows for scaling based on the workload. This can help minimize costs while efficiently handling the monthly batch process.

	-------

	Bard:

		The correct answer is A. Migrate the workload to a Compute Engine Preemptible VM.

			This is the most cost-effective option for migrating the batch process to the cloud as Preemptible VMs offer up to 90% lower compute costs. Since the batch process runs monthly and can be performed offline, the potential for preemptions is minimal, further reducing costs.

		Option B, migrating to a Google Kubernetes Engine (GKE) cluster with Preemptible nodes, would involve additional complexity and management overhead. While GKE offers flexibility and scalability, it might not be necessary for a single batch process.

		Option C, migrating to a Compute Engine VM and starting and stopping the instance as needed, would not be as cost-effective as using Preemptible VMs. Since the process runs monthly, there would be significant downtime associated with stopping and starting the instance each time.

		Option D, creating an Instance Template with Preemptible VMs On, creating a Managed Instance Group from the template, and adjusting Target CPU Utilization, is an alternative approach that could optimize costs. However, it involves more setup and maintenance compared to directly using Preemptible VMs.

		Therefore, migrating the workload directly to a Compute Engine Preemptible VM is the most cost-effective and straightforward solution for running the monthly batch process. The availability of Preemptible VMs allows Google Cloud to offer significant savings without compromising the process's ability to run offline and be restarted if interrupted.